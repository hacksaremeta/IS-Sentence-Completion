{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hacksaremeta/IS-Sentence-Completion/blob/model/is_autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph1q75Fa-a4Z"
   },
   "source": [
    "# Sentence Completion (TUD IS Project)\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "## Table of contents\n",
    "* 1 [Introduction](#introduction)  \n",
    "* 2 [Training data](#data_preparation)  \n",
    "    * 2.1 [DataManager class](#data_manager)  \n",
    "    * 2.2 [DataUtils class](#data_utils)  \n",
    "* 3 [Neural Network Implementation](#impl_nn)  \n",
    "    * 3.1 [Data preparation](#keras_preparation)  \n",
    "    * 3.2 [Creating a neural network](#keras_create_rnn)\n",
    "    * 3.3 [Training the neural network](#keras_train_rnn)\n",
    "    * 3.4 [Making Predictions](#keras_predict_rnn) \n",
    "* 4 [Evaluation](#evaluation)\n",
    "    * 4.1 [Dataset comparison](#eval_dataset)\n",
    "    * 4.2 [Data preparation comparison](#eval_preparation)\n",
    "    * 4.3 [Network comparison](#eval_rnn)\n",
    "    * 4.4 [Conclusion](#eval_conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this project our task is to extend sentences from abstracts fetched from PubMed. To solve this problem we decided to use a recurrent neural network (RNN) because such networks are good at solving this kind of task. After training the network on sequences of words it can be used to extend such sentences. In short<sup>[1](#fn1)</sup>, the network takes a number of words as input and then predicts the next word in that sequence. We can then append the predicted word to the original sequence and input that into the network again and repeat this process to predict an arbitrary number of words. In order to implement such a network, we used [Keras](https://keras.io/) from [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "The task can be divided into obtaining datasets, preparing the datasets for input to the RNN, constructing and training the RNN and finally making predictions. During our testing, we try find a good model<sup>[2](#fn2)</sup> by training the network independently on different datasets, changing preparation stages and tweaking parameters. We then compare our models in the [Evaluation](#evaluation) section.\n",
    "\n",
    "We used the following methods from the lecture:\n",
    "- Regular Expressions\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fn1\">1</a>: Simplified explanation, the details differ: The network calculates a function on numeric data, therefore the mentioned \"words\" are actually represented as vectors of integers. Likewise, the output is a floating-point vector that can be interpreted as a prediction. For details on how this network operates see [Neural Network Implementation](#impl_nn).\n",
    "\n",
    "<a name=\"fn2\">2</a>: (serializable) Keras representation of the neural network; The models we saved in the process contain both information about the structure of the network as well as weights of the connections (models serialized after training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83a_WoQu-a4d"
   },
   "source": [
    "<a id=\"data_preparation\"></a>\n",
    "## Training data\n",
    "\n",
    "In order to fetch data from PubMed and save it into different datasets as well as to load those datasets, some functionality is needed. This functionality will be provided by the [DataManager class](#data_manager).\n",
    "The loaded dataset then has to be prepared for training the neural network. This includes tokenization, label and feature extraction and encoding, all of which is handled by the [DataUtils class](#data_utils).\n",
    "TODO: more explanation / documentation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ncf2XA-a4d"
   },
   "source": [
    "<a id=\"data_manager\"></a>\n",
    "### DataManager Class\n",
    "\n",
    "- Provides functionality regarding data including fetch, persistence and TF2/Keras preparation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgNEorLo-gU_",
    "outputId": "ac24006c-ca3f-4b58-f9a9-833c62892435"
   },
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1-rHWaum-a4e"
   },
   "outputs": [],
   "source": [
    "import os, json, logging, string\n",
    "from Bio import Entrez, Medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0ynhg2b0-a4f"
   },
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \"\"\"Provides fetch, save and load functionality for datasets in json format\"\"\"\n",
    "    \n",
    "    def __init__(self, email, root_dir):\n",
    "        self.email = email\n",
    "        self.root_dir = root_dir\n",
    "        self.log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def _exists_dataset(self, name):\n",
    "        \"\"\"Checks whether a dataset with the given name exists\"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            return False\n",
    "            \n",
    "        for file in os.listdir(self.root_dir):\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(self.root_dir, file), 'r') as f:\n",
    "                    content = json.load(f)\n",
    "                    if content[\"name\"] == name:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def _fetch_papers(self, query : str, limit : int) -> 'list[dict]':\n",
    "        \"\"\"Retrieves data from PubMed\"\"\"\n",
    "        Entrez.email = self.email\n",
    "        record = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=limit))\n",
    "        idlist = record[\"IdList\"]\n",
    "        self.log.info(\"Found %d records for %s.\" % (len(idlist), query.strip()))\n",
    "        records = Medline.parse(Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode = \"text\"))\n",
    "        return [r for r in records if \"AB\" in r]\n",
    "\n",
    "    \n",
    "    def _fetch_abstracts(self, query : str, limit : int) -> 'list[str]':\n",
    "        \"\"\"Retrieves abstracts from PubMed\"\"\"\n",
    "        papers = self._fetch_papers(query, limit)\n",
    "        list_of_abstracts = [p['AB'] for p in papers]\n",
    "\n",
    "        return list_of_abstracts\n",
    "        \n",
    "    def create_dataset(self, queries : 'list[str]', name : str, limit=50, overwrite=False) -> None:\n",
    "        \"\"\"\n",
    "        Wraps other methods in this class\n",
    "        Creates a dataset from multiple queries\n",
    "        Does nothing if the dataset is already present (param overwrite)\n",
    "        Limits every query to <limit> results\n",
    "        \"\"\"\n",
    "        exists_dataset = self._exists_dataset(name)\n",
    "        if not exists_dataset or (exists_dataset and overwrite):\n",
    "            self.log.info(\"Dataset does not exist, fetching from PubMed...\")\n",
    "\n",
    "            res = dict()\n",
    "            res[\"name\"] = name\n",
    "            res[\"data\"] = list()\n",
    "            \n",
    "            for q in queries:\n",
    "                q_data = dict()\n",
    "                q_data[\"query\"] = q\n",
    "                q_data[\"abstracts\"] = self._fetch_abstracts(q, limit)\n",
    "                res[\"data\"].append(q_data)\n",
    "            \n",
    "            self._save_dataset(res, name)\n",
    "        else:\n",
    "            self.log.info(\"Dataset already exists, skipping fetch\")\n",
    "\n",
    "    def _save_dataset(self, dataset: dict, name : str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a file <name>.json in the dataset directory\n",
    "        For JSON file structure see below\n",
    "        Param dataset has a structure analogous to the JSON file\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            os.makedirs(self.root_dir)\n",
    "\n",
    "        with open(os.path.join(self.root_dir, name + \".json\"), 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "    def load_full_dataset(self, name : str) -> 'list[str]':\n",
    "        \"\"\"\n",
    "        Finds the file that matches given <name> in JSON information,\n",
    "        parses it, loading all abstracts into a list (one string for each abstract)\n",
    "        and returns it (Error if dataset doesn't exist)\n",
    "        \"\"\"\n",
    "\n",
    "        if  not self._exists_dataset(name):\n",
    "            self.log.info(\"Dataset does not exist\")\n",
    "            \n",
    "        else:\n",
    "           with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
    "                abstract_list=[]\n",
    "                jsonObject = json.load(file)\n",
    "                data_list= jsonObject['data']\n",
    "                for item in data_list:\n",
    "                    abstract_list.extend(item['abstracts'])\n",
    "                return abstract_list\n",
    "\n",
    "    def load_query_from_dataset(self, name : str, query : str) -> 'list[str]':\n",
    "        \"\"\"Like load_full_dataset but only loads abstracts for a single query\"\"\"\n",
    "\n",
    "\n",
    "        result = self._exists_dataset(name)\n",
    "\n",
    "        if  result:\n",
    "\n",
    "            with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
    "\n",
    "                query_abstracts=[]\n",
    "                jsonObject = json.load(file)\n",
    "                data_list= jsonObject['data']\n",
    "\n",
    "                q_names = [x['query'] for x in data_list]\n",
    "\n",
    "                if query not in q_names:\n",
    "                    self.log.info(\"The Query that you are searching for,does not exist in the Dataset\")\n",
    "                else:\n",
    "\n",
    "                      for queries in data_list:\n",
    "                            if queries['query'] == query:\n",
    "                              query_abstracts.extend(queries['abstracts'])\n",
    "                              return query_abstracts\n",
    "\n",
    "        else:\n",
    "             self.log.info(\"Dataset does not exist\")\n",
    "\n",
    "\n",
    "    def remove_punctuation(self, name:str) -> 'list[str]':\n",
    "\n",
    "\n",
    "            abstracts_list= self.load_full_dataset(name)\n",
    "\n",
    "            for text in abstracts:\n",
    "\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "                abstracts_list.append(text)\n",
    "\n",
    "\n",
    "            return  abstracts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5byX7fyc-a4h"
   },
   "source": [
    "<a id=\"data_utils\"></a>\n",
    "### DataUtils Class\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "- Static class providing utility functions to prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vxsw3AMu-a4h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Any\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gn4elnKa-a4i"
   },
   "outputs": [],
   "source": [
    "# TODO: unify method param types (all np.array instead of list)\n",
    "class DataUtils():\n",
    "    \"\"\"Provides utility functions for data preparation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_features_and_labels(sequences : 'list[list[Any]]',\n",
    "                                            train_interval : 'tuple[int, int]') -> 'tuple[list[Any], list[Any]]':\n",
    "        \"\"\"\n",
    "        Choses a random number l from <train_interval> (chosen for every sequence) and extracts\n",
    "        features of dynamic length l from every sequence\n",
    "        Every l+1-th word is extracted as a label\n",
    "        Reurns tuple(features, labels)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        for s in sequences:\n",
    "            l = random.randrange(*train_interval, 1)\n",
    "            for i in range(l, len(s)):\n",
    "                # First l words are features\n",
    "                features.append(s[i-l : i])\n",
    "                \n",
    "                # l-th word is label\n",
    "                labels.append(s[i])\n",
    "        \n",
    "        return (features, labels)\n",
    "        \n",
    "    @staticmethod\n",
    "    def encode_data(labels : 'list[Any]', num_code_words : int) -> np.array:\n",
    "        \"\"\"\n",
    "        One-hot encode labels using numpy to\n",
    "        improve the training speed of the network\n",
    "        \"\"\"\n",
    "\n",
    "        # Use numpy for better compatibility and performance\n",
    "        # Data type: 8bit integers for binary numbers (0, 1)\n",
    "        # Could be optimized in space by using single bits instead\n",
    "        # But that adds overhead in calculation (tradeoff time - space)\n",
    "        # Since we want improved training speed we just use\n",
    "        # numpys smallest data type byte/uint8 here\n",
    "        labels_encoded = np.zeros((len(labels), num_code_words), dtype=np.uint8)\n",
    "\n",
    "        # One-hot encode\n",
    "        for i, word in enumerate(labels):\n",
    "            labels_encoded[i, word] = 1\n",
    "            \n",
    "        return labels_encoded\n",
    "    \n",
    "    # Uses Scikit-learn here for simplicity\n",
    "    @staticmethod\n",
    "    def split_data(features: np.array, labels: np.array, _test_size=0.2) -> Any:\n",
    "        \"\"\"\n",
    "        Splits features and labels into training and validation data sets\n",
    "        Returns: (features_training, features_validation, labels_training, labels_validation)\n",
    "        \"\"\"\n",
    "        return train_test_split(features, labels, test_size=_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DJfRQZSgWEU"
   },
   "source": [
    "<a id=\"impl_nn\"></a>\n",
    "## Neural Network Implementation (LSTM RNN)  \n",
    "For the general methodology regarding Keras neural networks see [Tensorflow Docs: Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation), [Sanchit Tanwar: Building our first neural network in keras](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5) and [Will Koehrsen: Recurrent Neural Networks by Example in Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470).\n",
    "\n",
    "After fetching the abstracts from PubMed, we first we have to convert the input abstracts into numeric sequences that the network can work with.\n",
    "Then we train the network by giving it n 'words' (features) from the sequences and having it predict the (n+1)-th word (label) in the sequence.\n",
    "The predicted word is then compared to the actual word present in the training data and back-propagation is used to tweak the network weights (this is by Keras during training). After the training process is done we can use the model to predict a number of words given an input sequence of arbitrary length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHgVW_Ut-a4j"
   },
   "source": [
    "<a id=\"keras_preparation\"></a>\n",
    "### Data preparation\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "The first step in this stage is to fetch the abstracts from PubMed. For this either a single or multiple queries with a specified limit for the number of abstracts to fetch can be used. The abstracts are then processed by multiple regular expressions to remove unwanted tokens such as emails, links, abbreviations, etc. After this, we use Keras' tokenizer (which is also saved in `res/tokenizers` for making predictions later) to convert the abstracts into sequences of integers where each word is represented by a unique integer. We also one-hot encode the labels for efficiency and to simplify the prediction process later. Note that this also increases the RAM usage considerably and can be omitted if deemed necessary.\n",
    "\n",
    "The final preparation step is to split the resulting data across the following (disjoint!) sets:\n",
    "\n",
    "- features (vectors of words that are used for training)\n",
    "- labels (vector of words that should be predicted)\n",
    "- validation features\n",
    "- validation labels\n",
    "\n",
    "The features and labels are used to train the network, whereas validation features and labels are used to test the network's predictions on different data of a similar domain<sup>[3](#fn3)</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fn3\">3</a>: similar domain only when using a single query to generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XLfhk0cS-a4k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, pickle, warnings\n",
    "from tensorflow import device\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Masking, Dense, Dropout, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TF2 GPU Warnings\n",
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_DQrwTagWEY",
    "outputId": "ab13ecdb-60aa-48a7-c92a-1c2fcfcaee09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] DataManager: Dataset already exists, skipping fetch\n",
      "[DEBUG] Main: First extracted abstract: Background: Surgical resection is frequently the recommended treatment for drug-resistant temporal lobe epilepsy (TLE), yet many factors play a role in patients' perceptions of brain surgery that ultimately impact decision-making. The purpose of the current study was to explore how people with epilepsy, in their own words, experienced the overall process of consenting to surgery for drug-resistant TLE. Methods and Materials: Data was drawn from in-person, semi-structured interviews of 19 adults with drug-resistant TLE eligible to undergo epilepsy surgery. A systematic thematic analysis was performed to code, sort and compare participant responses. The mean age of these 12 (63%) women and seven (37%) men was 37.6 years (18-68 years), with average duration of epilepsy of 13 years (2-30 years). Results: Meeting the neurosurgeon and consenting to surgery represented an important treatment milestone across a prolonged treatment trajectory. Four themes were identified: (1) Understanding the language of risk; (2) Overcoming risk; (3) Family-centered, shared decision-making, and (4) Building decisional-confidence. Conclusion: Despite living with the restrictions of chronic uncontrolled seizures, considering an elective brain procedure raised unique and complex questions. Personal beliefs and expectations related to treatment outcomes influenced how the consent process was ultimately experienced. Decisions to pursue surgery had frequently been made ahead of meeting the surgeon, with many describing the act of signing as personally empowering. Overall, satisfaction was expressed with the information provided during the surgical visit, despite later inaccurate recall of the facts. These findings support the resultant recommendation that the practice of informed consent be conceptualized as a systematic, structured interdisciplinary process which occurs over time and encompasses three stages: preparation, signing and follow-up after signing.\n",
      "[DEBUG] Main: First extracted abstract after RegEx preprocessing: Background : Surgical resection is frequently the recommended treatment for drug-resistant temporal lobe epilepsy , yet many factors play a role in patients' perceptions of brain surgery that ultimately impact decision-making . The purpose of the current study was to explore how people with epilepsy , in their own words , experienced the overall process of consenting to surgery for drug-resistant TLE . Data was drawn from in-person , semi-structured interviews of 19 adults with drug-resistant TLE eligible to undergo epilepsy surgery . A systematic thematic analysis was performed to code , sort and compare participant responses . The mean age of these 12 women and seven men was 37 . 6 years , with average duration of epilepsy of 13 years . Results : Meeting the neurosurgeon and consenting to surgery represented an important treatment milestone across a prolonged treatment trajectory . Four themes were identified : Understanding the language of risk ; Overcoming risk ; Family-centered , shared decision-making , and Building decisional-confidence . Conclusion : Despite living with the restrictions of chronic uncontrolled seizures , considering an elective brain procedure raised unique and complex questions . Personal beliefs and expectations related to treatment outcomes influenced how the consent process was ultimately experienced . Decisions to pursue surgery had frequently been made ahead of meeting the surgeon , with many describing the act of signing as personally empowering . Overall , satisfaction was expressed with the information provided during the surgical visit , despite later inaccurate recall of the facts . These findings support the resultant recommendation that the practice of informed consent be conceptualized as a systematic , structured interdisciplinary process which occurs over time and encompasses three stages : preparation , signing and follow-up after signing . \n",
      "[DEBUG] Main: First tokenized sequence: [102, 22, 38, 54, 16, 588, 1, 844, 31, 11, 1129, 183, 409, 51, 6, 589, 302, 114, 2515, 8, 233, 5, 732, 1569, 2, 10, 14, 18, 1570, 410, 1301, 4, 1, 205, 2, 1, 161, 34, 13, 7, 590, 303, 327, 9, 51, 6, 5, 73, 1130, 2516, 6, 524, 1, 234, 367, 2, 2517, 7, 14, 11, 1129, 1859, 4, 66, 13, 3653, 25, 3654, 6, 6135, 1571, 2, 1131, 387, 9, 1129, 1859, 1572, 7, 1573, 51, 14, 4, 8, 344, 2518, 79, 13, 67, 7, 3655, 6, 3656, 3, 666, 2519, 411, 4, 1, 206, 116, 2, 40, 226, 1919, 3, 1270, 2520, 13, 1860, 4, 207, 130, 6, 9, 515, 751, 2, 51, 2, 591, 130, 4, 32, 22, 2521, 1, 964, 3, 2517, 7, 14, 1920, 26, 345, 31, 3657, 965, 8, 966, 31, 1921, 4, 439, 3658, 15, 212, 22, 241, 1, 33, 2, 105, 135, 3659, 105, 135, 6136, 6, 2522, 1301, 6, 3, 3660, 6137, 4, 108, 22, 346, 1922, 9, 1, 1516, 2, 845, 1923, 192, 6, 1302, 26, 2523, 10, 169, 2524, 667, 3, 482, 1517, 4, 1574, 3661, 3, 3662, 287, 7, 31, 89, 967, 303, 1, 1303, 367, 13, 1570, 524, 4, 1132, 7, 2525, 14, 60, 588, 63, 668, 3663, 2, 2521, 1, 592, 6, 9, 302, 1575, 1, 1924, 2, 1925, 17, 3664, 3665, 4, 234, 6, 577, 13, 1576, 9, 1, 213, 669, 30, 1, 38, 3666, 6, 346, 752, 3667, 1926, 2, 1, 3668, 4, 40, 184, 412, 1, 3669, 2526, 18, 1, 440, 2, 1927, 1303, 27, 3670, 17, 8, 344, 6, 1928, 3671, 367, 56, 846, 265, 70, 3, 3672, 214, 968, 22, 1577, 6, 1925, 3, 652, 37, 1925, 4]\n",
      "[DEBUG] Main: First extracted feature: background : surgical resection is frequently the recommended treatment for drug-resistant temporal lobe epilepsy , yet many factors play a role in patients' perceptions of brain surgery that ultimately impact decision-making . the purpose of the current study was to explore how people with epilepsy , in their own words , experienced the overall process of consenting to surgery for [102, 22, 38, 54, 16, 588, 1, 844, 31, 11, 1129, 183, 409, 51, 6, 589, 302, 114, 2515, 8, 233, 5, 732, 1569, 2, 10, 14, 18, 1570, 410, 1301, 4, 1, 205, 2, 1, 161, 34, 13, 7, 590, 303, 327, 9, 51, 6, 5, 73, 1130, 2516, 6, 524, 1, 234, 367, 2, 2517, 7, 14, 11]\n",
      "[DEBUG] Main: First extracted label: drug-resistant [1129]\n",
      "[DEBUG] Main: First one-hot encoded label: [0 ... 1 (at index 1129) ... 0]\n",
      "[INFO] Main: Loaded 72969 sequences with an encoded length of ~954 bytes per sequence\n",
      "[INFO] Main: Size of training data: 58375 sequences\n",
      "[INFO] Main: Size of validation data: 14594 sequences\n",
      "[INFO] Main: Training data preparation finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Init logging\n",
    "    logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
    "    log = logging.getLogger(\"Main\")\n",
    "\n",
    "    # Create DataManager in 'res/datasets' folder\n",
    "    data_folder = os.path.join(\"res\", \"datasets\")\n",
    "    dman = DataManager(\"mymail@example.com\", data_folder)\n",
    "\n",
    "\n",
    "    queries = [\"brain surgery[Title/Abstract]\"]\n",
    "    records = 200\n",
    "    dataset_name = \"{0} Dataset {1}\".format(', '.join(queries).replace('/', '_'), str(records))\n",
    "    #query = [\"RNA\", \"mRNA\", \"tRNA\"]\n",
    "    #dataset_name = f\"RNA Dataset\"\n",
    "\n",
    "    # Gather maximum of 100 abstracts for each query\n",
    "    # I would suggest around 5 - 20 abstracts in total for the small data sets\n",
    "    # and maybe 500 - 5000 for the final ones but we'll have to test\n",
    "    # since that depends on how long it takes to train the network\n",
    "    # This only queries PubMed if data if the data is not already present\n",
    "    dman.create_dataset(queries, dataset_name, records, overwrite=False)\n",
    "\n",
    "    # Load the dataset\n",
    "    abstracts = dman.load_full_dataset(dataset_name)\n",
    "    #abstracts_mrna = dman.load_query_from_dataset(dataset_name, query)\n",
    "\n",
    "    ab = dman.remove_punctuation(dataset_name)\n",
    "\n",
    "    assert(len(ab) > 0)\n",
    "    log.debug(f\"First extracted abstract: {ab[0]}\")\n",
    "\n",
    "    # Perform some regex preprocessing to improve data quality\n",
    "    # Do this sequentially for better code readability\n",
    "    \n",
    "    # Remove section titles often present in abstracts\n",
    "    ab = [re.sub(r\"(\\.\\s|^)(?:\\w+[,:]?\\s){1,2}\\w+:\\s\", r\"\\1\", w) for w in ab]\n",
    "    \n",
    "    # Separate punctuation to keep in tokenization \n",
    "    ab = [re.sub(r\"([.:,;!?])\", r\" \\1 \", w) for w in ab]\n",
    "    \n",
    "    # Remove URLs and Emails\n",
    "    # Credit: Matthew O'Riordan\n",
    "    # https://blog.mattheworiordan.com/post/13174566389/url-regular-expression-for-links-with-or-without\n",
    "    regex_url_email = (r'((([A-Za-z]{3,9}:(?:\\/\\/)?)(?:[-;:&=\\+\\$,\\w]+@)?'\n",
    "                       r'[A-Za-z0-9.-]+|(?:www.|[-;:&=\\+\\$,\\w]+@)[A-Za-z0-9.-]+)((?:\\/[\\+~%'\n",
    "                       r'\\/.\\w\\-_]*)?\\??(?:[-\\+=&;%@.\\w_]*)#?(?:[.\\!\\/\\\\w]*))?)')\n",
    "    ab = [re.sub(regex_url_email, '', w) for w in ab]\n",
    "    \n",
    "    # Remove references like [...], (...)\n",
    "    ab = [re.sub(r\"(\\(.*?\\)|\\[.*?\\])\", '', w) for w in ab]\n",
    "        \n",
    "    # Substitute special Symbols like &, /, ...\n",
    "    ab = [re.sub(r\"([^\\s]*)&([^\\s]*)\", r\"\\1 and \\2\", w) for w in ab]\n",
    "    ab = [re.sub(r\"([^\\s]*)\\/([^\\s]*)\", r\"\\1 or \\2\", w) for w in ab]\n",
    "    \n",
    "    # Remove duplicate spaces\n",
    "    ab = [re.sub(r\"\\s\\s+\", ' ', w) for w in ab]\n",
    "    \n",
    "    assert(len(ab) > 0)\n",
    "    log.debug(f\"First extracted abstract after RegEx preprocessing: {ab[0]}\")\n",
    "    \n",
    "    # Tokenize abstracts\n",
    "    # See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "    # Filters slightly modified (comp. to docs) to keep punctuation\n",
    "    # Lowercase has to be used for pre-trained embeddings (filters='#$%&()*+-,<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    # '\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=None, \n",
    "        filters='\"#*+<>@=~_|Â´`\\\\[]{}\\t\\n',\n",
    "        lower = True, split = ' '\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(ab)\n",
    "\n",
    "    # Generates list of lists of integers\n",
    "    # Can be reversed with the sequences_to_texts() function of the tokenizer\n",
    "    sequences = tokenizer.texts_to_sequences(ab)\n",
    "\n",
    "    assert(len(sequences) > 0)\n",
    "    log.debug(f\"First tokenized sequence: {sequences[0]}\")\n",
    "\n",
    "    # Prepare data for input to RNN\n",
    "    # Extract features and labels\n",
    "    # Number of words before prediction:\n",
    "    # random number from feature_len_range\n",
    "    # Use tuple(n, n+1) for fixed feature length n\n",
    "    feature_len_range = (60, 61)\n",
    "    features, labels = DataUtils.extract_features_and_labels(sequences, feature_len_range)\n",
    "\n",
    "    assert(len(features) > 0 and len(labels) > 0)\n",
    "    log.debug(f\"First extracted feature: {tokenizer.sequences_to_texts(features)[0]} {features[0]}\")\n",
    "    log.debug(f\"First extracted label: {tokenizer.index_word[labels[0]]} [{labels[0]}]\")\n",
    "\n",
    "    # One-hot encode data for improved training performance\n",
    "    num_code_words = len(tokenizer.index_word) + 1\n",
    "    labels_encoded = DataUtils.encode_data(labels, num_code_words)\n",
    "\n",
    "    assert(len(labels_encoded) > 0)\n",
    "    log.debug(f\"First one-hot encoded label: [0 ... {labels_encoded[0][labels[0]]} (at index {labels[0]}) ... 0]\")\n",
    "\n",
    "    # Final log for prepared data\n",
    "    log.info(f\"Loaded {labels_encoded.shape[0]} sequences\"\n",
    "             f\" with an encoded length of ~{labels_encoded.shape[1] // 8} bytes per sequence\")\n",
    "    \n",
    "    # Convert features to numpy array\n",
    "    # This is necessary for input to the RNN\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    features_training, features_validation, labels_training, labels_validation = \\\n",
    "    DataUtils.split_data(features, labels_encoded, 0.2)\n",
    "    \n",
    "    assert(len(features_training) > 0 and len(features_validation) > 0 and\n",
    "           len(labels_training) > 0 and len(labels_validation) > 0)\n",
    "    log.info(f\"Size of training data: {features_training.shape[0]} sequences\")\n",
    "    log.info(f\"Size of validation data: {features_validation.shape[0]} sequences\")\n",
    "    \n",
    "    log.info(\"Training data preparation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNnGtqZq-a4l"
   },
   "source": [
    "<a id=\"keras_create_rnn\"></a>\n",
    "### Creating the neural network\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "Note: We use the words 'nodes', 'neurons' and 'cells' synonymously in context of the neural network.\n",
    "\n",
    "Our baseline network (`res/models/model-baseline_252276_200_1_128_128_150.h5`) consists of the following layers:\n",
    "- Embedding: maps input sequences to vectors\n",
    "- LSTM: Recurrent layer consisting of LSTM cells\n",
    "- Dense: For additional 'learning capacity'\n",
    "- Dropout: To regulate fitting and prevent overfitting\n",
    "- Dense: maps input to normalized probability distribution\n",
    "\n",
    "The parameters used for the baseline network are as follows:\n",
    "\n",
    "    # Network params\n",
    "    train_embedding = True\n",
    "    embed_vec_size = 200\n",
    "\n",
    "    num_nodes_lstm = 128\n",
    "    num_nodes_dense = 128\n",
    "\n",
    "    # Training params\n",
    "    num_batch = 2800\n",
    "    num_epochs = 150\n",
    "\n",
    "In this case our input sequence is an integer vector representation of a (variable) number of words. Since the labels are one-hot encoded, the probability distribution generated by the output layer can be used with argmax to pick the predicted word. We discuss influence of the parameters in the [Network Evaluation](#eval_rnn) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VRHKm03T-a4l"
   },
   "outputs": [],
   "source": [
    "# Network params\n",
    "train_embedding = True\n",
    "embed_vec_size = 250\n",
    "\n",
    "num_nodes_lstm = 1024\n",
    "num_nodes_dense = 2048\n",
    "\n",
    "# Training params\n",
    "num_batch = 128\n",
    "num_epochs = 150\n",
    "\n",
    "X_train = features_training\n",
    "y_train = labels_training\n",
    "X_valid = features_validation\n",
    "y_valid = labels_validation\n",
    "\n",
    "# Model save params\n",
    "model_name = \"model-xy-1234\"\n",
    "\n",
    "# Create directories if necessary\n",
    "model_dir = os.path.join(\"res\", \"models\")\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Filename semantics:\n",
    "# <model_name>_<train_size>_<output_dim>_<selftrained_embeddings>_<num_nodes_lstm>_<num_nodes_dense>_<num_epochs>.h5\n",
    "model_file = model_name + \"_\" + str(X_train.shape[0]) \\\n",
    "    + \"_\" + str(embed_vec_size) + \"_\" + str(int(train_embedding)) \\\n",
    "    + \"_\" + str(num_nodes_lstm) + \"_\" + str(num_nodes_dense) \\\n",
    "    + \"_\" + str(num_epochs) + \".h5\"\n",
    "\n",
    "# To make predictions later the tokenizer has to be saved\n",
    "tokenizer_dir = os.path.join(\"res\", \"tokenizers\")\n",
    "if not os.path.isdir(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "    \n",
    "with open(os.path.join(tokenizer_dir, model_file[:-3] + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "def make_model(input_dim : int,\n",
    "               embed_vec_size : int, nodes_lstm : int,\n",
    "               nodes_dense : int, dropout_lstm : float = 0.1,\n",
    "               dropout_lstm_recurrent : float = 0.1, dropout_dense : float = 0.5) -> Any:\n",
    "    \"\"\"\n",
    "    Creates a sequential Keras Model with given parameters\n",
    "    The embeddings have to be trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    # num_code_words = number of unique words\n",
    "    # training_length we use the first num_pred words\n",
    "    model.add(\n",
    "        Embedding(input_dim = input_dim,\n",
    "                  input_length = None,\n",
    "                  output_dim = embed_vec_size,\n",
    "                  trainable = True,\n",
    "                  mask_zero = True))\n",
    "\n",
    "    # LSTM layer (recurrent part of the network)\n",
    "    model.add(LSTM(nodes_lstm, return_sequences = False, \n",
    "                   dropout = dropout_lstm, recurrent_dropout = dropout_lstm_recurrent))\n",
    "\n",
    "    # Dense layer (connections to all nodes in previous layer)\n",
    "    model.add(Dense(nodes_dense, activation = 'relu'))\n",
    "\n",
    "    # Dropout layer (helps to prevent overfitting during training)\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer (converts to normalized probability distribution)\n",
    "    model.add(Dense(input_dim, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def make_model_pretrained(input_dim : int,\n",
    "               embed_vec_size : int, embedding_matrix : np.array,\n",
    "               nodes_lstm : int, nodes_dense : int,\n",
    "               dropout_lstm : float = 0.1, dropout_lstm_recurrent : float = 0.1,\n",
    "               dropout_dense : float = 0.5) -> Any:\n",
    "    \"\"\"\n",
    "    Creates a sequential Keras Model with given parameters\n",
    "    Uses pre-trained embeddings\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Allow for variable length input\n",
    "    model.add(Input(shape = (None,)))\n",
    "    \n",
    "    # Embedding layer\n",
    "    # num_code_words numb of unique words \n",
    "    # training_length we use the first num_pred words\n",
    "    model.add(\n",
    "        Embedding(input_dim = input_dim,\n",
    "                  input_length = None,\n",
    "                  output_dim = embed_vec_size,\n",
    "                  weights = embedding_matrix,\n",
    "                  trainable = False,\n",
    "                  mask_zero = True))\n",
    "\n",
    "    # Masking layer for pre-trained embeddings\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(nodes_lstm, return_sequences = False, \n",
    "                   dropout = dropout_lstm, recurrent_dropout = dropout_lstm_recurrent))\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(nodes_dense, activation = 'relu'))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(input_dim, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "\n",
    "# Create Keras model\n",
    "if train_embedding:\n",
    "    model = make_model(num_code_words,\n",
    "                       embed_vec_size, num_nodes_lstm,\n",
    "                       num_nodes_dense)\n",
    "else:\n",
    "    #model = make_model_pretrained(num_pred, num_code_words,\n",
    "    #                              embed_vec_size, embedding_matrix,\n",
    "    #                              num_nodes_lstm, num_nodes_dense)\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 250)         1908750   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1024)              5222400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7635)              15644115  \n",
      "=================================================================\n",
      "Total params: 24,874,465\n",
      "Trainable params: 24,874,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_train_rnn\"></a>\n",
    "### Training the neural network\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "\n",
    "We decided to use the validation set metrics to define our best model as the one with the lowest validation loss (val_loss) for simplicity.\n",
    "In short, the callbacks used here are called after each training step and can save the best model (Checkpoint) and logging data for plotting diagrams of the metrics (TensorBoard) and optionally stop the training process prematurely if the val_loss is no longer decreasing (EarlyStopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1bwkTMh0_gO"
   },
   "outputs": [],
   "source": [
    "train_log_dir = os.path.join(\"logs\", \"training\", model_file[:-3])\n",
    "\n",
    "def make_callbacks(model_name, save=True):\n",
    "    \"\"\"\n",
    "    Creates callbacks for saving the model after each step\n",
    "    and stopping once learning process is finished\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    #stopping = EarlyStopping(monitor='val_loss', patience = 5)\n",
    "    #callbacks.append(stopping)\n",
    "    \n",
    "    reduce = ReduceLROnPlateau(monitor = 'loss', factor = 0.2,\n",
    "                               patience = 5, min_lr = 0.0001)\n",
    "    callbacks.append(reduce)\n",
    "    \n",
    "    board = TensorBoard(log_dir = train_log_dir, profile_batch = 0)\n",
    "    callbacks.append(board)\n",
    "    \n",
    "    if save:\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            os.path.join(model_dir, model_file),\n",
    "            save_best_only = True,\n",
    "            save_weights_only = False)\n",
    "        callbacks.append(checkpoint)\n",
    "        \n",
    "    return callbacks\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On windows use 'tensorboard.exe --logdir logs/training' instead\n",
    "%tensorboard --logdir \"logs/training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-OY4rVbEr_x",
    "outputId": "5cb776a8-2bc8-4f90-85ee-7f40f57da897"
   },
   "outputs": [],
   "source": [
    "# Train the model and save the 'best' version\n",
    "history = model.fit(X_train,  y_train,  batch_size = num_batch, \n",
    "          epochs = num_epochs, callbacks = callbacks,\n",
    "          validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_predict_rnn\"></a>\n",
    "### Making Predictions\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "This section is the final step to achieve our goal. We load our Keras model and it's corresponding tokenizer and use it to make predictions. The routine given here takes arbitrary user input but for best results we recommend to supply input from the learning domain or at least the general bio-medical field (as PubMed is mainly a bio-medical database). It then generates words until it predicts a sentence delimiter (.?!) or a timeout occurs (in that case it predicts a fixed number of words). Since predictions will never be 100% accurate, the model does not always predict a sentence delimiter within a reasonable amount of time. In a more comprehensive solution one could change this to something like a method that relies on an HMM classifier to find a suitable end of the sentence and add punctuation accordingly and skip predicting punctuation entirely. The word that is selected for prediction from the output distribution of the network can be chosen by either appyling argmax to the output directly (can cause loops) or by picking from the probability distribution with a certain randomness (probabilities are still taken into account). We opted for the latter in this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from tensorflow import device\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "\n",
    "# Suppress TF2 GPU Warnings\n",
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"changed-network-brain-20-more-nodes_4370_250_1_1024_2048_150.h5\"\n",
    "\n",
    "model_dir = os.path.join(\"res\", \"models\")\n",
    "tokenizer_dir = os.path.join(\"res\", \"tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 250)         379000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1024)              5222400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1516)              3106284   \n",
      "=================================================================\n",
      "Total params: 10,806,884\n",
      "Trainable params: 10,806,884\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading with TF2-DirectML on GPU fails on Windows\n",
    "# Therefore force loading on CPU\n",
    "with device('/cpu:0'):\n",
    "    model = load_model(os.path.join(model_dir, model_file))\n",
    "\n",
    "model.summary()\n",
    "#model.evaluate(X_valid, y_valid, batch_size=4096, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.errors import UnimplementedError\n",
    "import numpy as np\n",
    "import time, warnings, logging\n",
    "\n",
    "# Init logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
    "log = logging.getLogger(\"Main\")\n",
    "\n",
    "# Suppress np.log divide by 0 warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "with open(os.path.join(tokenizer_dir, model_file[:-3] + \".pkl\"), \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness factor for picking predictions\n",
    "# from probability distribution (in range (0, 1])\n",
    "rand = 0.4\n",
    "\n",
    "# Timeout for prediction in seconds\n",
    "timeout = 2\n",
    "\n",
    "# Fallback prediction length\n",
    "# in case end of sentence is not predicted in time\n",
    "pred_len = 10\n",
    "\n",
    "# Get input sequence\n",
    "s = [input(\"Enter a sentence to complete: \")]\n",
    "sequences = np.array(tokenizer.texts_to_sequences(s))\n",
    "len_orig = len(sequences)\n",
    "\n",
    "# Make prediction\n",
    "predictions = []\n",
    "elem = \"\"\n",
    "t_start = time.time()\n",
    "\n",
    "# Stop on punctuation\n",
    "end_symbols = ['.', '?', '!']\n",
    "end_tokens = []\n",
    "for t in end_symbols:\n",
    "    try:\n",
    "        t = tokenizer.word_index[t]\n",
    "        end_tokens.append(t)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "while elem not in end_tokens:\n",
    "    try:\n",
    "        pred = model.predict(sequences)[0]\n",
    "    except UnimplementedError:\n",
    "        log.warning(\"Input contains words that are not in vocabulary. Prediction will be inaccurate.\")\n",
    "    \n",
    "    # Introduce random factor to prevent getting\n",
    "    # stuck in prediction loop\n",
    "    pred = np.exp(np.log(pred) / rand)\n",
    "    \n",
    "    # Issue: https://github.com/numpy/numpy/issues/8317\n",
    "    # Solution: https://stackoverflow.com/a/53605818\n",
    "    pred = np.asarray(pred).astype('float64')\n",
    "    \n",
    "    # Softmax\n",
    "    pred = pred / pred.sum()\n",
    "    \n",
    "    # Pick one word from the\n",
    "    # generated probability distribution\n",
    "    probs = np.random.multinomial(1, pred, 1)[0]\n",
    "    elem = np.argmax(probs)\n",
    "    \n",
    "    # Separately save predictions\n",
    "    predictions.append(elem)\n",
    "    \n",
    "    # Append to current sequence for new input\n",
    "    sequences = np.append(sequences, [[elem]], axis=1)\n",
    "    \n",
    "    # Timeout and fallback\n",
    "    elapsed = time.time() - t_start\n",
    "    if elapsed >= timeout:\n",
    "        log.warning(f\"Could not predict end of sentence within {timeout}s. Falling back to predicting {pred_len} words.\")\n",
    "        sequences = sequences[:len_orig + pred_len]\n",
    "        predictions = predictions[:pred_len]\n",
    "        break\n",
    "\n",
    "# Convert model output to human-readable text\n",
    "predictions = tokenizer.sequences_to_texts([predictions])[0]\n",
    "\n",
    "log.info(f\"Predicted sequence:\\n{''.join(s[0])}\\n{''.join(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## Evaluation\n",
    "\n",
    "For evaluation of our models we will mainly use the val_loss (validation loss) and val_acc (validation accuracy) metrics. We define the 'best' model (across epochs) as the one with the lowest validation loss. We will then make changes to parameters of the [dataset](#eval_dataset), [data preparation](#eval_preparation) and [network and training](#eval_rnn) to improve our model with respect to the individual stages. In order to keep things simple and organized, not all diagrams are shown in this document. For all diagrams and the corresponding model names, check the 'doc' folder.\n",
    "\n",
    "The model with initial, basic parameters is our baseline model. The params are as follows: \n",
    "    \n",
    "    # Network params\n",
    "    train_embedding = True\n",
    "    embed_vec_size = 200\n",
    "\n",
    "    num_nodes_lstm = 128\n",
    "    num_nodes_dense = 128\n",
    "\n",
    "    # Training params\n",
    "    num_batch = 2800\n",
    "    num_epochs = 150\n",
    "    \n",
    "Additionally, the tokenizer does not convert words to lowercase and punctuation is kept. The dataset used contains 1000 abstracts and 252276 sequences are extracted as training data from it. The PubMed query used is 'clustering[ti] algorithm' and the train/validation split is 0.8/0.2.\n",
    "\n",
    "    model-baseline_252276_200_1_128_128_150\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/baseline/images/epoch_val_loss.svg\" alt=\"baseline model val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/baseline/images/epoch_val_acc.svg\" alt=\"baseline model val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>baseline model val_loss</td>\n",
    "    <td>baseline model val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 5 Epochs per vertical line\n",
    "\n",
    "Metrics:\n",
    "\n",
    "|    Metrics        \t| Baseline Model |\n",
    "|:---------------------:|:--------------:|\n",
    "| acc (max)           \t|     23.33%     |\n",
    "| loss (min)          \t|      4.504     |\n",
    "| val_acc (max)       \t|     21.45%     |\n",
    "| val_loss (min)      \t|      5.553     |\n",
    "| step (max val_loss) \t|       44       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_dataset\"></a>\n",
    "### Dataset comparison\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "The dataset size has a great impact on the quality of the model. In this graph we compare the single query 'clustering[ti] algorithm' which is also used for the baseline model (<font color=\"#0A69A0\">blue curve</font> in all dataset diagrams) with 1000 abstracts. The query used for this dataset yielded around 4000 results on PubMed.  \n",
    "\n",
    "Other curves:\n",
    "\n",
    "- <font color=\"#C83369\">100 abstracts</font> (pink curve)\n",
    "- <font color=\"#009988\">200 abstracts</font> (green curve)\n",
    "- <font color=\"#CC3311\">500 abstracts</font> (red curve)\n",
    "- <font color=\"#33A0C8\">2000 abstracts</font> (light blue curve)\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_loss_cluster.svg\" alt=\"Clustering Algorithm Dataset val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_acc_cluster.svg\" alt=\"Clustering Algorithm Dataset val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Clustering Algorithm Dataset val_loss</td>\n",
    "    <td>Clustering Algorithm Dataset val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "We can see that the validation loss gets lower with larger dataset sizes. The dataset with 2000 abstracts achieves the lowest val_loss with 5.471 in epoch 32. with more data, the improvement of the model per epoch rises. The validation accuracy is roughly between 20% and 27% for the sizes > 100 abstracts and the greatest val_acc is achieved by the dataset with only 200 abstracts after 92 epochs with 26.98%. We also tested models trained on small datasets with <50 abstracts, which achieve really high validation accuracies and low loss values but the vocabulary of these models is quite limited and they don't generalize well compared to the models trained on more data.\n",
    "\n",
    "#### Trying different datasets\n",
    "\n",
    "Query: 'covid-19'  \n",
    "Results on PubMed: 211602  \n",
    "Curves:  \n",
    "\n",
    "- <font color=\"#0077BB\">200 abstracts</font> (blue curve; val_loss higher than baseline; val_acc lower than baseline)\n",
    "- <font color=\"#CC3311\">500 abstracts</font> (red curve)\n",
    "- <font color=\"#FF7043\">2000 abstracts</font> (orange curve)\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_loss_covid.svg\" alt=\"Covid-19 Dataset val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_acc_covid.svg\" alt=\"Covid-19 Dataset val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Covid-19 Dataset val_loss</td>\n",
    "    <td>Covid-19 Dataset val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "With this more general query (significantly more results than the clustering algorithm one) we can see a stable, gradual improvement in val_loss and val_acc with dataset size. The best model in this batch is the one with 2000 abstracts, which achieves a minimum val_loss of 5.616 and a maximum val_acc of 22.32%. Overall these models perform worse compared to the clustering algorithm dataset models.\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "Query: 'covid-19 vaccine'  \n",
    "Results on PubMed: 15601  \n",
    "Curves:  \n",
    "\n",
    "- <font color=\"#009988\">200 abstracts</font> (green curve)\n",
    "- <font color=\"#EE3377\">500 abstracts</font> (pink curve)\n",
    "- <font color=\"#33BBEE\">2000 abstracts</font> (light blue curve)\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_loss_covid_vaccine.svg\" alt=\"Covid-19 vaccine Dataset val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_acc_covid_vaccine.svg\" alt=\"Covid-19 vaccine Dataset val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Covid-19 vaccine Dataset val_loss</td>\n",
    "    <td>Covid-19 vaccine Dataset val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "In an effort to narrow down the query results we found this dataset to be a good middle ground.\n",
    "The best model was the one trained on 2000 abstracts again, the val_loss was lowered to 'only' 5.402 and the val_acc stayed about the same with 22.31%. We suspect that more general queries decrease the performance on predictions related to specific parts of the topic but increase the performance on predictions regarding a broader spectrum of the topic. This would mean that narrowing down the query for a really specific subject does not necessarily mean better models in every case since at some point the model is only really useful for very specific input data. A good middle ground between fitting to the data and generalization has to be found in that regard, which could be achieved by testing and evaluating the actual predictions by hand and picking the best model according to the requirements of the application.\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "Query: 'brain surgery[Title/Abstract]'  \n",
    "Results on PubMed: 2079  \n",
    "Curves:  \n",
    "\n",
    "- <font color=\"#AD3318\">200 abstracts</font> (red curve)\n",
    "- <font color=\"#0077BB\">500 abstracts</font> (blue curve; val_loss higher than baseline; val_acc lower than baseline)\n",
    "- <font color=\"#D66440\">1000 abstracts</font> (orange curve)\n",
    "- <font color=\"#A0A0A0\">2000 abstracts</font> (grey curve)\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_loss_brain_surgery.svg\" alt=\"brain surgery Dataset val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_acc_brain_surgery.svg\" alt=\"brain surgery Dataset val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Brain surgery Dataset val_loss</td>\n",
    "    <td>Brain surgery Dataset val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "Narrowing down results even more to just over 2000, we can see that the dataset with 2000 abstracts still yields the best model with val_loss = 5.504 and val_acc = 25.78%. The val_loss of this model slightly better than the one of the baseline model and val_acc is significantly better as well, again we expect this to be caused by the more narrow query.\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "Queries: [\"brain surgery[Title/Abstract]\", \"clustering[ti] algorithm\", \"covid-19 vaccine\"]  \n",
    "Results on PubMed: -    \n",
    "Curves:  \n",
    "\n",
    "- <font color=\"#33BBEE\">210 abstracts</font> (light blue curve)\n",
    "- <font color=\"#C83369\">510 abstracts</font> (pink curve)\n",
    "- <font color=\"#0A8477\">999 abstracts</font> (green curve)\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_loss_multiple.svg\" alt=\"Multiple query Dataset val_loss\" width=\"500\"></td>\n",
    "    <td><img src=\"doc/datasets/images/epoch_val_acc_multiple.svg\" alt=\"Multiple query Dataset val_acc\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Multiple query Dataset val_loss</td>\n",
    "    <td>Multiple query Dataset val_acc</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "Lastly, we tried using the previous queries combined to possibly enable predictions for broader topics. As expected, the val_loss and val_acc values are significantly worse than most models we tried so far. Since our baseline network only has 256 neurons in the hidden layers in total it is quite possible that increasing the amount of LSTM and dense nodes will allow for more learning parameters and therefore more 'learning capacity'. The best models we tested so far seem limited to about 30% val_acc and 5.4 val_loss, which might be caused by the small number of neurons 'bottlenecking' performance. With more neurons, more general and multiple queries as a base for the dataset probably yield models that are a lot better; due to our limited computing capacity and time we will not pursue that direction further here though.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_preparation\"></a>\n",
    "### Data preparation comparison\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_rnn\"></a>\n",
    "### Network comparison\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "The val_acc of 'best' models with the initial network seemed to max out at around 27-30%. To combat this for a final try to improve our model we opted for significantly more LSTM and dense nodes in combination with additional preprocessing and the narrow 'brain surgery[Title/Abstract]' dataset. The new network params are now as follows:\n",
    "\n",
    "    # Network params\n",
    "    train_embedding = True\n",
    "    embed_vec_size = 250\n",
    "\n",
    "    num_nodes_lstm = 1024\n",
    "    num_nodes_dense = 2048\n",
    "\n",
    "    # Training params\n",
    "    num_batch = 128\n",
    "    num_epochs = 150\n",
    "    \n",
    "Curves:  \n",
    "\n",
    "- <font color=\"#355C95\">Baseline model (1000 abstracts)</font> (blue curve)\n",
    "- <font color=\"#3F786D\">Clustering algorithm dataset model (200 abstracts)</font> (green curve)\n",
    "- <font color=\"#903A1D\">20 abstracts</font> (red curve)\n",
    "- <font color=\"#BA643E\">200 abstracts + lowercase tokenizer</font> (orange curve)\n",
    "- <font color=\"#5891BD\">2000 abstracts</font> (light blue curve)\n",
    "\n",
    "<img src=\"doc/optimization/images/epoch_val_loss_all.svg\" alt=\"val_loss\" width=\"1000\">\n",
    "Multiple query Dataset val_loss\n",
    "\n",
    "<img src=\"doc/optimization/images/epoch_val_acc_all.svg\" alt=\"val_acc\" width=\"1000\">\n",
    "Multiple query Dataset val_acc\n",
    "\n",
    "x-Axis scale: 10 Epochs per vertical line\n",
    "\n",
    "Metrics:\n",
    "\n",
    "|                     \t| Baseline Model \t| Best Dataset 200 \t| More Nodes 20 \t| More Nodes 2000 \t| More Nodes 200 (lowercase) \t|\n",
    "|---------------------\t|:--------------:\t|:----------------:\t|:-------------:\t|:---------------:\t|:--------------------------:\t|\n",
    "| acc (max)           \t|     23.33%     \t|      35.46%      \t|     71.72%    \t|      75.93%     \t|           62.67%           \t|\n",
    "| loss (min)          \t|      4.504     \t|       2.978      \t|     0.998     \t|      0.942      \t|            1.664           \t|\n",
    "| val_acc (max)       \t|     21.45%     \t|      26.98%      \t|     48.58%    \t|      54.54%     \t|           41.09%           \t|\n",
    "| val_loss (min)      \t|      5.553     \t|       5.677      \t|     5.037     \t|      4.832      \t|            5.019           \t|\n",
    "| step (max val_loss) \t|       44       \t|        92        \t|       18      \t|        18       \t|             10             \t|\n",
    "\n",
    "TODO: Write evalutation when we have data preparation and network results and possibly test another model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
