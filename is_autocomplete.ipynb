{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hacksaremeta/IS-Sentence-Completion/blob/model/is_autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph1q75Fa-a4Z"
   },
   "source": [
    "# Sentence Completion (TUD IS Project)\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "## Table of contents\n",
    "* 1 [Introduction (TODO)](#introduction)  \n",
    "* 2 [Training data](#data_preparation)  \n",
    "    * 2.1 [DataManager class](#data_manager)  \n",
    "    * 2.2 [DataUtils class](#data_utils)  \n",
    "* 3 [Neural Network Implementation](#impl_nn)  \n",
    "    * 3.1 [Data preparation](#keras_preparation)  \n",
    "    * 3.2 [Creating a neural network](#keras_create_rnn)\n",
    "    * 3.3 [Training the neural network](#keras_train_rnn)\n",
    "    * 3.4 [Evaluating the neural network (this will be removed in future)](#keras_evaluate_rnn)\n",
    "    * 3.5 [Making Predictions](#keras_predict_rnn) \n",
    "* 4 [Evaluation](#evaluation)\n",
    "    * 4.1 [Dataset comparison](#eval_dataset)\n",
    "    * 4.2 [Data preparation comparison](#eval_preparation)\n",
    "    * 4.3 [Network comparison](#eval_rnn)\n",
    "    * 4.4 [Conclusion](#eval_conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83a_WoQu-a4d"
   },
   "source": [
    "<a id=\"data_preparation\"></a>\n",
    "## Training data\n",
    "\n",
    "In order to fetch data from PubMed and save it into different datasets as well as to load those datasets, some functionality is needed. This functionality will be provided by the [DataManager class](#data_manager).\n",
    "The loaded dataset then has to be prepared for training the neural network. This includes tokenization, label and feature extraction and encoding, all of which is handled by the [DataUtils class](#data_utils).\n",
    "TODO: more explanation / documentation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ncf2XA-a4d"
   },
   "source": [
    "<a id=\"data_manager\"></a>\n",
    "### DataManager Class\n",
    "\n",
    "- Provides functionality regarding data including fetch, persistence and TF2/Keras preparation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgNEorLo-gU_",
    "outputId": "ac24006c-ca3f-4b58-f9a9-833c62892435"
   },
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1-rHWaum-a4e"
   },
   "outputs": [],
   "source": [
    "import os, json, logging, string\n",
    "from Bio import Entrez, Medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0ynhg2b0-a4f"
   },
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \"\"\"Provides fetch, save and load functionality for datasets in json format\"\"\"\n",
    "    \n",
    "    def __init__(self, email, root_dir):\n",
    "        self.email = email\n",
    "        self.root_dir = root_dir\n",
    "        self.log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def _exists_dataset(self, name):\n",
    "        \"\"\"Checks whether a dataset with the given name exists\"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            return False\n",
    "            \n",
    "        for file in os.listdir(self.root_dir):\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(self.root_dir, file), 'r') as f:\n",
    "                    content = json.load(f)\n",
    "                    if content[\"name\"] == name:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def _fetch_papers(self, query : str, limit : int) -> 'list[dict]':\n",
    "        \"\"\"Retrieves data from PubMed\"\"\"\n",
    "        Entrez.email = self.email\n",
    "        record = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=limit))\n",
    "        idlist = record[\"IdList\"]\n",
    "        self.log.info(\"Found %d records for %s.\" % (len(idlist), query.strip()))\n",
    "        records = Medline.parse(Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode = \"text\"))\n",
    "        return [r for r in records if \"AB\" in r]\n",
    "\n",
    "        \n",
    "    def create_dataset(self, query : str, name : str, limit=50, overwrite=False) -> None:\n",
    "        \"\"\"\n",
    "        Wraps other methods in this class\n",
    "        Creates a dataset from multiple queries\n",
    "        Does nothing if the dataset is already present (param overwrite)\n",
    "        Limits every query to <limit> results\n",
    "        \"\"\"\n",
    "        exists_dataset = self._exists_dataset(name)\n",
    "        if not exists_dataset or (exists_dataset and overwrite):\n",
    "            self.log.info(\"Dataset does not exist, fetching from PubMed...\")\n",
    "            q_data = dict()\n",
    "            q_data[\"query\"] = query\n",
    "            papers = self._fetch_papers(query, limit)\n",
    "            list_of_abstracts = [p[\"AB\"] for p in papers]\n",
    "            q_data[\"abstracts\"] = list_of_abstracts\n",
    "            res = {\"name\": name}\n",
    "            res[\"data\"] = list()\n",
    "            res[\"data\"].append(q_data)\n",
    "            self._save_dataset(res, name)\n",
    "        else:\n",
    "            self.log.info(\"Dataset already exists, skipping fetch\")\n",
    "\n",
    "    def _save_dataset(self, dataset: dict, name : str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a file <name>.json in the dataset directory\n",
    "        For JSON file structure see below\n",
    "        Param dataset has a structure analogous to the JSON file\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            os.makedirs(self.root_dir)\n",
    "\n",
    "        with open(os.path.join(self.root_dir, name + \".json\"), 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "    def load_full_dataset(self, name : str) -> 'list[str]':\n",
    "        \"\"\"\n",
    "        Finds the file that matches given <name> in JSON information,\n",
    "        parses it, loading all abstracts into a list (one string for each abstract)\n",
    "        and returns it (Error if dataset doesn't exist)\n",
    "        \"\"\"\n",
    "\n",
    "        if  not self._exists_dataset(name):\n",
    "            self.log.info(\"Dataset does not exist\")\n",
    "            \n",
    "        else:\n",
    "           with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
    "                abstract_list=[]\n",
    "                jsonObject = json.load(file)\n",
    "                data_list= jsonObject['data']\n",
    "                for item in data_list:\n",
    "                    abstract_list.extend(item['abstracts'])\n",
    "                return abstract_list\n",
    "\n",
    "    def load_query_from_dataset(self, name : str, query : str) -> 'list[str]':\n",
    "        \"\"\"Like load_full_dataset but only loads abstracts for a single query\"\"\"\n",
    "\n",
    "\n",
    "        result = self._exists_dataset(name)\n",
    "\n",
    "        if  result:\n",
    "\n",
    "            with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
    "\n",
    "                query_abstracts=[]\n",
    "                jsonObject = json.load(file)\n",
    "                data_list= jsonObject['data']\n",
    "\n",
    "                q_names = [x['query'] for x in data_list]\n",
    "\n",
    "                if query not in q_names:\n",
    "                    self.log.info(\"The Query that you are searching for,does not exist in the Dataset\")\n",
    "                else:\n",
    "\n",
    "                      for queries in data_list:\n",
    "                            if queries['query'] == query:\n",
    "                              query_abstracts.extend(queries['abstracts'])\n",
    "                              return query_abstracts\n",
    "\n",
    "        else:\n",
    "             self.log.info(\"Dataset does not exist\")\n",
    "\n",
    "\n",
    "    def remove_punctuation(self, name:str) -> 'list[str]':\n",
    "\n",
    "\n",
    "            abstracts_list= self.load_full_dataset(name)\n",
    "\n",
    "            for text in abstracts:\n",
    "\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "                abstracts_list.append(text)\n",
    "\n",
    "\n",
    "            return  abstracts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5byX7fyc-a4h"
   },
   "source": [
    "<a id=\"data_utils\"></a>\n",
    "### DataUtils Class\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "- Static class providing utility functions to prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vxsw3AMu-a4h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Any\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gn4elnKa-a4i"
   },
   "outputs": [],
   "source": [
    "# TODO: unify method param types (all np.array instead of list)\n",
    "class DataUtils():\n",
    "    \"\"\"Provides utility functions for data preparation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_features_and_labels(sequences : 'list[list[Any]]',\n",
    "                                            train_interval : 'tuple[int, int]') -> 'tuple[list[Any], list[Any]]':\n",
    "        \"\"\"\n",
    "        Choses a random number l from <train_interval> (chosen for every sequence) and extracts\n",
    "        features of dynamic length l from every sequence\n",
    "        Every l+1-th word is extracted as a label\n",
    "        Reurns tuple(features, labels)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        for s in sequences:\n",
    "            l = random.randrange(*train_interval, 1)\n",
    "            for i in range(l, len(s)):\n",
    "                # First l words are features\n",
    "                features.append(s[i-l : i])\n",
    "                \n",
    "                # l-th word is label\n",
    "                labels.append(s[i])\n",
    "        \n",
    "        return (features, labels)\n",
    "        \n",
    "    @staticmethod\n",
    "    def encode_data(labels : 'list[Any]', num_code_words : int) -> np.array:\n",
    "        \"\"\"\n",
    "        One-hot encode labels using numpy to\n",
    "        improve the training speed of the network\n",
    "        \"\"\"\n",
    "\n",
    "        # Use numpy for better compatibility and performance\n",
    "        # Data type: 8bit integers for binary numbers (0, 1)\n",
    "        # Could be optimized in space by using single bits instead\n",
    "        # But that adds overhead in calculation (tradeoff time - space)\n",
    "        # Since we want improved training speed we just use\n",
    "        # numpys smallest data type byte/uint8 here\n",
    "        labels_encoded = np.zeros((len(labels), num_code_words), dtype=np.uint8)\n",
    "\n",
    "        # One-hot encode\n",
    "        for i, word in enumerate(labels):\n",
    "            labels_encoded[i, word] = 1\n",
    "            \n",
    "        return labels_encoded\n",
    "    \n",
    "    # Uses Scikit-learn here for simplicity\n",
    "    @staticmethod\n",
    "    def split_data(features: np.array, labels: np.array, _test_size=0.2) -> Any:\n",
    "        \"\"\"\n",
    "        Splits features and labels into training and validation data sets\n",
    "        Returns: (features_training, features_validation, labels_training, labels_validation)\n",
    "        \"\"\"\n",
    "        return train_test_split(features, labels, test_size=_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DJfRQZSgWEU"
   },
   "source": [
    "<a id=\"impl_nn\"></a>\n",
    "## Neural Network Implementation (LSTM RNN)  \n",
    "For the general methodology regarding Keras neural networks see [Tensorflow Docs: Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation), [Sanchit Tanwar: Building our first neural network in keras](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5) and [Will Koehrsen: Recurrent Neural Networks by Example in Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470).\n",
    "In this case the sequences given are words instead of characters and the RNN predicts the next word.\n",
    "Therefore we use the Keras Tokenizer to convert sentences to vectors of word representatives (integers).\n",
    "After tokenization each 'word' will be converted to a feature vector using Keras pre-trained embeddings.\n",
    "Then we train the network by giving it n 'words' (features) from the PubMed training data and having it predict the (n+1)-th word (label) in the sequence.\n",
    "The predicted word is then compared to the actual word present in the training data and back-propagation is used to tweak the network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHgVW_Ut-a4j"
   },
   "source": [
    "<a id=\"keras_preparation\"></a>\n",
    "### Data preparation\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XLfhk0cS-a4k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, pickle, warnings\n",
    "from tensorflow import device\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Masking, Dense, Dropout, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TF2 GPU Warnings\n",
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_DQrwTagWEY",
    "outputId": "ab13ecdb-60aa-48a7-c92a-1c2fcfcaee09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] DataManager: Dataset already exists, skipping fetch\n",
      "[DEBUG] Main: First extracted abstract: In this research study, we first define the strong degree of a vertex in an m-polar fuzzy graph. Then we present various useful properties and prove some results concerning this new concept, in the case of complete m-polar fuzzy graphs. Further, we introduce the concept of m-polar fuzzy strength sequence of vertices, and we also investigate it in the particular instance of complete m-polar fuzzy graphs. We discuss connectivity parameters in m-polar fuzzy graphs with precise examples, and we investigate the m-polar fuzzy analogue of Whitney's theorem. Furthermore, we present a clustering method for vertices in an m-polar fuzzy graph based on the strength of connectedness between pairs of vertices. In order to formulate this method, we introduce terminologies such as A-reachable vertices in m-polar fuzzy graphs, A-connected m-polar fuzzy graphs, or A-connected m-polar fuzzy subgraphs (in case the m-polar fuzzy graph itself is not A-connected). Moreover, we discuss an application for clustering different companies in consideration of their multi-polar uncertain information. We then provide an algorithm to clearly understand the clustering methodology that we use in our application. Finally, we present a comparative analysis of our research work with existing techniques to prove its applicability and effectiveness.\n",
      "[DEBUG] Main: First extracted abstract after RegEx preprocessing: In this research study , we first define the strong degree of a vertex in an m-polar fuzzy graph . Then we present various useful properties and prove some results concerning this new concept , in the case of complete m-polar fuzzy graphs . Further , we introduce the concept of m-polar fuzzy strength sequence of vertices , and we also investigate it in the particular instance of complete m-polar fuzzy graphs . We discuss connectivity parameters in m-polar fuzzy graphs with precise examples , and we investigate the m-polar fuzzy analogue of Whitney's theorem . Furthermore , we present a clustering method for vertices in an m-polar fuzzy graph based on the strength of connectedness between pairs of vertices . In order to formulate this method , we introduce terminologies such as A-reachable vertices in m-polar fuzzy graphs , A-connected m-polar fuzzy graphs , or A-connected m-polar fuzzy subgraphs . Moreover , we discuss an application for clustering different companies in consideration of their multi-polar uncertain information . We then provide an algorithm to clearly understand the clustering methodology that we use in our application . Finally , we present a comparative analysis of our research work with existing techniques to prove its applicability and effectiveness . \n",
      "[DEBUG] Main: First tokenized sequence: [28, 24, 183, 57, 6, 21, 143, 763, 1, 836, 733, 2, 7, 6154, 8, 26, 3571, 114, 112, 4, 308, 21, 117, 160, 453, 549, 3, 1095, 182, 34, 2413, 24, 64, 825, 6, 8, 1, 442, 2, 751, 3571, 114, 500, 4, 952, 6, 21, 367, 1, 825, 2, 3571, 114, 1390, 582, 2, 1983, 6, 3, 21, 83, 1017, 58, 8, 1, 388, 1513, 2, 751, 3571, 114, 500, 4, 36, 1984, 524, 150, 8, 3571, 114, 500, 13, 2071, 1160, 6, 3, 21, 1017, 1, 3571, 114, 4875, 2, 14069, 6155, 4, 309, 6, 21, 117, 7, 9, 20, 11, 1983, 8, 26, 3571, 114, 112, 35, 14, 1, 1390, 2, 4126, 56, 975, 2, 1983, 4, 28, 239, 5, 1985, 24, 20, 6, 21, 367, 8639, 72, 22, 14070, 1983, 8, 3571, 114, 500, 6, 8640, 3571, 114, 500, 6, 44, 8640, 3571, 114, 6156, 4, 262, 6, 21, 1984, 26, 286, 11, 9, 45, 6157, 8, 2093, 2, 66, 14071, 2850, 54, 4, 36, 169, 124, 26, 17, 5, 1986, 976, 1, 9, 513, 16, 21, 122, 8, 55, 286, 4, 195, 6, 21, 117, 7, 1161, 38, 2, 55, 183, 155, 13, 101, 151, 5, 1095, 102, 1598, 3, 240, 4]\n",
      "[DEBUG] Main: First extracted feature: In this research study , we first define the strong degree of a vertex in an m-polar fuzzy graph . Then we present various useful properties and prove some results concerning this new concept , in the case of complete m-polar fuzzy graphs . Further , we introduce the concept of m-polar fuzzy strength sequence of vertices , and we [28, 24, 183, 57, 6, 21, 143, 763, 1, 836, 733, 2, 7, 6154, 8, 26, 3571, 114, 112, 4, 308, 21, 117, 160, 453, 549, 3, 1095, 182, 34, 2413, 24, 64, 825, 6, 8, 1, 442, 2, 751, 3571, 114, 500, 4, 952, 6, 21, 367, 1, 825, 2, 3571, 114, 1390, 582, 2, 1983, 6, 3, 21]\n",
      "[DEBUG] Main: First extracted label: also [83]\n",
      "[DEBUG] Main: First one-hot encoded label: [0 ... 1 (at index 83) ... 0]\n",
      "[INFO] Main: Loaded 315346 sequences with an encoded length of ~2398 bytes per sequence\n",
      "[INFO] Main: Size of training data: 252276 sequences\n",
      "[INFO] Main: Size of validation data: 63070 sequences\n",
      "[INFO] Main: Training data preparation finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Init logging\n",
    "    logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
    "    log = logging.getLogger(\"Main\")\n",
    "\n",
    "    # Create DataManager in 'res/datasets' folder\n",
    "    data_folder = os.path.join(\"res\", \"datasets\")\n",
    "    dman = DataManager(\"mymail@example.com\", data_folder)\n",
    "\n",
    "\n",
    "    query = \"clustering[ti] algorithm\"\n",
    "    records = 1000\n",
    "    dataset_name = f\"{query} Dataset {records}\"\n",
    "    #query = [\"RNA\", \"mRNA\", \"tRNA\"]\n",
    "    #dataset_name = f\"RNA Dataset\"\n",
    "\n",
    "    # Gather maximum of 100 abstracts for each query\n",
    "    # I would suggest around 5 - 20 abstracts in total for the small data sets\n",
    "    # and maybe 500 - 5000 for the final ones but we'll have to test\n",
    "    # since that depends on how long it takes to train the network\n",
    "    # This only queries PubMed if data if the data is not already present\n",
    "    dman.create_dataset(query, dataset_name, records, overwrite=False)\n",
    "\n",
    "    # Load the dataset\n",
    "    abstracts = dman.load_full_dataset(dataset_name)\n",
    "    abstracts_mrna = dman.load_query_from_dataset(dataset_name, query)\n",
    "\n",
    "    ab = dman.remove_punctuation(dataset_name)\n",
    "\n",
    "    assert(len(ab) > 0)\n",
    "    log.debug(f\"First extracted abstract: {ab[0]}\")\n",
    "\n",
    "    # Perform some regex preprocessing to improve data quality\n",
    "    # Do this sequentially for better code readability\n",
    "    \n",
    "    # Separate punctuation to keep in tokenization \n",
    "    ab = [re.sub(r\"([.:,;!?])\", r\" \\1 \", w) for w in ab]\n",
    "    \n",
    "    # Remove URLs and Emails\n",
    "    # Credit: Matthew O'Riordan\n",
    "    # https://blog.mattheworiordan.com/post/13174566389/url-regular-expression-for-links-with-or-without\n",
    "    regex_url_email = (r'((([A-Za-z]{3,9}:(?:\\/\\/)?)(?:[-;:&=\\+\\$,\\w]+@)?'\n",
    "                       r'[A-Za-z0-9.-]+|(?:www.|[-;:&=\\+\\$,\\w]+@)[A-Za-z0-9.-]+)((?:\\/[\\+~%'\n",
    "                       r'\\/.\\w\\-_]*)?\\??(?:[-\\+=&;%@.\\w_]*)#?(?:[.\\!\\/\\\\w]*))?)')\n",
    "    ab = [re.sub(regex_url_email, '', w) for w in ab]\n",
    "    \n",
    "    # Remove references like [...], (...)\n",
    "    ab = [re.sub(r\"(\\(.*?\\)|\\[.*?\\])\", '', w) for w in ab]\n",
    "        \n",
    "    # Substitute special Symbols like &, /, ...\n",
    "    ab = [re.sub(r\"([^\\s]*)&([^\\s]*)\", r\"\\1 and \\2\", w) for w in ab]\n",
    "    ab = [re.sub(r\"([^\\s]*)\\/([^\\s]*)\", r\"\\1 or \\2\", w) for w in ab]\n",
    "    \n",
    "    # Remove duplicate spaces\n",
    "    ab = [re.sub(r\"\\s\\s+\", ' ', w) for w in ab]\n",
    "    \n",
    "    assert(len(ab) > 0)\n",
    "    log.debug(f\"First extracted abstract after RegEx preprocessing: {ab[0]}\")\n",
    "    \n",
    "    # Tokenize abstracts\n",
    "    # See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "    # Filters slightly modified (comp. to docs) to keep punctuation\n",
    "    # Lowercase has to be used for pre-trained embeddings (filters='#$%&()*+-,<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    # '\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=None, \n",
    "        filters='\"#*+<>@=~_|Â´`\\\\[]{}\\t\\n',\n",
    "        lower = False, split = ' '\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(ab)\n",
    "\n",
    "    # Generates list of lists of integers\n",
    "    # Can be reversed with the sequences_to_texts() function of the tokenizer\n",
    "    sequences = tokenizer.texts_to_sequences(ab)\n",
    "\n",
    "    assert(len(sequences) > 0)\n",
    "    log.debug(f\"First tokenized sequence: {sequences[0]}\")\n",
    "\n",
    "    # Prepare data for input to RNN\n",
    "    # Extract features and labels\n",
    "    # Number of words before prediction:\n",
    "    # random number from feature_len_range\n",
    "    # Use tuple(n, n+1) for fixed feature length n\n",
    "    feature_len_range = (60, 61)\n",
    "    features, labels = DataUtils.extract_features_and_labels(sequences, feature_len_range)\n",
    "\n",
    "    assert(len(features) > 0 and len(labels) > 0)\n",
    "    log.debug(f\"First extracted feature: {tokenizer.sequences_to_texts(features)[0]} {features[0]}\")\n",
    "    log.debug(f\"First extracted label: {tokenizer.index_word[labels[0]]} [{labels[0]}]\")\n",
    "\n",
    "    # One-hot encode data for improved training performance\n",
    "    num_code_words = len(tokenizer.index_word) + 1\n",
    "    labels_encoded = DataUtils.encode_data(labels, num_code_words)\n",
    "\n",
    "    assert(len(labels_encoded) > 0)\n",
    "    log.debug(f\"First one-hot encoded label: [0 ... {labels_encoded[0][labels[0]]} (at index {labels[0]}) ... 0]\")\n",
    "\n",
    "    # Final log for prepared data\n",
    "    log.info(f\"Loaded {labels_encoded.shape[0]} sequences\"\n",
    "             f\" with an encoded length of ~{labels_encoded.shape[1] // 8} bytes per sequence\")\n",
    "    \n",
    "    # Convert features to numpy array\n",
    "    # This is necessary for input to the RNN\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    features_training, features_validation, labels_training, labels_validation = \\\n",
    "    DataUtils.split_data(features, labels_encoded, 0.2)\n",
    "    \n",
    "    assert(len(features_training) > 0 and len(features_validation) > 0 and\n",
    "           len(labels_training) > 0 and len(labels_validation) > 0)\n",
    "    log.info(f\"Size of training data: {features_training.shape[0]} sequences\")\n",
    "    log.info(f\"Size of validation data: {features_validation.shape[0]} sequences\")\n",
    "    \n",
    "    log.info(\"Training data preparation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNnGtqZq-a4l"
   },
   "source": [
    "<a id=\"keras_create_rnn\"></a>\n",
    "### Creating the neural network\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VRHKm03T-a4l"
   },
   "outputs": [],
   "source": [
    "# Network params\n",
    "train_embedding = True\n",
    "embed_vec_size = 200\n",
    "\n",
    "num_nodes_lstm = 128\n",
    "num_nodes_dense = 128\n",
    "\n",
    "# Training params\n",
    "num_batch = 2800\n",
    "num_epochs = 150\n",
    "\n",
    "X_train = features_training\n",
    "y_train = labels_training\n",
    "X_valid = features_validation\n",
    "y_valid = labels_validation\n",
    "\n",
    "# Model save params\n",
    "model_name = \"model-baseline\"\n",
    "\n",
    "# Create directories if necessary\n",
    "model_dir = os.path.join(\"res\", \"models\")\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Filename semantics:\n",
    "# <model_name>_<train_size>_<output_dim>_<selftrained_embeddings>_<num_nodes_lstm>_<num_nodes_dense>_<num_epochs>.h5\n",
    "model_file = model_name + \"_\" + str(X_train.shape[0]) \\\n",
    "    + \"_\" + str(embed_vec_size) + \"_\" + str(int(train_embedding)) \\\n",
    "    + \"_\" + str(num_nodes_lstm) + \"_\" + str(num_nodes_dense) \\\n",
    "    + \"_\" + str(num_epochs) + \".h5\"\n",
    "\n",
    "# To make predictions later the tokenizer has to be saved\n",
    "tokenizer_dir = os.path.join(\"res\", \"tokenizers\")\n",
    "if not os.path.isdir(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "    \n",
    "with open(os.path.join(tokenizer_dir, model_file[:-3] + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "def make_model(input_dim : int,\n",
    "               embed_vec_size : int, nodes_lstm : int,\n",
    "               nodes_dense : int, dropout_lstm : float = 0.1,\n",
    "               dropout_lstm_recurrent : float = 0.1, dropout_dense : float = 0.5) -> Any:\n",
    "    \"\"\"\n",
    "    Creates a sequential Keras Model with given parameters\n",
    "    The embeddings have to be trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding layer\n",
    "    # num_code_words = number of unique words\n",
    "    # training_length we use the first num_pred words\n",
    "    model.add(\n",
    "        Embedding(input_dim = input_dim,\n",
    "                  input_length = None,\n",
    "                  output_dim = embed_vec_size,\n",
    "                  trainable = True,\n",
    "                  mask_zero = True))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(nodes_lstm, return_sequences = False, \n",
    "                   dropout = dropout_lstm, recurrent_dropout = dropout_lstm_recurrent))\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(nodes_dense, activation = 'relu'))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(input_dim, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def make_model_pretrained(input_dim : int,\n",
    "               embed_vec_size : int, embedding_matrix : np.array,\n",
    "               nodes_lstm : int, nodes_dense : int,\n",
    "               dropout_lstm : float = 0.1, dropout_lstm_recurrent : float = 0.1,\n",
    "               dropout_dense : float = 0.5) -> Any:\n",
    "    \"\"\"\n",
    "    Creates a sequential Keras Model with given parameters\n",
    "    Uses pre-trained embeddings\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Allow for variable length input\n",
    "    model.add(Input(shape = (None,)))\n",
    "    \n",
    "    # Embedding layer\n",
    "    # num_code_words numb of unique words \n",
    "    # training_length we use the first num_pred words\n",
    "    model.add(\n",
    "        Embedding(input_dim = input_dim,\n",
    "                  input_length = None,\n",
    "                  output_dim = embed_vec_size,\n",
    "                  weights = embedding_matrix,\n",
    "                  trainable = False,\n",
    "                  mask_zero = True))\n",
    "\n",
    "    # Masking layer for pre-trained embeddings\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "\n",
    "    # Recurrent layer\n",
    "    model.add(LSTM(nodes_lstm, return_sequences = False, \n",
    "                   dropout = dropout_lstm, recurrent_dropout = dropout_lstm_recurrent))\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(nodes_dense, activation = 'relu'))\n",
    "\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(input_dim, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "\n",
    "# Create Keras model\n",
    "if train_embedding:\n",
    "    model = make_model(num_code_words,\n",
    "                       embed_vec_size, num_nodes_lstm,\n",
    "                       num_nodes_dense)\n",
    "else:\n",
    "    #model = make_model_pretrained(num_pred, num_code_words,\n",
    "    #                              embed_vec_size, embedding_matrix,\n",
    "    #                              num_nodes_lstm, num_nodes_dense)\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         3838000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19190)             2475510   \n",
      "=================================================================\n",
      "Total params: 6,498,470\n",
      "Trainable params: 6,498,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_train_rnn\"></a>\n",
    "### Training the neural network\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1bwkTMh0_gO"
   },
   "outputs": [],
   "source": [
    "train_log_dir = os.path.join(\"logs\", \"training\", model_file[:-3])\n",
    "\n",
    "def make_callbacks(model_name, save=True):\n",
    "    \"\"\"\n",
    "    Creates callbacks for saving the model after each step\n",
    "    and stopping once learning process is finished\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    stopping = EarlyStopping(monitor='val_loss', patience = 5)\n",
    "    callbacks.append(stopping)\n",
    "    \n",
    "    reduce = ReduceLROnPlateau(monitor = 'loss', factor = 0.2,\n",
    "                               patience = 5, min_lr = 0.0001)\n",
    "    callbacks.append(reduce)\n",
    "    \n",
    "    board = TensorBoard(log_dir = train_log_dir, profile_batch = 0)\n",
    "    callbacks.append(board)\n",
    "    \n",
    "    if save:\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            os.path.join(model_dir, model_file),\n",
    "            save_best_only = True,\n",
    "            save_weights_only = False)\n",
    "        callbacks.append(checkpoint)\n",
    "        \n",
    "    return callbacks\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On windows use 'tensorboard.exe --logdir logs/training' instead\n",
    "%tensorboard --logdir \"logs/training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-OY4rVbEr_x",
    "outputId": "5cb776a8-2bc8-4f90-85ee-7f40f57da897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 252276 samples, validate on 63070 samples\n",
      "Epoch 1/150\n",
      "252276/252276 [==============================] - 47s 185us/sample - loss: 7.6724 - acc: 0.0435 - val_loss: 7.0044 - val_acc: 0.0552\n",
      "Epoch 2/150\n",
      "252276/252276 [==============================] - 45s 178us/sample - loss: 6.9263 - acc: 0.0545 - val_loss: 6.9276 - val_acc: 0.0552\n",
      "Epoch 3/150\n",
      "252276/252276 [==============================] - 45s 178us/sample - loss: 6.8527 - acc: 0.0573 - val_loss: 6.8944 - val_acc: 0.0621\n",
      "Epoch 4/150\n",
      "252276/252276 [==============================] - 44s 176us/sample - loss: 6.7599 - acc: 0.0674 - val_loss: 6.7742 - val_acc: 0.0776\n",
      "Epoch 5/150\n",
      "252276/252276 [==============================] - 45s 177us/sample - loss: 6.6169 - acc: 0.0807 - val_loss: 6.6511 - val_acc: 0.0840\n",
      "Epoch 6/150\n",
      "252276/252276 [==============================] - 44s 176us/sample - loss: 6.4996 - acc: 0.0896 - val_loss: 6.8007 - val_acc: 0.0780\n",
      "Epoch 7/150\n",
      "252276/252276 [==============================] - 45s 177us/sample - loss: 6.5897 - acc: 0.0915 - val_loss: 6.5779 - val_acc: 0.1009\n",
      "Epoch 8/150\n",
      "252276/252276 [==============================] - 43s 171us/sample - loss: 6.4065 - acc: 0.1027 - val_loss: 6.4869 - val_acc: 0.1086\n",
      "Epoch 9/150\n",
      "252276/252276 [==============================] - 43s 170us/sample - loss: 6.2916 - acc: 0.1108 - val_loss: 6.4062 - val_acc: 0.1165\n",
      "Epoch 10/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 6.1834 - acc: 0.1173 - val_loss: 6.3442 - val_acc: 0.1201\n",
      "Epoch 11/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 6.0939 - acc: 0.1217 - val_loss: 6.3029 - val_acc: 0.1262\n",
      "Epoch 12/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 6.0211 - acc: 0.1274 - val_loss: 6.2496 - val_acc: 0.1295\n",
      "Epoch 13/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.9478 - acc: 0.1313 - val_loss: 6.2185 - val_acc: 0.1333\n",
      "Epoch 14/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.8896 - acc: 0.1366 - val_loss: 6.1593 - val_acc: 0.1404\n",
      "Epoch 15/150\n",
      "252276/252276 [==============================] - 43s 170us/sample - loss: 5.8390 - acc: 0.1404 - val_loss: 6.1418 - val_acc: 0.1433\n",
      "Epoch 16/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 5.7930 - acc: 0.1436 - val_loss: 6.1156 - val_acc: 0.1450\n",
      "Epoch 17/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.7193 - acc: 0.1490 - val_loss: 6.0828 - val_acc: 0.1500\n",
      "Epoch 18/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.6477 - acc: 0.1554 - val_loss: 6.0498 - val_acc: 0.1534\n",
      "Epoch 19/150\n",
      "252276/252276 [==============================] - 43s 170us/sample - loss: 5.5820 - acc: 0.1612 - val_loss: 6.0063 - val_acc: 0.1574\n",
      "Epoch 20/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.5169 - acc: 0.1660 - val_loss: 5.9833 - val_acc: 0.1594\n",
      "Epoch 21/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.4716 - acc: 0.1696 - val_loss: 5.9517 - val_acc: 0.1648\n",
      "Epoch 22/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.4259 - acc: 0.1738 - val_loss: 5.9278 - val_acc: 0.1682\n",
      "Epoch 23/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 5.4140 - acc: 0.1756 - val_loss: 5.9486 - val_acc: 0.1668\n",
      "Epoch 24/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 5.4580 - acc: 0.1743 - val_loss: 5.8944 - val_acc: 0.1718\n",
      "Epoch 25/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.3461 - acc: 0.1825 - val_loss: 5.8571 - val_acc: 0.1778\n",
      "Epoch 26/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.2675 - acc: 0.1873 - val_loss: 5.8455 - val_acc: 0.1805\n",
      "Epoch 27/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.2149 - acc: 0.1902 - val_loss: 5.8219 - val_acc: 0.1818\n",
      "Epoch 28/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.1653 - acc: 0.1937 - val_loss: 5.7930 - val_acc: 0.1838\n",
      "Epoch 29/150\n",
      "252276/252276 [==============================] - 43s 170us/sample - loss: 5.1207 - acc: 0.1974 - val_loss: 5.7820 - val_acc: 0.1870\n",
      "Epoch 30/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 5.0689 - acc: 0.1997 - val_loss: 5.7616 - val_acc: 0.1881\n",
      "Epoch 31/150\n",
      "252276/252276 [==============================] - 43s 170us/sample - loss: 5.0183 - acc: 0.2027 - val_loss: 5.7420 - val_acc: 0.1905\n",
      "Epoch 32/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.9870 - acc: 0.2052 - val_loss: 5.7367 - val_acc: 0.1938\n",
      "Epoch 33/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 5.0392 - acc: 0.2041 - val_loss: 5.6749 - val_acc: 0.1954\n",
      "Epoch 34/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.9588 - acc: 0.2084 - val_loss: 5.6484 - val_acc: 0.1976\n",
      "Epoch 35/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.8994 - acc: 0.2120 - val_loss: 5.6334 - val_acc: 0.2003\n",
      "Epoch 36/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 4.8492 - acc: 0.2149 - val_loss: 5.6169 - val_acc: 0.2022\n",
      "Epoch 37/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.8060 - acc: 0.2162 - val_loss: 5.6017 - val_acc: 0.2035\n",
      "Epoch 38/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 4.7625 - acc: 0.2193 - val_loss: 5.6043 - val_acc: 0.2045\n",
      "Epoch 39/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.7178 - acc: 0.2216 - val_loss: 5.5970 - val_acc: 0.2064\n",
      "Epoch 40/150\n",
      "252276/252276 [==============================] - 43s 169us/sample - loss: 4.6830 - acc: 0.2241 - val_loss: 5.5830 - val_acc: 0.2082\n",
      "Epoch 41/150\n",
      "252276/252276 [==============================] - 42s 168us/sample - loss: 4.6437 - acc: 0.2258 - val_loss: 5.5739 - val_acc: 0.2092\n",
      "Epoch 42/150\n",
      "252276/252276 [==============================] - 42s 167us/sample - loss: 4.6076 - acc: 0.2274 - val_loss: 5.5627 - val_acc: 0.2107\n",
      "Epoch 43/150\n",
      "252276/252276 [==============================] - 41s 163us/sample - loss: 4.5712 - acc: 0.2293 - val_loss: 5.5618 - val_acc: 0.2130\n",
      "Epoch 44/150\n",
      "252276/252276 [==============================] - 41s 163us/sample - loss: 4.5383 - acc: 0.2317 - val_loss: 5.5666 - val_acc: 0.2128\n",
      "Epoch 45/150\n",
      "252276/252276 [==============================] - 41s 163us/sample - loss: 4.5035 - acc: 0.2333 - val_loss: 5.5527 - val_acc: 0.2145\n",
      "Epoch 46/150\n",
      "252276/252276 [==============================] - 41s 164us/sample - loss: 4.4726 - acc: 0.2355 - val_loss: 5.5546 - val_acc: 0.2149\n",
      "Epoch 47/150\n",
      "252276/252276 [==============================] - 41s 163us/sample - loss: 4.4402 - acc: 0.2383 - val_loss: 5.5617 - val_acc: 0.2154\n",
      "Epoch 48/150\n",
      "252276/252276 [==============================] - 41s 164us/sample - loss: 4.4103 - acc: 0.2387 - val_loss: 5.5542 - val_acc: 0.2174\n"
     ]
    }
   ],
   "source": [
    "# Train the model and save the 'best' version\n",
    "history = model.fit(X_train,  y_train,  batch_size = num_batch, \n",
    "          epochs = num_epochs, callbacks = callbacks,\n",
    "          validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_evaluate_rnn\"></a>\n",
    "### Evaluating the neural network (this will be removed in future)\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, warnings\n",
    "from tensorflow import device\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.compat.v1.logging import set_verbosity, ERROR\n",
    "\n",
    "# Suppress TF2 GPU Warnings\n",
    "set_verbosity(ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model-baseline_156805_200_1_128_128_150.h5\"\n",
    "\n",
    "model_dir = os.path.join(\"res\", \"models\")\n",
    "tokenizer_dir = os.path.join(\"res\", \"tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         2511400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12557)             1619853   \n",
      "=================================================================\n",
      "Total params: 4,316,213\n",
      "Trainable params: 4,316,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading with TF2-DirectML on GPU fails for me\n",
    "# so use CPU instead\n",
    "with device('/cpu:0'):\n",
    "    model = load_model(os.path.join(model_dir, model_file))\n",
    "\n",
    "model.summary()\n",
    "# model.evaluate(X_valid, y_valid, batch_size=4096, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = history.history['val_accuracy']\n",
    "loss = history.history['val_loss']\n",
    "epochs = range(0,num_epochs)\n",
    "plt.plot(epochs, accuracy_val, 'g', label='Accuracy')\n",
    "plt.plot(epochs, loss, 'b', label='Loss')\n",
    "plt.title('Accuracy and Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_predict_rnn\"></a>\n",
    "### Making Predictions\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.errors import UnimplementedError\n",
    "import numpy as np\n",
    "import warnings, logging\n",
    "\n",
    "# Init logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
    "log = logging.getLogger(\"Main\")\n",
    "\n",
    "# Suppress np.log divide by 0 warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "with open(os.path.join(tokenizer_dir, model_file[:-3] + \".pkl\"), \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to complete: Clustering cells and depicting the lineage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Main: Predicted sequence:\n",
      "Clustering cells and depicting the lineage\n",
      "The proposed algorithm is obtained with the corresponding clustering performance and population computational analysis is dramatically dedicated to other via supervised processing performance In addition the first sparsity based of VAE of parameters by the classification and quotient handwritten operation for misclassification or axial dataset They mixed-type the inherent filter results on the human Connectome Regularization , to a tree-based strategy .\n"
     ]
    }
   ],
   "source": [
    "# Randomness factor (in range [0, 1]\n",
    "# 1.0 means no randomness\n",
    "rand = 1.0\n",
    "\n",
    "# Timeout for prediction in seconds\n",
    "timeout = 2\n",
    "\n",
    "# Fallback prediction length\n",
    "# in case end of sentence is not predicted in time\n",
    "pred_len = 10\n",
    "\n",
    "# Get input sequence\n",
    "s = [input(\"Enter a sentence to complete: \")]\n",
    "sequences = np.array(tokenizer.texts_to_sequences(s))\n",
    "len_orig = len(sequences)\n",
    "\n",
    "# Make prediction\n",
    "predictions = []\n",
    "elem = \"\"\n",
    "t_start = time.time()\n",
    "\n",
    "# Stop on punctuation\n",
    "end_symbols = ['.', '?', '!']\n",
    "end_tokens = []\n",
    "for t in end_symbols:\n",
    "    try:\n",
    "        t = tokenizer.word_index[t]\n",
    "        end_tokens.append(t)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "while elem not in end_tokens:\n",
    "    try:\n",
    "        pred = model.predict(sequences)[0]\n",
    "    except UnimplementedError:\n",
    "        log.warning(\"Input contains words that are not in vocabulary. Prediction will be inaccurate.\")\n",
    "    \n",
    "    # Introduce random factor to prevent getting\n",
    "    # stuck in prediction loop\n",
    "    pred = np.exp(np.log(pred) / rand)\n",
    "    \n",
    "    # Issue: https://github.com/numpy/numpy/issues/8317\n",
    "    # Solution: https://stackoverflow.com/a/53605818\n",
    "    pred = np.asarray(pred).astype('float64')\n",
    "    \n",
    "    # Softmax\n",
    "    pred = pred / pred.sum()\n",
    "    \n",
    "    # Pick one word from the\n",
    "    # generated probability distribution\n",
    "    probs = np.random.multinomial(1, pred, 1)[0]\n",
    "    elem = np.argmax(probs)\n",
    "    \n",
    "    # Separately save predictions\n",
    "    predictions.append(elem)\n",
    "    \n",
    "    # Append to current sequence for new input\n",
    "    sequences = np.append(sequences, [[elem]], axis=1)\n",
    "    \n",
    "    # Timeout and fallback\n",
    "    elapsed = time.time() - t_start\n",
    "    if elapsed >= timeout:\n",
    "        log.warning(f\"Could not predict end of sentence within {timeout}s. Falling back to predicting {pred_len} words.\")\n",
    "        sequences = sequences[:len_orig + pred_len]\n",
    "        predictions = predictions[:pred_len]\n",
    "        break\n",
    "\n",
    "# Convert model output to human-readable text\n",
    "predictions = tokenizer.sequences_to_texts([predictions])[0]\n",
    "\n",
    "log.info(f\"Predicted sequence:\\n{''.join(s[0])}\\n{''.join(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_dataset\"></a>\n",
    "### Dataset comparison\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_preparation\"></a>\n",
    "### Data preparation comparison\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_rnn\"></a>\n",
    "### Network comparison\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval_conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
