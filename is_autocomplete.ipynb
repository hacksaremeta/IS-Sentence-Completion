{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hacksaremeta/IS-Sentence-Completion/blob/model/is_autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph1q75Fa-a4Z"
      },
      "source": [
        "# Sentence Completion (TUD IS Project)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/hacksaremeta/IS-Sentence-Completion/blob/datasets/is_autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Table of contents\n",
        "* 1 [Introduction (TODO)](#introduction)  \n",
        "* 2 [Training data preparation](#data_preparation)  \n",
        "    * 2.1 [DataManager class](#data_manager)  \n",
        "    * 2.2 [DataUtils class](#data_utils)  \n",
        "* 3 [Keras Implementation](#impl_keras)  \n",
        "    * 3.1 [Data preparation](#keras_preparation)  \n",
        "    * 3.2 [The neural network](#keras_rnn)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83a_WoQu-a4d"
      },
      "source": [
        "<a id=\"data_preparation\"></a>\n",
        "## Training data preparation\n",
        "\n",
        "In order to fetch data from PubMed and save it into different datasets as well as to load those datasets, some functionality is needed. This functionality will be provided by the [DataManager class](#data_manager).\n",
        "The loaded dataset then has to be prepared for training the neural network. This includes tokenization, label and feature extraction and encoding, all of which is handled by the [DataUtils class](#data_utils).\n",
        "TODO: more explanation / documentation ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ncf2XA-a4d"
      },
      "source": [
        "<a id=\"data_manager\"></a>\n",
        "### DataManager Class\n",
        "\n",
        "- Provides functionality regarding data including fetch, persistence and TF2/Keras preparation utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgNEorLo-gU_",
        "outputId": "ac24006c-ca3f-4b58-f9a9-833c62892435"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.19.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1-rHWaum-a4e"
      },
      "outputs": [],
      "source": [
        "import os, json, logging, string\n",
        "from Bio import Entrez, Medline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0ynhg2b0-a4f"
      },
      "outputs": [],
      "source": [
        "class DataManager():\n",
        "    \"\"\"Provides fetch, save and load functionality for datasets in json format\"\"\"\n",
        "    \n",
        "    def __init__(self, email, root_dir):\n",
        "        self.email = email\n",
        "        self.root_dir = root_dir\n",
        "        self.log = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def _exists_dataset(self, name):\n",
        "        \"\"\"Checks whether a dataset with the given name exists\"\"\"\n",
        "        if not os.path.isdir(self.root_dir):\n",
        "            return False\n",
        "            \n",
        "        for file in os.listdir(self.root_dir):\n",
        "            if file.endswith(\".json\"):\n",
        "                with open(os.path.join(self.root_dir, file), 'r') as f:\n",
        "                    content = json.load(f)\n",
        "                    if content[\"name\"] == name:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    def _fetch_papers(self, query : str, limit : int) -> 'list[dict]':\n",
        "        \"\"\"Retrieves data from PubMed\"\"\"\n",
        "        Entrez.email = self.email\n",
        "        record = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=limit))\n",
        "        idlist = record[\"IdList\"]\n",
        "        self.log.info(\"\\nFound %d records for %s.\" % (len(idlist), query.strip()))\n",
        "        records = Medline.parse(Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode = \"text\"))\n",
        "        return [r for r in records if \"AB\" in r]\n",
        "\n",
        "        \n",
        "    def create_dataset(self, query : str, name : str, limit=50, overwrite=False) -> None:\n",
        "        \"\"\"\n",
        "        Wraps other methods in this class\n",
        "        Creates a dataset from multiple queries\n",
        "        Does nothing if the dataset is already present (param overwrite)\n",
        "        Limits every query to <limit> results\n",
        "        \"\"\"\n",
        "        exists_dataset = self._exists_dataset(name)\n",
        "        if not exists_dataset or (exists_dataset and overwrite):\n",
        "            self.log.info(\"Dataset does not exist, fetching from PubMed...\")\n",
        "            q_data = dict()\n",
        "            q_data[\"query\"] = query\n",
        "            papers = self._fetch_papers(query, limit)\n",
        "            list_of_abstracts = [p[\"AB\"] for p in papers]\n",
        "            q_data[\"abstracts\"] = list_of_abstracts\n",
        "            res = {\"name\": name}\n",
        "            res[\"data\"] = list()\n",
        "            res[\"data\"].append(q_data)\n",
        "            self._save_dataset(res, name)\n",
        "        else:\n",
        "            self.log.info(\"Dataset already exists, skipping fetch\")\n",
        "\n",
        "    def _save_dataset(self, dataset: dict, name : str) -> None:\n",
        "        \"\"\"\n",
        "        Creates a file <name>.json in the dataset directory\n",
        "        For JSON file structure see below\n",
        "        Param dataset has a structure analogous to the JSON file\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(self.root_dir):\n",
        "            os.makedirs(self.root_dir)\n",
        "\n",
        "        with open(os.path.join(self.root_dir, name + \".json\"), 'w') as f:\n",
        "            json.dump(dataset, f, indent=2)\n",
        "        \n",
        "    def load_full_dataset(self, name : str) -> 'list[str]':\n",
        "        \"\"\"\n",
        "        Finds the file that matches given <name> in JSON information,\n",
        "        parses it, loading all abstracts into a list (one string for each abstract)\n",
        "        and returns it (Error if dataset doesn't exist)\n",
        "        \"\"\"\n",
        "\n",
        "        if  not self._exists_dataset(name):\n",
        "            self.log.info(\"Dataset does not exist\")\n",
        "            \n",
        "        else:\n",
        "           with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
        "                abstract_list=[]\n",
        "                jsonObject = json.load(file)\n",
        "                data_list= jsonObject['data']\n",
        "                for item in data_list:\n",
        "                    abstract_list.extend(item['abstracts'])\n",
        "                return abstract_list\n",
        "\n",
        "    def load_query_from_dataset(self, name : str, query : str) -> 'list[str]':\n",
        "        \"\"\"Like load_full_dataset but only loads abstracts for a single query\"\"\"\n",
        "\n",
        "\n",
        "        result = self._exists_dataset(name)\n",
        "\n",
        "        if  result:\n",
        "\n",
        "            with open(os.path.join(self.root_dir, name+'.json'), 'r') as file:\n",
        "\n",
        "                query_abstracts=[]\n",
        "                jsonObject = json.load(file)\n",
        "                data_list= jsonObject['data']\n",
        "\n",
        "                q_names = [x['query'] for x in data_list]\n",
        "\n",
        "                if query not in q_names:\n",
        "                    self.log.info(\"The Query that you are searching for,does not exist in the Dataset\")\n",
        "                else:\n",
        "\n",
        "                      for queries in data_list:\n",
        "                            if queries['query'] == query:\n",
        "                              query_abstracts.extend(queries['abstracts'])\n",
        "                              return query_abstracts\n",
        "\n",
        "        else:\n",
        "             self.log.info(\"Dataset does not exist\")\n",
        "\n",
        "\n",
        "    def remove_punctuation(self, name:str) -> 'list[str]':\n",
        "\n",
        "\n",
        "            abstracts_list= self.load_full_dataset(name)\n",
        "\n",
        "            for text in abstracts:\n",
        "\n",
        "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "                abstracts_list.append(text)\n",
        "\n",
        "\n",
        "            return  abstracts_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5byX7fyc-a4h"
      },
      "source": [
        "<a id=\"data_utils\"></a>\n",
        "### DataUtils Class\n",
        "- Static class providing utility functions to prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Vxsw3AMu-a4h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Any\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gn4elnKa-a4i"
      },
      "outputs": [],
      "source": [
        "# TODO: unify method param types (all np.array instead of list)\n",
        "class DataUtils():\n",
        "    \"\"\"Provides utility functions for data preparation\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_features_and_labels(sequences : 'list[list[Any]]', train_len : int) -> 'tuple[list[Any], list[Any]]':\n",
        "        \"\"\"\n",
        "        Extracts features of size <train_len> from the sequences\n",
        "        Also extracts every (<train_len>+1)-th word as labels\n",
        "        Returns tuple(features, labels)\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        labels = []\n",
        "        for s in sequences:\n",
        "            for i in range(train_len, len(s)):\n",
        "\n",
        "                # Extract <train_len> + 1 words and\n",
        "                # shift by 1 after each iteration\n",
        "                # That way it generates a lot of training\n",
        "                # samples from a relatively small amount of data\n",
        "                ex = s[i-train_len : i+1]\n",
        "\n",
        "                # First <train_len> words are features\n",
        "                features.append(ex[:-1])\n",
        "                \n",
        "                # (<train_len>+1)-th word is label\n",
        "                labels.append(ex[-1])\n",
        "        \n",
        "        return (features, labels)\n",
        "             \n",
        "    @staticmethod\n",
        "    def encode_data(labels: 'list[Any]', num_code_words : int) -> np.array:\n",
        "        \"\"\"\n",
        "        One-hot encode labels using numpy to\n",
        "        improve the training speed of the network\n",
        "        \"\"\"\n",
        "\n",
        "        # Use numpy for better compatibility and performance\n",
        "        # Data type: 8bit integers for binary numbers (0, 1)\n",
        "        # Could be optimized in space by using single bits instead\n",
        "        # But that adds overhead in calculation (tradeoff time - space)\n",
        "        # Since we want improved training speed we just use\n",
        "        # numpys smallest data type byte/uint8 here\n",
        "        labels_encoded = np.zeros((len(labels), num_code_words), dtype=np.uint8)\n",
        "\n",
        "        # One-hot encode\n",
        "        for i, word in enumerate(labels):\n",
        "            labels_encoded[i, word] = 1\n",
        "            \n",
        "        return labels_encoded\n",
        "    \n",
        "    # Uses Scikit-learn here; maybe replace with own method in the future\n",
        "    @staticmethod\n",
        "    def split_data(features: np.array, labels: np.array, _test_size=0.2) -> Any:\n",
        "        \"\"\"\n",
        "        Splits features and labels into training and validation data sets\n",
        "        Returns: (features_training, features_validation, labels_training, labels_validation)\n",
        "        \"\"\"\n",
        "        return train_test_split(features, labels, test_size=_test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DJfRQZSgWEU"
      },
      "source": [
        "<a id=\"impl_keras\"></a>\n",
        "## Keras Implementation (LSTM RNN)  \n",
        "For the general methodology regarding Keras neural networks see [Tensorflow Docs: Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation), [Sanchit Tanwar: Building our first neural network in keras](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5) and [Will Koehrsen: Recurrent Neural Networks by Example in Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470).\n",
        "In this case the sequences given are words instead of characters and the RNN predicts the next word.\n",
        "Therefore we use the Keras Tokenizer to convert sentences to vectors of word representatives (integers).\n",
        "After tokenization each 'word' will be converted to a feature vector using Keras pre-trained embeddings.\n",
        "Then we train the network by giving it n 'words' (features) from the PubMed training data and having it predict the (n+1)-th word (label) in the sequence.\n",
        "The predicted word is then compared to the actual word present in the training data and back-propagation is used to tweak the network layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHgVW_Ut-a4j"
      },
      "source": [
        "<a id=\"keras_preparation\"></a>\n",
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "XLfhk0cS-a4k"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Masking, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_DQrwTagWEY",
        "outputId": "ab13ecdb-60aa-48a7-c92a-1c2fcfcaee09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] DataManager: Dataset does not exist, fetching from PubMed...\n",
            "[INFO] DataManager: \n",
            "Found 5 records for clustering[ti] algorithm.\n",
            "[DEBUG] Main: First extracted abstract: The rapid emergence of novel psychoactive substances (NPS) poses new challenges and requirements for forensic testing/analysis techniques. This paper aims to explore the application of unsupervised clustering of NPS compounds' infrared spectra. Two statistical measures, Pearson and Spearman, were used to quantify the spectral similarity and to generate similarity matrices for hierarchical clustering. The correspondence of spectral similarity clustering trees to the commonly used structural/pharmacological categorization was evaluated and compared to the clustering generated using 2D/3D molecular fingerprints. Hybrid model feature selections were applied using different filter-based feature ranking algorithms developed for unsupervised clustering tasks. Since Spearman tends to overestimate the spectral similarity based on the overall pattern of the full spectrum, the clustering result shows the highest degree of improvement from having the nondiscriminative features removed. The loading plots of the first two principal components of the optimal feature subsets confirmed that the most important vibrational bands contributing to the clustering of NPS compounds were selected using non-negative discriminative feature selection (NDFS) algorithms.\n",
            "[DEBUG] Main: First tokenized sequence: [1, 178, 179, 2, 180, 181, 182, 55, 96, 56, 97, 3, 183, 8, 184, 468, 469, 31, 185, 186, 4, 98, 1, 99, 2, 51, 9, 2, 55, 470, 187, 471, 38, 100, 188, 189, 3, 101, 6, 52, 4, 190, 1, 39, 40, 3, 4, 191, 40, 192, 8, 102, 472, 1, 193, 2, 39, 40, 9, 103, 4, 1, 161, 52, 473, 194, 18, 57, 3, 58, 4, 1, 9, 195, 23, 474, 59, 475, 196, 162, 24, 197, 6, 60, 23, 61, 163, 36, 24, 198, 13, 41, 8, 51, 9, 476, 199, 101, 200, 4, 201, 1, 39, 40, 36, 42, 1, 202, 203, 2, 1, 104, 204, 1, 9, 205, 206, 1, 62, 207, 2, 208, 25, 209, 1, 210, 105, 477, 1, 211, 212, 2, 1, 213, 38, 214, 215, 2, 1, 216, 24, 217, 218, 14, 1, 106, 63, 219, 220, 107, 4, 1, 9, 2, 55, 164, 6, 108, 23, 221, 165, 222, 24, 64, 223, 109]\n",
            "[DEBUG] Main: First extracted feature: the rapid emergence of novel psychoactive substances nps poses new challenges and requirements for forensic testing/analysis techniques. this paper aims [1, 178, 179, 2, 180, 181, 182, 55, 96, 56, 97, 3, 183, 8, 184, 468, 469, 31, 185, 186]\n",
            "[DEBUG] Main: First extracted label: to [4]\n",
            "[DEBUG] Main: First one-hot encoded label: [0 ... 1 (at index 4) ... 0]\n",
            "[INFO] Main: Loaded 2166 sequences with an encoded length of ~72 bytes per sequence\n",
            "[INFO] Main: Size of training data: 1949 sequences\n",
            "[INFO] Main: Size of validation data: 217 sequences\n",
            "[INFO] Main: Training data preparation finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITEM {'query': 'clustering[ti] algorithm', 'abstracts': [\"The rapid emergence of novel psychoactive substances (NPS) poses new challenges and requirements for forensic testing/analysis techniques. This paper aims to explore the application of unsupervised clustering of NPS compounds' infrared spectra. Two statistical measures, Pearson and Spearman, were used to quantify the spectral similarity and to generate similarity matrices for hierarchical clustering. The correspondence of spectral similarity clustering trees to the commonly used structural/pharmacological categorization was evaluated and compared to the clustering generated using 2D/3D molecular fingerprints. Hybrid model feature selections were applied using different filter-based feature ranking algorithms developed for unsupervised clustering tasks. Since Spearman tends to overestimate the spectral similarity based on the overall pattern of the full spectrum, the clustering result shows the highest degree of improvement from having the nondiscriminative features removed. The loading plots of the first two principal components of the optimal feature subsets confirmed that the most important vibrational bands contributing to the clustering of NPS compounds were selected using non-negative discriminative feature selection (NDFS) algorithms.\", 'Under the new trend of industry 4.0 software-defined network, the value of meta heuristic algorithm was explored in the recognition of depression in patients with androgenic alopecia (AGA), and there was an analysis on the effect of comprehensive psychological interventions in the rehabilitation of AGA patients. Based on the meta heuristic algorithm, the Filter and Wrapper algorithms were combined in this study to form a new feature selection algorithm FAW-FS. Then, the classification accuracy of FAW-FS and the ability to identify depression disorders were verified under different open data sets. 54 patients with AGA who went to the Medical Cosmetic Center of Tongji Hospital were selected as the research objects and rolled into a control group (routine psychological intervention) and an intervention group (routine + comprehensive psychological interventions) according to different psychological intervention methods, with 27 cases in each group. The differences of the self-rating anxiety scale (SAS), self-rating depression scale (SDS), Hamilton depression scale (HAMD), and physical, psychological, social, and substance function scores before and after intervention were compared between the two groups of AGA patients, and the depression efficacy and compliance of the two groups were analyzed after intervention. The results showed that the classification accuracy of FAW-FS algorithm was the highest in logistic regression (LR), decision tree (DT), K-nearest neighbor (KNN) algorithm, support vector machine (SVM) algorithm, and random forest (RF) algorithm, which was 80.87, 79.24, 80.42, 83.07, and 81.45%, respectively. The LR algorithm had the highest feature selection accuracy of 82.94%, and the classification accuracy of depression disorder in RF algorithm was up to 73.01%. Besides, the SDS, SAS, and HAMD scores of the intervention group were lower sharply than the scores of the control group (p < 0.05). The physical function, psychological function, social function, and substance function scores of the intervention group were higher markedly than those of the control group (p < 0.05). In addition, the proportions of cured, markedly effective, total effective, full compliance, and total compliance patients in the intervention group increased obviously in contrast to the proportions of the control group (p < 0.05). Therefore, it indicated that the FAW-FS algorithm established in this study had significant advantages in the recognition of depression in AGA patients, and comprehensive psychological intervention had a positive effect in the rehabilitation of depression in AGA patients.', \"Studying the molecular development of the human brain presents unique challenges for selecting a data analysis approach. The rare and valuable nature of human postmortem brain tissue, especially for developmental studies, means the sample sizes are small (n), but the use of high throughput genomic and proteomic methods measure the expression levels for hundreds or thousands of variables [e.g., genes or proteins (p)] for each sample. This leads to a data structure that is high dimensional (p >> n) and introduces the curse of dimensionality, which poses a challenge for traditional statistical approaches. In contrast, high dimensional analyses, especially cluster analyses developed for sparse data, have worked well for analyzing genomic datasets where p >> n. Here we explore applying a lasso-based clustering method developed for high dimensional genomic data with small sample sizes. Using protein and gene data from the developing human visual cortex, we compared clustering methods. We identified an application of sparse k-means clustering [robust sparse k-means clustering (RSKC)] that partitioned samples into age-related clusters that reflect lifespan stages from birth to aging. RSKC adaptively selects a subset of the genes or proteins contributing to partitioning samples into age-related clusters that progress across the lifespan. This approach addresses a problem in current studies that could not identify multiple postnatal clusters. Moreover, clusters encompassed a range of ages like a series of overlapping waves illustrating that chronological- and brain-age have a complex relationship. In addition, a recently developed workflow to create plasticity phenotypes (Balsor et al., 2020) was applied to the clusters and revealed neurobiologically relevant features that identified how the human visual cortex changes across the lifespan. These methods can help address the growing demand for multimodal integration, from molecular machinery to brain imaging signals, to understand the human brain's development.\", 'BACKGROUND: Heterogeneity in Acute Respiratory Distress Syndrome (ARDS), as a consequence of its non-specific definition, has led to a multitude of negative randomised controlled trials (RCTs). Investigators have sought to identify heterogeneity of treatment effect (HTE) in RCTs using clustering algorithms. We evaluated the proficiency of several commonly-used machine-learning algorithms to identify clusters where HTE may be detected. METHODS: Five unsupervised: Latent class analysis (LCA), K-means, partition around medoids, hierarchical, and spectral clustering; and four supervised algorithms: model-based recursive partitioning, Causal Forest (CF), and X-learner with Random Forest (XL-RF) and Bayesian Additive Regression Trees were individually applied to three prior ARDS RCTs. Clinical data and research protein biomarkers were used as partitioning variables, with the latter excluded for secondary analyses. For a clustering schema, HTE was evaluated based on the interaction term of treatment group and cluster with day-90 mortality as the dependent variable. FINDINGS: No single algorithm identified clusters with significant HTE in all three trials. LCA, XL-RF, and CF identified HTE most frequently (2/3 RCTs). Important partitioning variables in the unsupervised approaches were consistent across algorithms and RCTs. In supervised models, important partitioning variables varied between algorithms and across RCTs. In algorithms where clusters demonstrated HTE in the same trial, patients frequently interchanged clusters from treatment-benefit to treatment-harm clusters across algorithms. LCA aside, results from all other algorithms were subject to significant alteration in cluster composition and HTE with random seed change. Removing research biomarkers as partitioning variables greatly reduced the chances of detecting HTE across all algorithms. INTERPRETATION: Machine-learning algorithms were inconsistent in their abilities to identify clusters with significant HTE. Protein biomarkers were essential in identifying clusters with HTE. Investigations using machine-learning approaches to identify clusters to seek HTE require cautious interpretation. FUNDING: NIGMS R35 GM142992 (PS), NHLBI R35 HL140026 (CSC); NIGMS R01 GM123193, Department of Defense W81XWH-21-1-0009, NIA R21 AG068720, NIDA R01 DA051464 (MMC).']}\n",
            "ITEM {'query': 'clustering[ti] algorithm', 'abstracts': [\"The rapid emergence of novel psychoactive substances (NPS) poses new challenges and requirements for forensic testing/analysis techniques. This paper aims to explore the application of unsupervised clustering of NPS compounds' infrared spectra. Two statistical measures, Pearson and Spearman, were used to quantify the spectral similarity and to generate similarity matrices for hierarchical clustering. The correspondence of spectral similarity clustering trees to the commonly used structural/pharmacological categorization was evaluated and compared to the clustering generated using 2D/3D molecular fingerprints. Hybrid model feature selections were applied using different filter-based feature ranking algorithms developed for unsupervised clustering tasks. Since Spearman tends to overestimate the spectral similarity based on the overall pattern of the full spectrum, the clustering result shows the highest degree of improvement from having the nondiscriminative features removed. The loading plots of the first two principal components of the optimal feature subsets confirmed that the most important vibrational bands contributing to the clustering of NPS compounds were selected using non-negative discriminative feature selection (NDFS) algorithms.\", 'Under the new trend of industry 4.0 software-defined network, the value of meta heuristic algorithm was explored in the recognition of depression in patients with androgenic alopecia (AGA), and there was an analysis on the effect of comprehensive psychological interventions in the rehabilitation of AGA patients. Based on the meta heuristic algorithm, the Filter and Wrapper algorithms were combined in this study to form a new feature selection algorithm FAW-FS. Then, the classification accuracy of FAW-FS and the ability to identify depression disorders were verified under different open data sets. 54 patients with AGA who went to the Medical Cosmetic Center of Tongji Hospital were selected as the research objects and rolled into a control group (routine psychological intervention) and an intervention group (routine + comprehensive psychological interventions) according to different psychological intervention methods, with 27 cases in each group. The differences of the self-rating anxiety scale (SAS), self-rating depression scale (SDS), Hamilton depression scale (HAMD), and physical, psychological, social, and substance function scores before and after intervention were compared between the two groups of AGA patients, and the depression efficacy and compliance of the two groups were analyzed after intervention. The results showed that the classification accuracy of FAW-FS algorithm was the highest in logistic regression (LR), decision tree (DT), K-nearest neighbor (KNN) algorithm, support vector machine (SVM) algorithm, and random forest (RF) algorithm, which was 80.87, 79.24, 80.42, 83.07, and 81.45%, respectively. The LR algorithm had the highest feature selection accuracy of 82.94%, and the classification accuracy of depression disorder in RF algorithm was up to 73.01%. Besides, the SDS, SAS, and HAMD scores of the intervention group were lower sharply than the scores of the control group (p < 0.05). The physical function, psychological function, social function, and substance function scores of the intervention group were higher markedly than those of the control group (p < 0.05). In addition, the proportions of cured, markedly effective, total effective, full compliance, and total compliance patients in the intervention group increased obviously in contrast to the proportions of the control group (p < 0.05). Therefore, it indicated that the FAW-FS algorithm established in this study had significant advantages in the recognition of depression in AGA patients, and comprehensive psychological intervention had a positive effect in the rehabilitation of depression in AGA patients.', \"Studying the molecular development of the human brain presents unique challenges for selecting a data analysis approach. The rare and valuable nature of human postmortem brain tissue, especially for developmental studies, means the sample sizes are small (n), but the use of high throughput genomic and proteomic methods measure the expression levels for hundreds or thousands of variables [e.g., genes or proteins (p)] for each sample. This leads to a data structure that is high dimensional (p >> n) and introduces the curse of dimensionality, which poses a challenge for traditional statistical approaches. In contrast, high dimensional analyses, especially cluster analyses developed for sparse data, have worked well for analyzing genomic datasets where p >> n. Here we explore applying a lasso-based clustering method developed for high dimensional genomic data with small sample sizes. Using protein and gene data from the developing human visual cortex, we compared clustering methods. We identified an application of sparse k-means clustering [robust sparse k-means clustering (RSKC)] that partitioned samples into age-related clusters that reflect lifespan stages from birth to aging. RSKC adaptively selects a subset of the genes or proteins contributing to partitioning samples into age-related clusters that progress across the lifespan. This approach addresses a problem in current studies that could not identify multiple postnatal clusters. Moreover, clusters encompassed a range of ages like a series of overlapping waves illustrating that chronological- and brain-age have a complex relationship. In addition, a recently developed workflow to create plasticity phenotypes (Balsor et al., 2020) was applied to the clusters and revealed neurobiologically relevant features that identified how the human visual cortex changes across the lifespan. These methods can help address the growing demand for multimodal integration, from molecular machinery to brain imaging signals, to understand the human brain's development.\", 'BACKGROUND: Heterogeneity in Acute Respiratory Distress Syndrome (ARDS), as a consequence of its non-specific definition, has led to a multitude of negative randomised controlled trials (RCTs). Investigators have sought to identify heterogeneity of treatment effect (HTE) in RCTs using clustering algorithms. We evaluated the proficiency of several commonly-used machine-learning algorithms to identify clusters where HTE may be detected. METHODS: Five unsupervised: Latent class analysis (LCA), K-means, partition around medoids, hierarchical, and spectral clustering; and four supervised algorithms: model-based recursive partitioning, Causal Forest (CF), and X-learner with Random Forest (XL-RF) and Bayesian Additive Regression Trees were individually applied to three prior ARDS RCTs. Clinical data and research protein biomarkers were used as partitioning variables, with the latter excluded for secondary analyses. For a clustering schema, HTE was evaluated based on the interaction term of treatment group and cluster with day-90 mortality as the dependent variable. FINDINGS: No single algorithm identified clusters with significant HTE in all three trials. LCA, XL-RF, and CF identified HTE most frequently (2/3 RCTs). Important partitioning variables in the unsupervised approaches were consistent across algorithms and RCTs. In supervised models, important partitioning variables varied between algorithms and across RCTs. In algorithms where clusters demonstrated HTE in the same trial, patients frequently interchanged clusters from treatment-benefit to treatment-harm clusters across algorithms. LCA aside, results from all other algorithms were subject to significant alteration in cluster composition and HTE with random seed change. Removing research biomarkers as partitioning variables greatly reduced the chances of detecting HTE across all algorithms. INTERPRETATION: Machine-learning algorithms were inconsistent in their abilities to identify clusters with significant HTE. Protein biomarkers were essential in identifying clusters with HTE. Investigations using machine-learning approaches to identify clusters to seek HTE require cautious interpretation. FUNDING: NIGMS R35 GM142992 (PS), NHLBI R35 HL140026 (CSC); NIGMS R01 GM123193, Department of Defense W81XWH-21-1-0009, NIA R21 AG068720, NIDA R01 DA051464 (MMC).']}\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Init logging\n",
        "    logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
        "    log = logging.getLogger(\"Main\")\n",
        "\n",
        "    # Create DataManager in '../res/datasets' folder\n",
        "    data_folder = os.path.join(\"..\", \"res\", \"datasets\")\n",
        "    dman = DataManager(\"mymail@example.com\", data_folder)\n",
        "\n",
        "\n",
        "    query = \"clustering[ti] algorithm\"\n",
        "    dataset_name = f\"{query} Dataset\"\n",
        "\n",
        "    # Gather maximum of 100 abstracts for each query\n",
        "    # I would suggest around 5 - 20 abstracts in total for the small data sets\n",
        "    # and maybe 500 - 5000 for the final ones but we'll have to test\n",
        "    # since that depends on how long it takes to train the network\n",
        "    # This only queries PubMed if data if the data is not already present\n",
        "\n",
        "    records = 5\n",
        "    dman.create_dataset(query, dataset_name, records, overwrite=True)\n",
        "\n",
        "    # Load the dataset\n",
        "    abstracts = dman.load_full_dataset(dataset_name)\n",
        "    abstracts_mrna = dman.load_query_from_dataset(dataset_name, query)\n",
        "\n",
        "    ab = dman.remove_punctuation(dataset_name)\n",
        "\n",
        "    assert(len(ab) > 0)\n",
        "    log.debug(f\"First extracted abstract: {ab[0]}\")\n",
        "\n",
        "    # Tokenize abstracts\n",
        "    # See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "    # Filters slightly modified (comp. to docs) to keep punctuation\n",
        "    # Lowercase has to be used for pre-trained embeddings\n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=None, \n",
        "        filters='#$%&()*+-,<=>@[\\\\]^_`{|}~\\t\\n',\n",
        "        lower = True, split = ' '\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(ab)\n",
        "\n",
        "    # Generates list of lists of integers\n",
        "    # Can be reversed with the sequences_to_texts() function of the tokenizer\n",
        "    sequences = tokenizer.texts_to_sequences(ab)\n",
        "\n",
        "    assert(len(sequences) > 0)\n",
        "    log.debug(f\"First tokenized sequence: {sequences[0]}\")\n",
        "\n",
        "    # Prepare data for input to RNN\n",
        "    # Extract features and labels\n",
        "    # Number of words before prediction: num_pred\n",
        "    num_pred = 20\n",
        "    features, labels = DataUtils.extract_features_and_labels(sequences, 20)\n",
        "\n",
        "    assert(len(features) > 0 and len(labels) > 0)\n",
        "    log.debug(f\"First extracted feature: {tokenizer.sequences_to_texts(features)[0]} {features[0]}\")\n",
        "    log.debug(f\"First extracted label: {tokenizer.index_word[labels[0]]} [{labels[0]}]\")\n",
        "\n",
        "    # One-hot encode data for improved training performance\n",
        "    num_code_words = len(tokenizer.index_word) + 1\n",
        "    labels_encoded = DataUtils.encode_data(labels, num_code_words)\n",
        "    \n",
        "    assert(len(labels_encoded) > 0)\n",
        "    log.debug(f\"First one-hot encoded label: [0 ... {labels_encoded[0][labels[0]]} (at index {labels[0]}) ... 0]\")\n",
        "\n",
        "    # Final log for prepared data\n",
        "    log.info(f\"Loaded {labels_encoded.shape[0]} sequences\"\n",
        "             f\" with an encoded length of ~{labels_encoded.shape[1] // 8} bytes per sequence\")\n",
        "    \n",
        "    # Convert features to numpy array\n",
        "    # This is necessary for input to the RNN\n",
        "    features = np.array(features)\n",
        "    \n",
        "    # Split dataset into training and validation sets\n",
        "    features_training, features_validation, labels_training, labels_validation = \\\n",
        "    DataUtils.split_data(features, labels_encoded, 0.1)\n",
        "    \n",
        "    assert(len(features_training) > 0 and len(features_validation) > 0 and\n",
        "           len(labels_training) > 0 and len(labels_validation) > 0)\n",
        "    log.info(f\"Size of training data: {features_training.shape[0]} sequences\")\n",
        "    log.info(f\"Size of validation data: {features_validation.shape[0]} sequences\")\n",
        "    \n",
        "    log.info(\"Training data preparation finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNnGtqZq-a4l"
      },
      "source": [
        "<a id=\"keras_rnn\"></a>\n",
        "### The neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRHKm03T-a4l"
      },
      "outputs": [],
      "source": [
        "# TODO: create neural network using Keras\n",
        "pass\n",
        "\n",
        "\n",
        "# Training the model\n",
        "# history = model.fit(features_training, labels_training,\n",
        "# validation_data=(features_validation, labels_validation), epochs=100, batch_size=64)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "# num_code_words numb of unique words \n",
        "# training_length we use the first 50 words\n",
        "training_length = num_pred\n",
        "model.add(\n",
        "    Embedding(input_dim=num_code_words,\n",
        "              input_length = training_length,\n",
        "              output_dim=labels_encoded.shape[1],\n",
        "              # weights=[labels_encoded],\n",
        "              trainable=False,\n",
        "              mask_zero=True))\n",
        "\n",
        "# Masking layer for pre-trained embeddings\n",
        "model.add(Masking(mask_value=0.0))\n",
        "\n",
        "# Recurrent layer\n",
        "model.add(LSTM(64, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_code_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO callfunktion if doesnt learn in 5 epoch stop"
      ],
      "metadata": {
        "id": "A1bwkTMh0_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = features_training\n",
        "y_train = labels_training\n",
        "X_valid = features_validation\n",
        "y_valid = labels_validation\n",
        "# save the best version of the model \n",
        "history = model.fit(X_train,  y_train,  batch_size=2048, \n",
        "          epochs=10, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "id": "q-OY4rVbEr_x",
        "outputId": "5cb776a8-2bc8-4f90-85ee-7f40f57da897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 4.8607 - accuracy: 0.0410 - val_loss: 5.3112 - val_accuracy: 0.0325\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 1s 742ms/step - loss: 4.8607 - accuracy: 0.0483 - val_loss: 5.2974 - val_accuracy: 0.0325\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 1s 737ms/step - loss: 4.8534 - accuracy: 0.0501 - val_loss: 5.2744 - val_accuracy: 0.0325\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 4.8683 - accuracy: 0.0510 - val_loss: 5.2551 - val_accuracy: 0.0325\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.8376 - accuracy: 0.0401 - val_loss: 5.2463 - val_accuracy: 0.0325\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 1s 739ms/step - loss: 4.8292 - accuracy: 0.0446 - val_loss: 5.2462 - val_accuracy: 0.0325\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 1s 716ms/step - loss: 4.8225 - accuracy: 0.0446 - val_loss: 5.2393 - val_accuracy: 0.0407\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.8019 - accuracy: 0.0483 - val_loss: 5.2270 - val_accuracy: 0.0407\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 1s 729ms/step - loss: 4.7852 - accuracy: 0.0528 - val_loss: 5.2092 - val_accuracy: 0.0325\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.7810 - accuracy: 0.0455 - val_loss: 5.2002 - val_accuracy: 0.0325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "3jc0a16tHXM0",
        "outputId": "2760dea0-dd07-485e-fa13-a9c813a0b8b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.03369763121008873,\n",
              "  0.04553734138607979,\n",
              "  0.0346083790063858,\n",
              "  0.0382513664662838,\n",
              "  0.040072858333587646,\n",
              "  0.03916211426258087,\n",
              "  0.03916211426258087,\n",
              "  0.03642987087368965,\n",
              "  0.05009107291698456,\n",
              "  0.04553734138607979,\n",
              "  0.04098360612988472,\n",
              "  0.04918032884597778,\n",
              "  0.04189435392618179,\n",
              "  0.04098360612988472,\n",
              "  0.03916211426258087,\n",
              "  0.04826958104968071,\n",
              "  0.04553734138607979,\n",
              "  0.04826958104968071,\n",
              "  0.044626593589782715,\n",
              "  0.05282331630587578],\n",
              " 'loss': [5.067013740539551,\n",
              "  5.077101230621338,\n",
              "  5.047342777252197,\n",
              "  5.055200099945068,\n",
              "  5.047453880310059,\n",
              "  5.054710865020752,\n",
              "  5.061315536499023,\n",
              "  5.039441108703613,\n",
              "  5.026025295257568,\n",
              "  5.029038429260254,\n",
              "  5.018542766571045,\n",
              "  5.003678798675537,\n",
              "  4.992433071136475,\n",
              "  4.996974468231201,\n",
              "  4.985339641571045,\n",
              "  4.978785037994385,\n",
              "  4.976564884185791,\n",
              "  4.986665725708008,\n",
              "  4.970412254333496,\n",
              "  4.951289176940918],\n",
              " 'val_accuracy': [0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.056910570710897446,\n",
              "  0.04878048598766327,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04065040498971939,\n",
              "  0.04878048598766327,\n",
              "  0.04878048598766327,\n",
              "  0.04878048598766327,\n",
              "  0.04878048598766327],\n",
              " 'val_loss': [5.212522506713867,\n",
              "  5.201118469238281,\n",
              "  5.189680099487305,\n",
              "  5.179745674133301,\n",
              "  5.171322822570801,\n",
              "  5.16370964050293,\n",
              "  5.157140254974365,\n",
              "  5.151780128479004,\n",
              "  5.147400379180908,\n",
              "  5.142074108123779,\n",
              "  5.133703231811523,\n",
              "  5.123533248901367,\n",
              "  5.113071918487549,\n",
              "  5.103494644165039,\n",
              "  5.095618724822998,\n",
              "  5.089694023132324,\n",
              "  5.085237979888916,\n",
              "  5.077899932861328,\n",
              "  5.069307327270508,\n",
              "  5.063593864440918]}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}