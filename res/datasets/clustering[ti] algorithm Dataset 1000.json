{
  "name": "clustering[ti] algorithm Dataset 1000",
  "data": [
    {
      "query": "clustering[ti] algorithm",
      "abstracts": [
        "In this research study, we first define the strong degree of a vertex in an m-polar fuzzy graph. Then we present various useful properties and prove some results concerning this new concept, in the case of complete m-polar fuzzy graphs. Further, we introduce the concept of m-polar fuzzy strength sequence of vertices, and we also investigate it in the particular instance of complete m-polar fuzzy graphs. We discuss connectivity parameters in m-polar fuzzy graphs with precise examples, and we investigate the m-polar fuzzy analogue of Whitney's theorem. Furthermore, we present a clustering method for vertices in an m-polar fuzzy graph based on the strength of connectedness between pairs of vertices. In order to formulate this method, we introduce terminologies such as A-reachable vertices in m-polar fuzzy graphs, A-connected m-polar fuzzy graphs, or A-connected m-polar fuzzy subgraphs (in case the m-polar fuzzy graph itself is not A-connected). Moreover, we discuss an application for clustering different companies in consideration of their multi-polar uncertain information. We then provide an algorithm to clearly understand the clustering methodology that we use in our application. Finally, we present a comparative analysis of our research work with existing techniques to prove its applicability and effectiveness.",
        "Markov models are extensively used for categorical sequence clustering and classification due to their inherent ability to capture complex chronological dependencies hidden in sequential data. Existing Markov models are based on an implicit assumption that the probability of the next state depends on the preceding context/pattern which is consist of consecutive states. This restriction hampers the models since some patterns, disrupted by noise, may be not frequent enough in a consecutive form, but frequent in a sparse form, which can not make use of the information hidden in the sequential data. A sparse pattern corresponds to a pattern in which one or some of the state(s) between the first and last one in the pattern is/are replaced by wildcard(s) that can be matched by a subset of values in the state set. In this paper, we propose a new model that generalizes the conventional Markov approach making it capable of dealing with the sparse pattern and handling the length of the sparse patterns adaptively, i.e. allowing variable length pattern with variable wildcards. The model, named Dynamic order Markov model (DOMM), allows deriving a new similarity measure between a sequence and a set of sequences/cluster. DOMM builds a sparse pattern from sub-frequent patterns that contain significant statistical information veiled by the noise. To implement DOMM, we propose a sparse pattern detector (SPD) based on the probability suffix tree (PST) capable of discovering both sparse and consecutive patterns, and then we develop a divisive clustering algorithm, named DMSC, for Dynamic order Markov model for categorical sequence clustering. Experimental results on real-world datasets demonstrate the promising performance of the proposed model.",
        "[This corrects the article DOI: 10.3389/fgene.2021.689515.].",
        "This paper presents a spike sorting processor based on an accurate spike clustering algorithm. The proposed spike sorting algorithm employs an L2-normalized convolutional autoencoder to extract features from the input, where the autoencoder is trained using the proposed spike sorting-aware loss. In addition, we propose a similarity-based K-means clustering algorithm that conditionally updates the means by observing the cosine similarity. The modified K-means algorithm exhibits better convergence and enables online clustering with higher classification accuracy. We implement a spike sorting processor based on the proposed algorithm using an efficient time-multiplexed hardware architecture in a 40-nm CMOS process. Experimental results show that the processor consumes 224.75W/mm(2) when processing 16 input channels at 7.68MHz and 0.55V. Our design achieves 95.54% clustering accuracy, outperforming prior spike sorting processor designs.",
        "We present an automatic algorithm for the group-wise parcellation of the cortical surface. The method is based on the structural connectivity obtained from representative brain fiber clusters, calculated via an inter-subject clustering scheme. Preliminary regions were defined from cluster-cortical mesh intersection points. The final parcellation was obtained using parcel probability maps to model and integrate the connectivity information of all subjects, and graphs to represent the overlap between parcels. Two inter-subject clustering schemes were tested, generating a total of 171 and 109 parcels, respectively. The resulting parcels were quantitatively compared with three state-of-the-art atlases. The best parcellation returned 69 parcels with a Dice similarity coefficient greater than 0.5. To the best of our knowledge, this is the first diffusion-based cortex parcellation method based on whole-brain inter-subject fiber clustering.",
        "The prediction at baseline of patients at high risk for therapy failure or recurrence would significantly impact on Hodgkin Lymphoma patients treatment, informing clinical practice. Current literature is extensively searching insights in radiomics, a promising framework for high-throughput imaging feature extraction, to derive biomarkers and quantitative prognostic factors from images. However, existing studies are limited by intrinsic radiomic limitations, high dimensionality among others. We propose an exhaustive patient representation and a recurrence-specific multi-view supervised clustering algorithm for estimating patient-to-patient similarity graph and learning recurrence probability. We stratified patients in two risk classes and characterize each group in terms of clinical variables.",
        "Modern advancements have allowed society to be at the most innovative stages of technology which involves the possibility of multimodal data collection. Dartmouth dataset is a rich dataset collected over 10 weeks from 60 participants. The dataset includes different types of data but this paper focuses on 10 different smartphone sensor data and a Patient Health Questionnaire (PHQ) 9 survey that monitors the severity of depression. This paper extracts key features from smartphone data to identify depression. A multi-view bi-clustering (MVBC) algorithm is applied to categorize homogeneous behaviour subgroups. MVBC takes multiple views of sensing data as input. The algorithm inputs three views: average, trend, and location views. MVBC categorizes the subjects to low, medium and high PHQ-9 scores. Real-world data collection may have fewer sensors, allowing for less features to be extracted. This creates a focus on prioritization of features. In this body of work, minimum redundancy maximum relevance (mRMR) is applied to the sensing features to prioritize the features that better distinguish the different groups. The resulting MVBC are compared to literature to support the categorized clusters. Decision Tree (DT) 10-fold cross validation shows that our method can classify individuals into the correct subgroups using a reduced number of features to achieve an overall accuracy of 94.7+/-1.62%. Achieving high accuracies with reduced features allows for focus on low power analysis and edge computing applications for long-term mental health monitoring using a smartphone.",
        "Multi-view subspace clustering has attracted intensive attention to effectively fuse multi-view information by exploring appropriate graph structures. Although existing works have made impressive progress in clustering performance, most of them suffer from the cubic time complexity which could prevent them from being efficiently applied into large-scale applications. To improve the efficiency, anchor sampling mechanism has been proposed to select vital landmarks to represent the whole data. However, existing anchor selecting usually follows the heuristic sampling strategy, e.g. k-means or uniform sampling. As a result, the procedures of anchor selecting and subsequent subspace graph construction are separated from each other which may adversely affect clustering performance. Moreover, the involved hyper-parameters further limit the application of traditional algorithms. To address these issues, we propose a novel subspace clustering method termed Fast Parameter-free Multi-view Subspace Clustering with Consensus Anchor Guidance (FPMVS-CAG). Firstly, we jointly conduct anchor selection and subspace graph construction into a unified optimization formulation. By this way, the two processes can be negotiated with each other to promote clustering quality. Moreover, our proposed FPMVS-CAG is proved to have linear time complexity with respect to the sample number. In addition, FPMVS-CAG can automatically learn an optimal anchor subspace graph without any extra hyper-parameters. Extensive experimental results on various benchmark datasets demonstrate the effectiveness and efficiency of the proposed method against the existing state-of-the-art multi-view subspace clustering competitors. These merits make FPMVS-CAG more suitable for large-scale subspace clustering. The code of FPMVS-CAG is publicly available at https://github.com/wangsiwei2010/FPMVS-CAG.",
        "Background: To develop a fuzzy clustering neural network to predict radiation-induced pneumonitis (RP) using four-dimensional computed tomography (4DCT) ventilation image (VI) based dosimetric parameters for thoracic cancer patients. Methods: The VI were retrospectively calculated from pre-treatment 4DCT data using a deformable image registration (DIR) and an improved VI algorithm. Similar to dose-volume histogram (DVH) of intensity modulated radiotherapy (IMRT), dose-function histogram (DFH) was derived from dose distribution and VI. Then, the dose-function metrics were calculated from DFH. For comparison, the dose-volume metrics were calculated from DVH. Correspondingly, two sets of feature vectors were formed from the dose-volume metrics and the dose-function metrics, respectively. For the feature vectors of each set, they were first pre-processed by principal component analysis (PCA) to reduce feature dimensions. Then, they were grouped to few clusters determined by fuzzy c-means (FCM) algorithm. Next, the neural network was trained to correlate the dosimetric parameters with RP based on the feature vectors of each cluster. Finally, the occurrence of RP was predicted by the neural network on the test data. Results: Through PCA analysis, the top 5 principal components were selected. Their contribution is more than 98%, which is adequate to represent the original feature space of input data. Based on the clustering validity indexes, the optimal number of clusters is 4 and used for subsequent fuzzy clustering of the input data. After network training, the areas under the curve (AUC) of the prediction model is 0.77 using VI-based dosimetric parameters and 0.67 using structure-based dosimetric parameters. Conclusions: Compared to the structure-based dosimetric features, the VI-based dosimetric features are more relevant to lung function and presented higher prediction accuracy of RP. The fuzzy clustering neural network improved the prediction accuracy of RP compared to the conventional neural network. The combination of VI-based dose-function metrics and fuzzy clustering neural network provides an effective predictive model for assessing lung toxicity risk after radiotherapy.",
        "At present, learning-based citrus blossom recognition models based on deep learning are highly complicated and have a large number of parameters. In order to estimate citrus flower quantities in natural orchards, this study proposes a lightweight citrus flower recognition model based on improved YOLOv4. In order to compress the backbone network, we utilize MobileNetv3 as a feature extractor, combined with deep separable convolution for further acceleration. The Cutout data enhancement method is also introduced to simulate citrus in nature for data enhancement. The test results show that the improved model has an mAP of 84.84%, 22% smaller than that of YOLOv4, and approximately two times faster. Compared with the Faster R-CNN, the improved citrus flower rate statistical model proposed in this study has the advantages of less memory usage and fast detection speed under the premise of ensuring a certain accuracy. Therefore, our solution can be used as a reference for the edge detection of citrus flowering.",
        "How can we efficiently find very large numbers of clusters C in very large datasets N of potentially high dimensionality D Here we address the question by using a novel variational approach to optimize Gaussian mixture models (GMMs) with diagonal covariance matrices. The variational method approximates expectation maximization (EM) by applying truncated posteriors as variational distributions and partial E-steps in combination with coresets. Run time complexity to optimize the clustering objective then reduces from O(NCD) per conventional EM iteration to O(N'G(2)D) for a variational EM iteration on coresets (with coreset size N'N and truncation parameter GC). Based on the strongly reduced run time complexity per iteration, which scales sublinearly with NC, we then provide a concrete, practically applicable, parallelized and highly efficient clustering algorithm. In numerical experiments on standard large-scale benchmarks we (A) show that also overall clustering times scale sublinearly, and (B) observe substantial wall-clock speedups compared to already highly efficient recently reported results. The algorithm's sublinear scaling allows for applications at scales where alternative methods cease to be applicable. We demonstrate such very large-scale applicability using the YFCC100M benchmark, for which we realize with a GMM of up to 50.000 clusters an optimization of a data density model with up to 150M parameters.",
        "The rapid emergence of novel psychoactive substances (NPS) poses new challenges and requirements for forensic testing/analysis techniques. This paper aims to explore the application of unsupervised clustering of NPS compounds' infrared spectra. Two statistical measures, Pearson and Spearman, were used to quantify the spectral similarity and to generate similarity matrices for hierarchical clustering. The correspondence of spectral similarity clustering trees to the commonly used structural/pharmacological categorization was evaluated and compared to the clustering generated using 2D/3D molecular fingerprints. Hybrid model feature selections were applied using different filter-based feature ranking algorithms developed for unsupervised clustering tasks. Since Spearman tends to overestimate the spectral similarity based on the overall pattern of the full spectrum, the clustering result shows the highest degree of improvement from having the nondiscriminative features removed. The loading plots of the first two principal components of the optimal feature subsets confirmed that the most important vibrational bands contributing to the clustering of NPS compounds were selected using non-negative discriminative feature selection (NDFS) algorithms.",
        "Under the new trend of industry 4.0 software-defined network, the value of meta heuristic algorithm was explored in the recognition of depression in patients with androgenic alopecia (AGA), and there was an analysis on the effect of comprehensive psychological interventions in the rehabilitation of AGA patients. Based on the meta heuristic algorithm, the Filter and Wrapper algorithms were combined in this study to form a new feature selection algorithm FAW-FS. Then, the classification accuracy of FAW-FS and the ability to identify depression disorders were verified under different open data sets. 54 patients with AGA who went to the Medical Cosmetic Center of Tongji Hospital were selected as the research objects and rolled into a control group (routine psychological intervention) and an intervention group (routine + comprehensive psychological interventions) according to different psychological intervention methods, with 27 cases in each group. The differences of the self-rating anxiety scale (SAS), self-rating depression scale (SDS), Hamilton depression scale (HAMD), and physical, psychological, social, and substance function scores before and after intervention were compared between the two groups of AGA patients, and the depression efficacy and compliance of the two groups were analyzed after intervention. The results showed that the classification accuracy of FAW-FS algorithm was the highest in logistic regression (LR), decision tree (DT), K-nearest neighbor (KNN) algorithm, support vector machine (SVM) algorithm, and random forest (RF) algorithm, which was 80.87, 79.24, 80.42, 83.07, and 81.45%, respectively. The LR algorithm had the highest feature selection accuracy of 82.94%, and the classification accuracy of depression disorder in RF algorithm was up to 73.01%. Besides, the SDS, SAS, and HAMD scores of the intervention group were lower sharply than the scores of the control group (p < 0.05). The physical function, psychological function, social function, and substance function scores of the intervention group were higher markedly than those of the control group (p < 0.05). In addition, the proportions of cured, markedly effective, total effective, full compliance, and total compliance patients in the intervention group increased obviously in contrast to the proportions of the control group (p < 0.05). Therefore, it indicated that the FAW-FS algorithm established in this study had significant advantages in the recognition of depression in AGA patients, and comprehensive psychological intervention had a positive effect in the rehabilitation of depression in AGA patients.",
        "Studying the molecular development of the human brain presents unique challenges for selecting a data analysis approach. The rare and valuable nature of human postmortem brain tissue, especially for developmental studies, means the sample sizes are small (n), but the use of high throughput genomic and proteomic methods measure the expression levels for hundreds or thousands of variables [e.g., genes or proteins (p)] for each sample. This leads to a data structure that is high dimensional (p >> n) and introduces the curse of dimensionality, which poses a challenge for traditional statistical approaches. In contrast, high dimensional analyses, especially cluster analyses developed for sparse data, have worked well for analyzing genomic datasets where p >> n. Here we explore applying a lasso-based clustering method developed for high dimensional genomic data with small sample sizes. Using protein and gene data from the developing human visual cortex, we compared clustering methods. We identified an application of sparse k-means clustering [robust sparse k-means clustering (RSKC)] that partitioned samples into age-related clusters that reflect lifespan stages from birth to aging. RSKC adaptively selects a subset of the genes or proteins contributing to partitioning samples into age-related clusters that progress across the lifespan. This approach addresses a problem in current studies that could not identify multiple postnatal clusters. Moreover, clusters encompassed a range of ages like a series of overlapping waves illustrating that chronological- and brain-age have a complex relationship. In addition, a recently developed workflow to create plasticity phenotypes (Balsor et al., 2020) was applied to the clusters and revealed neurobiologically relevant features that identified how the human visual cortex changes across the lifespan. These methods can help address the growing demand for multimodal integration, from molecular machinery to brain imaging signals, to understand the human brain's development.",
        "Unsupervised learning, the task of clustering observations in such a way that observations within cluster are more similar than those assigned to other clusters is one the central tasks of data science. Its exploratory and descriptive nature make it one of the most underused and underappreciated methods. In the present chapter we describe its core function with applied examples, explore different approaches, and discuss meaningful applications of the approach for the practicing researcher.",
        "BACKGROUND: Heterogeneity in Acute Respiratory Distress Syndrome (ARDS), as a consequence of its non-specific definition, has led to a multitude of negative randomised controlled trials (RCTs). Investigators have sought to identify heterogeneity of treatment effect (HTE) in RCTs using clustering algorithms. We evaluated the proficiency of several commonly-used machine-learning algorithms to identify clusters where HTE may be detected. METHODS: Five unsupervised: Latent class analysis (LCA), K-means, partition around medoids, hierarchical, and spectral clustering; and four supervised algorithms: model-based recursive partitioning, Causal Forest (CF), and X-learner with Random Forest (XL-RF) and Bayesian Additive Regression Trees were individually applied to three prior ARDS RCTs. Clinical data and research protein biomarkers were used as partitioning variables, with the latter excluded for secondary analyses. For a clustering schema, HTE was evaluated based on the interaction term of treatment group and cluster with day-90 mortality as the dependent variable. FINDINGS: No single algorithm identified clusters with significant HTE in all three trials. LCA, XL-RF, and CF identified HTE most frequently (2/3 RCTs). Important partitioning variables in the unsupervised approaches were consistent across algorithms and RCTs. In supervised models, important partitioning variables varied between algorithms and across RCTs. In algorithms where clusters demonstrated HTE in the same trial, patients frequently interchanged clusters from treatment-benefit to treatment-harm clusters across algorithms. LCA aside, results from all other algorithms were subject to significant alteration in cluster composition and HTE with random seed change. Removing research biomarkers as partitioning variables greatly reduced the chances of detecting HTE across all algorithms. INTERPRETATION: Machine-learning algorithms were inconsistent in their abilities to identify clusters with significant HTE. Protein biomarkers were essential in identifying clusters with HTE. Investigations using machine-learning approaches to identify clusters to seek HTE require cautious interpretation. FUNDING: NIGMS R35 GM142992 (PS), NHLBI R35 HL140026 (CSC); NIGMS R01 GM123193, Department of Defense W81XWH-21-1-0009, NIA R21 AG068720, NIDA R01 DA051464 (MMC).",
        "Single-cell RNA sequencing (scRNA-seq) is a new technology different from previous sequencing methods that measure the average expression level for each gene across a large population of cells. Thus, new computational methods are required to reveal cell types among cell populations. We present a clustering ensemble algorithm using optimized multiobjective particle (CEMP). It is featured with several mechanisms: 1) A multi-subspace projection method for mapping the original data to low-dimensional subspaces is applied in order to detect complex data structure at both gene level and sample level. 2) The basic partition module in different subspaces is utilized to generate clustering solutions. 3) A transforming representation between clusters and particles is used to bridge the gap between the discrete clustering ensemble optimization problem and the continuous multiobjective optimization algorithm. 4) We propose a clustering ensemble optimization. To guide the multiobjective ensemble optimization process, three cluster metrics are embedded into CEMP as objective functions in which the final clustering will be dynamically evaluated. Experiments on 9 real scRNA-seq datasets indicated that CEMP had superior performance over several other clustering algorithms in clustering accuracy and robustness. The case study conducted on mouse neuronal cells identified main cell types and cell subtypes successfully.",
        "Estimating the minimum streamflows in rivers is essential to solving problems related to water resources. In gauged watersheds, this task is relatively easy. However, the spatial and temporal insufficiency of gauged watercourses in Brazil makes researchers rely on the hydrological regionalization technique. This study's objective was to compare different hierarchical and non-hierarchical clustering approaches for the delimitation of hydrologically homogeneous regions in the state of Rio Grande do Sul, Brazil, aiming to regionalize the minimum streamflow that is equaled or exceeded in 90% of the time (Q90). The methodological development for the regionalization of Q90 consisted of using regression analysis supported by multivariate statistics. With respect to independent variables for regionalization, this study considered the morphoclimatic attributes of 100 watersheds located in southern Brazil. The results of this study highlighted that: (i) the clustering techniques had the potential to define hydrologically homogeneous regions, in the context of Q90 in the Rio Grande do Sul State, mostly the Ward algorithm associated with the Manhattan distance; (ii) drainage area, perimeter, centroids X and Y, and mean annual total rainfall aggregated important information that increased the accuracy of the cluster; and (iii) the refined mathematical models provided excellent performance and can be used to estimate Q90 in ungauged rivers.",
        "Using a nonconvex nonsmooth optimization approach, we introduce a model for semisupervised clustering (SSC) with pairwise constraints. In this model, the objective function is represented as a sum of three terms: the first term reflects the clustering error for unlabeled data points, the second term expresses the error for data points with must-link (ML) constraints, and the third term represents the error for data points with cannot-link (CL) constraints. This function is nonconvex and nonsmooth. To find its optimal solutions, we introduce an adaptive SSC (A-SSC) algorithm. This algorithm is based on the combination of the nonsmooth optimization method and an incremental approach, which involves the auxiliary SSC problem. The algorithm constructs clusters incrementally starting from one cluster and gradually adding one cluster center at each iteration. The solutions to the auxiliary SSC problem are utilized as starting points for solving the nonconvex SSC problem. The discrete gradient method (DGM) of nonsmooth optimization is applied to solve the underlying nonsmooth optimization problems. This method does not require subgradient evaluations and uses only function values. The performance of the A-SSC algorithm is evaluated and compared with four benchmarking SSC algorithms on one synthetic and 12 real-world datasets. Results demonstrate that the proposed algorithm outperforms the other four algorithms in identifying compact and well-separated clusters while satisfying most constraints.",
        "Clustering cells and depicting the lineage relationship among cell subpopulations are fundamental tasks in single-cell omics studies. However, existing analytical methods face challenges in stratifying cells, tracking cellular trajectories, and identifying critical points of cell transitions. To overcome these, we proposed a novel Markov hierarchical clustering algorithm (MarkovHC), a topological clustering method that leverages the metastability of exponentially perturbed Markov chains for systematically reconstructing the cellular landscape. Briefly, MarkovHC starts with local connectivity and density derived from the input and outputs a hierarchical structure for the data. We firstly benchmarked MarkovHC on five simulated datasets and ten public single-cell datasets with known labels. Then, we used MarkovHC to investigate the multi-level architectures and transition processes during human embryo preimplantation development and gastric cancer procession. MarkovHC found heterogeneous cell states and sub-cell types in lineage-specific progenitor cells and revealed the most possible transition paths and critical points in the cellular processes. These results demonstrated MarkovHC's effectiveness in facilitating the stratification of cells, identification of cell populations, and characterization of cellular trajectories and critical points.",
        "MOTIVATION: Single-cell RNA sequencing (scRNA-seq) provides transcriptomic profiling for individual cells, allowing researchers to study the heterogeneity of tissues, recognize rare cell identities and discover new cellular subtypes. Clustering analysis is usually used to predict cell class assignments and infer cell identities. However, the high sparsity of scRNA-seq data, accentuated by dropout events generates challenges that have motivated the development of numerous dedicated clustering methods. Nevertheless, there is still no consensus on the best performing method. RESULTS: graph-sc is a new method leveraging a graph autoencoder network to create embeddings for scRNA-seq cell data. While this work analyzes the performance of clustering the embeddings with various clustering algorithms, other downstream tasks can also be performed. A broad experimental study has been performed on both simulated and scRNA-seq datasets. The results indicate that although there is no consistently best method across all the analyzed datasets, graph-sc compares favorably to competing techniques across all types of datasets. Furthermore, the proposed method is stable across consecutive runs, robust to input down-sampling, generally insensitive to changes in the network architecture or training parameters and more computationally efficient than other competing methods based on neural networks. Modeling the data as a graph provides increased flexibility to define custom features characterizing the genes, the cells and their interactions. Moreover, external data (e.g. gene network) can easily be integrated into the graph and used seamlessly under the same optimization task. AVAILABILITY AND IMPLEMENTATION: https://github.com/ciortanmadalina/graph-sc. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Spatial transcriptomics has been emerging as a powerful technique for resolving gene expression profiles while retaining tissue spatial information. These spatially resolved transcriptomics make it feasible to examine the complex multicellular systems of different microenvironments. To answer scientific questions with spatial transcriptomics and expand our understanding of how cell types and states are regulated by microenvironment, the first step is to identify cell clusters by integrating the available spatial information. Here, we introduce SC-MEB, an empirical Bayes approach for spatial clustering analysis using a hidden Markov random field. We have also derived an efficient expectation-maximization algorithm based on an iterative conditional mode for SC-MEB. In contrast to BayesSpace, a recently developed method, SC-MEB is not only computationally efficient and scalable to large sample sizes but is also capable of choosing the smoothness parameter and the number of clusters. We performed comprehensive simulation studies to demonstrate the superiority of SC-MEB over some existing methods. We applied SC-MEB to analyze the spatial transcriptome of human dorsolateral prefrontal cortex tissues and mouse hypothalamic preoptic region. Our analysis results showed that SC-MEB can achieve a similar or better clustering performance to BayesSpace, which uses the true number of clusters and a fixed smoothness parameter. Moreover, SC-MEB is scalable to large 'sample sizes'. We then employed SC-MEB to analyze a colon dataset from a patient with colorectal cancer (CRC) and COVID-19, and further performed differential expression analysis to identify signature genes related to the clustering results. The heatmap of identified signature genes showed that the clusters identified using SC-MEB were more separable than those obtained with BayesSpace. Using pathway analysis, we identified three immune-related clusters, and in a further comparison, found the mean expression of COVID-19 signature genes was greater in immune than non-immune regions of colon tissue. SC-MEB provides a valuable computational tool for investigating the structural organizations of tissues from spatial transcriptomic data.",
        "Identifying the population structure of the newly emerged coronavirus SARS-CoV-2 has significant potential to inform public health management and diagnosis. As SARS-CoV-2 sequencing data accrued, grouping them into clusters is important for organizing the landscape of the population structure of the virus. Due to the limited prior information on the newly emerged coronavirus, we utilized four different clustering algorithms to group 16,873 SARS-CoV-2 strains, which automatically enables the identification of spatial structure for SARS-CoV-2. A total of six distinct genomic clusters were identified using mutation profiles as input features. Comparison of the clustering results reveals that the four algorithms produced highly consistent results, but the state-of-the-art unsupervised deep learning clustering algorithm performed best and produced the smallest intra-cluster pairwise genetic distances. The varied proportions of the six clusters within different continents revealed specific geographical distributions. In particular, our analysis found that Oceania was the only continent on which the strains were dispersively distributed into six clusters. In summary, this study provides a concrete framework for the use of clustering methods to study the global population structure of SARS-CoV-2. In addition, clustering methods can be used for future studies of variant population structures in specific regions of these fast-growing viruses.",
        "BACKGROUND: Lactic acidosis is a heterogeneous condition with multiple underlying causes and associated outcomes. The use of multi-dimensional patient data to subtype lactic acidosis can personalize patient care. Machine learning consensus clustering may identify lactic acidosis subgroups with unique clinical profiles and outcomes. METHODS: We used the Medical Information Mart for Intensive Care III database to abstract electronic medical record data from patients admitted to intensive care units (ICU) in a tertiary care hospital in the United States. We included patients who developed lactic acidosis (defined as serum lactate >/= 4 mmol/L) within 48 h of ICU admission. We performed consensus clustering analysis based on patient characteristics, comorbidities, vital signs, organ supports, and laboratory data to identify clinically distinct lactic acidosis subgroups. We calculated standardized mean differences to show key subgroup features. We compared outcomes among subgroups. RESULTS: We identified 1919 patients with lactic acidosis. The algorithm revealed three best unique lactic acidosis subgroups based on patient variables. Cluster 1 (n = 554) was characterized by old age, elective admission to cardiac surgery ICU, vasopressor use, mechanical ventilation use, and higher pH and serum bicarbonate. Cluster 2 (n = 815) was characterized by young age, admission to trauma/surgical ICU with higher blood pressure, lower comorbidity burden, lower severity index, and less vasopressor use. Cluster 3 (n = 550) was characterized by admission to medical ICU, history of liver disease and coagulopathy, acute kidney injury, lower blood pressure, higher comorbidity burden, higher severity index, higher serum lactate, and lower pH and serum bicarbonate. Cluster 3 had the worst outcomes, while cluster 1 had the most favorable outcomes in terms of persistent lactic acidosis and mortality. CONCLUSIONS: Consensus clustering analysis synthesized the pattern of clinical and laboratory data to reveal clinically distinct lactic acidosis subgroups with different outcomes.",
        "Connected autonomous vehicles can leverage communication and artificial intelligence technologies to effectively overcome the perceived limitations of individuals and enhance driving safety and stability. However, due to the high dynamics of the vehicular network and frequent interruptions and handovers, it is still challenging to provide stable communication connections between vehicles, which is likely to cause disasters. To address this issue, in this paper, we propose an intelligent clustering mechanism based on driving patterns in heterogeneous Cognitive Internet of Vehicles (CIoVs). In the proposed approach, we analyze the driving mode containing multiple feature parameters to accurately capture the driving characteristics. To ensure the accuracy of pattern recognition, a genetic algorithm-based neural network pattern recognition algorithm is proposed to support the reliable clustering of connected autonomous vehicles. The cognitive engines recognize the driving modes to group vehicles with a similar driving mode into a relatively stable cluster. In addition, we formulate the stability and survival time of clusters and analyze the communication performance of the clustering mechanism. Simulation results show that the proposed mechanism improves the reliable communication throughput and average cluster lifetime by approximately 14.4% and 11.5% respectively compared to the state-of-the-art approaches.",
        "In Opportunistic Networks (OppNets), mobility of and contact between nodes are explored to create communication opportunities and exchange messages and information. A basic premise for a better performance of these networks is a collaboration of the nodes during communication. However, due to energy restriction factors, nodes may eventually fail to collaborate with message exchanges. In this work, we propose a routing mechanism called eGPDMI to improve message probability of delivery while optimizing nodes' energy consumption. Unlike other algorithms proposed in OppNets literature, eGPDMI groups nodes by energy level and nodes interests using clustering techniques. Our major assumption is that retaining messages in nodes with the highest energy levels can improve network performance, thus overcoming the problem of nodes' disconnection due to unwillingness to cooperate due to low energy values. Through questionnaire application and factorial design experiments, we characterize the impacts of energy levels in OppNets. Further, we apply performance evaluation of the eGPDMI mechanism in terms of effectiveness using mobility from real-world scenarios. The results show that our mechanism effectively reduces the degradation of the probability of delivery when the minimum energy level used for nodes to cooperate with communication increases.",
        "Individual responses to methylphenidate (MPH) can significantly differ in children with attention-deficit/hyperactivity disorder (ADHD) in terms of the extent of clinical amelioration, optimal dosage needed, possible side effects, and short- and long-term duration of the benefits. In the present repeated-measures observational study, we undertook a proof-of-concept study to determine whether clustering analysis could be useful to characterize different clusters of responses to MPH in children with ADHD. We recruited 33 children with ADHD who underwent a comprehensive clinical, cognitive, and neurophysiological assessment before and after one month of MPH treatment. Symptomatology changes were assessed by parents and clinicians. The neuropsychological measures used comprised pen-and-paper and computerized tasks. Functional near-infrared spectroscopy was used to measure cortical hemodynamic activation during an attentional task. We developed an unsupervised machine learning algorithm to characterize the possible clusters of responses to MPH in our multimodal data. A symptomatology improvement was observed for both clinical and neuropsychological measures. Our model identified distinct clusters of amelioration that were related to symptom severity and visual-attentional performances. The present findings provide preliminary evidence that clustering analysis can potentially be useful in identifying different responses to MPH in children with ADHD, highlighting the importance of a personalized medicine approach within the clinical framework.",
        "Single-cell sequencing provides novel means to interpret the transcriptomic profiles of individual cells. To obtain in-depth analysis of single-cell sequencing, it requires effective computational methods to accurately predict single-cell clusters because single-cell sequencing techniques only provide the transcriptomic profiles of each cell. Although an accurate estimation of the cell-to-cell similarity is an essential first step to derive reliable single-cell clustering results, it is challenging to obtain the accurate similarity measurement because it highly depends on a selection of genes for similarity evaluations and the optimal set of genes for the accurate similarity estimation is typically unknown. Moreover, due to technical limitations, single-cell sequencing includes a larger number of artificial zeros, and the technical noise makes it difficult to develop effective single-cell clustering algorithms. Here, we describe a novel single-cell clustering algorithm that can accurately predict single-cell clusters in large-scale single-cell sequencing by effectively reducing the zero-inflated noise and accurately estimating the cell-to-cell similarities. First, we construct an ensemble similarity network based on different similarity estimates, and reduce the artificial noise using a random walk with restart framework. Finally, starting from a larger number small size but highly consistent clusters, we iteratively merge a pair of clusters with the maximum similarities until it reaches the predicted number of clusters. Extensive performance evaluation shows that the proposed single-cell clustering algorithm can yield the accurate single-cell clustering results and it can help deciphering the key messages underlying complex biological mechanisms.",
        "Support vector clustering (SVC) is a boundary-based algorithm, which has several advantages over other clustering methods, including identifying clusters of arbitrary shapes and numbers. Leveraged by the high generalization ability of the large margin distribution machine (LDM) and the optimal margin distribution clustering (ODMC), we propose a new clustering method: minimum distribution for support vector clustering (MDSVC), for improving the robustness of boundary point recognition, which characterizes the optimal hypersphere by the first-order and second-order statistics and tries to minimize the mean and variance simultaneously. In addition, we further prove, theoretically, that our algorithm can obtain better generalization performance. Some instructive insights for adjusting the number of support vector points are gained. For the optimization problem of MDSVC, we propose a double coordinate descent algorithm for small and medium samples. The experimental results on both artificial and real datasets indicate that our MDSVC has a significant improvement in generalization performance compared to SVC.",
        "OBJECTIVE: To describe and evaluate vigiGroup - a consensus clustering algorithm which can identify groups of individual case reports referring to similar suspected adverse drug reactions and describe associated adverse event profiles, accounting for co-reported adverse event terms. MATERIALS AND METHODS: Consensus clustering is achieved by grouping pairs of reports that are repeatedly placed together in the same clusters across a set of mixture model-based cluster analyses. The latter use empirical Bayes statistical shrinkage for improved performance. As baseline comparison, we considered a regular mixture model-based cluster analysis. Three randomly selected drugs in VigiBase, the World Health Organization's global database of Individual Case Safety Reports were analyzed: sumatriptan, ambroxol and tacrolimus. Clustering stability was assessed using the adjusted Rand index, ranging between -1 and +1, and clinical coherence was assessed through an intruder detection analysis. RESULTS: For the three drugs considered, vigiGroup achieved stable and coherent results with adjusted Rand indices between +0.80 and +0.92, and intruder detection rates between 86% and 94%. Consensus clustering improved both stability and clinical coherence compared to mixture model-based clustering alone. Statistical shrinkage improved the stability of clusters compared to the baseline mixture model, as well as the cross-validated log-likelihood. CONCLUSIONS: The proposed algorithm can achieve adequate stability and clinical coherence in clustering individual case reports, thereby enabling better identification of case series and associated adverse event profiles in pharmacovigilance. The use of empirical Bayes shrinkage and consensus clustering each led to meaningful improvements in performance.",
        "Prostate cancer disease is one of the common types that cause men's prostate damage all over the world. Prostate-specific membrane antigen (PSMA) expressed by type-II is an extremely attractive style for imaging-based diagnosis of prostate cancer. Clinically, photodynamic therapy (PDT) is used as noninvasive therapy in treatment of several cancers and some other diseases. This paper aims to segment or cluster and analyze pixels of histological and near-infrared (NIR) prostate cancer images acquired by PSMA-targeting PDT low weight molecular agents. Such agents can provide image guidance to resection of the prostate tumors and permit for the subsequent PDT in order to remove remaining or noneradicable cancer cells. The color prostate image segmentation is accomplished using an optimized image segmentation approach. The optimized approach combines the k-means clustering algorithm with elbow method that can give better clustering of pixels through automatically determining the best number of clusters. Clusters' statistics and ratio results of pixels in the segmented images show the applicability of the proposed approach for giving the optimum number of clusters for prostate cancer analysis and diagnosis.",
        "Numerous limitations of Shot-based and Content-based key-frame extraction approaches have encouraged the development of Cluster-based algorithms. This paper proposes an Optimal Threshold and Maximum Weight (OTMW) clustering approach that allows accurate and automatic extraction of video summarization. Firstly, the video content is analyzed using the image color, texture and information complexity, and video feature dataset is constructed. Then a Golden Section method is proposed to determine the threshold function optimal solution. The initial cluster center and the cluster number k are automatically obtained by employing the improved clustering algorithm. k-clusters video frames are produced with the help of K-MEANS algorithm. The representative frame of each cluster is extracted using the Maximum Weight method and an accurate video summarization is obtained. The proposed approach is tested on 16 multi-type videos, and the obtained key-frame quality evaluation index, and the average of Fidelity and Ratio are 96.11925 and 97.128, respectively. Fortunately, the key-frames extracted by the proposed approach are consistent with artificial visual judgement. The performance of the proposed approach is compared with several state-of-the-art cluster-based algorithms, and the Fidelity are increased by 12.49721, 10.86455, 10.62984 and 10.4984375, respectively. In addition, the Ratio is increased by 1.958 on average with small fluctuations. The obtained experimental results demonstrate the advantage of the proposed solution over several related baselines on sixteen diverse datasets and validated that proposed approach can accurately extract video summarization from multi-type videos.",
        "Spatial co-location pattern mining discovers the subsets of spatial features frequently observed together in nearby geographic space. To reduce time and space consumption in checking the clique relationship of row instances of the traditional co-location pattern mining methods, the existing work adopted density peak clustering to materialize the neighbor relationship between instances instead of judging the neighbor relationship by a specific distance threshold. This approach had two drawbacks: first, there was no consideration in the fuzziness of the distance between the center and other instances when calculating the local density; second, forcing an instance to be divided into each cluster resulted in a lack of accuracy in fuzzy participation index calculations. To solve the above problems, three improvement strategies are proposed for the density peak clustering in the co-location pattern mining in this paper. Then a new prevalence measurement of co-location pattern is put forward. Next, we design the spatial co-location pattern mining algorithm based on the improved density peak clustering and the fuzzy neighbor relationship. Many experiments are executed on the synthetic and real datasets. The experimental results show that, compared to the existing method, the proposed algorithm is more effective, and can significantly save the time and space complexity in the phase of generating prevalent co-location patterns.",
        "When insufficient samples in the spatial frequency domain could be effectively compensated by the modified CLEAN algorithm, a novel aperture-synthetic scheme of ghost imaging takes advantage of a superior speed of modulation and an enhancement on the spatial resolution. However, there still exist some imperfections in the modified CLEAN reconstructions, such as the fact that some omitted scatter noise still remains or the object contour may be incomplete. Therefore, we optimize the modified CLEAN algorithm by proposing a density clustering algorithm to overcome these drawbacks and improve the visual quality.",
        "Multi-view clustering aims at simultaneously obtaining a consensus underlying subspace across multiple views and conducting clustering on the learned consensus subspace, which has gained a variety of interest in image processing. In this paper, we propose the Semi-supervised Structured Subspace Learning algorithm for clustering data points from Multiple sources (SSSL-M). We explicitly extend the traditional multi-view clustering with a semi-supervised manner and then build an anti-block-diagonal indicator matrix with small amount of supervisory information to pursue the block-diagonal structure of the shared affinity matrix. SSSL-M regularizes multiple view-specific affinity matrices into a shared affinity matrix based on reconstruction through a unified framework consisting of backward encoding networks and the self-expressive mapping. The shared affinity matrix is comprehensive and can flexibly encode complementary information from multiple view-specific affinity matrices. An enhanced structural consistency of affinity matrices from different views can be achieved and the intrinsic relationships among affinity matrices from multiple views can be effectively reflected in this manner. Technically, we formulate the proposed model as an optimization problem, which can be solved by an alternating optimization scheme. Experimental results over seven different benchmark datasets demonstrate that better clustering results can be obtained by our method compared with the state-of-the-art approaches.",
        "Weighted multi-view clustering (MVC) aims to combine the complementary information of multi-view data (such as image data with different types of features) in a weighted manner to obtain a consistent clustering result. However, when the cluster-wise weights across views are vastly different, most existing weighted MVC methods may fail to fully utilize the complementary information, because they are based on view-wise weight learning and can not learn the fine-grained cluster-wise weights. Additionally, extra parameters are needed for most of them to control the weight distribution sparsity or smoothness, which are hard to tune without prior knowledge. To address these issues, in this paper we propose a novel and effective Cluster-weighted mUlti-view infoRmation bottlEneck (CURE) clustering algorithm, which can automatically learn the cluster-wise weights to discover the discriminative clusters across multiple views and thus can enhance the clustering performance by properly exploiting the cluster-level complementary information. To learn the cluster-wise weights, we design a new weight learning scheme by exploring the relation between the mutual information of the joint distribution of a specific cluster (containing a group of data samples) and the weight of this cluster. Finally, a novel draw-and-merge method is presented to solve the optimization problem. Experimental results on various multi-view datasets show the superiority and effectiveness of our cluster-wise weighted CURE over several state-of-the-art methods.",
        "Blind modulation format identification (MFI) is indispensable for correct signal demodulation and optical performance monitoring in future elastic optical networks (EON). Existing MFI schemes based on a clustering algorithm in Stokes space have gained good performance, while only limited types of modulation formats could be correctly identified, and the complexities are relatively high. In this work, we have proposed an MFI scheme with a low computational complexity, which combines an improved particle swarm optimization (I-PSO) clustering algorithm with a 2D Stokes plane. The main idea of I-PSO is to add a new field of view on each particle and limit each particle to only communicate with its neighbor particles, so as to realize the correct judgment of the number of multiple clusters (local extrema) on the density images of the s2-s3 plane. The effectiveness has been verified by 28 GBaud polarization division multiplexing (PDM)-BPSK/PDM-QPSK/PDM-8QAM/PDM-16QAM/PDM-32QAM/PDM-64QAM simulation EON systems and 28 GBaud PDM-QPSK/PDM-8QAM/PDM-16QAM/PDM-32QAM proof-of-concept transmission experiments. The results show that, using this MFI scheme, the minimum optical signal-to-noise ratio (OSNR) values to achieve 100% MFI success rate are all equal to or lower than those of the corresponding 7% forward error correction (FEC) thresholds. At the same time, the MFI scheme also obtains good tolerance to residual chromatic dispersion and differential group delay. Besides that, the proposed scheme achieves 100% MFI success rate within a maximum launch power range of -2 approximately +6 dBm. More importantly, its computational complexity can be denoted as O(N).",
        "Massive inherent speckle noise and extremely low contrast make it difficult to binarize electronic speckle pattern interferometry (ESPI) fringe patterns. In this paper, we present a binarization based on preprocessing and fuzzy C-means (FCM) clustering for low-quality ESPI fringe patterns. First, we use the multiscale retinex (MSR) algorithm to enhance the original fringe pattern to improve the contrast between the bright and dark fringes. Then, the local entropy of the enhanced fringe pattern is calculated and the second-order oriented partial differential equation algorithm is introduced to filter the local entropy map. Finally, the FCM is applied to cluster the local entropy filtering map, and the pixels of the fringe pattern are classified into two categories: bright fringes and dark fringes. To verify the reliability and universality of the proposed method, we provide a qualitative evaluation of six experimental ESPI subtraction fringe patterns and two computer-simulated ESPI addition fringe patterns. Experimental results exhibit that the proposed method can provide good binarization performances.",
        "Traditional clustering methods often cannot avoid the problem of selecting neighborhood parameters and the number of clusters, and the optimal selection of these parameters varies among different shapes of data, which requires prior knowledge. To address the above parameter selection problem, we propose an effective clustering algorithm based on adaptive neighborhood, which can obtain satisfactory clustering results without setting the neighborhood parameters and the number of clusters. The core idea of the algorithm is to first iterate adaptively to a logarithmic stable state and obtain neighborhood information according to the distribution characteristics of the dataset, and then mark and peel the boundary points according to this neighborhood information, and finally cluster the data clusters with the core points as the centers. We have conducted extensive comparative experiments on datasets of different sizes and different distributions and achieved satisfactory experimental results.",
        "The development of visual tools for the timely identification of spatio-temporal clusters will assist in implementing control measures to prevent further damage. From January 2015 to June 2020, a total number of 1463 avian influenza outbreak farms were detected in Taiwan and further confirmed to be affected by highly pathogenic avian influenza subtype H5Nx. In this study, we adopted two common concepts of spatio-temporal clustering methods, the Knox test and scan statistics, with visual tools to explore the dynamic changes of clustering patterns. Since most (68.6%) of the outbreak farms were detected in 2015, only the data from 2015 was used in this study. The first two-stage algorithm performs the Knox test, which established a threshold of 7 days and identified 11 major clusters in the six counties of southwestern Taiwan, followed by the standard deviational ellipse (SDE) method implemented on each cluster to reveal the transmission direction. The second algorithm applies scan likelihood ratio statistics followed by AGC index to visualize the dynamic changes of the local aggregation pattern of disease clusters at the regional level. Compared to the one-stage aggregation approach, Knox-based and AGC mapping were more sensitive in small-scale spatio-temporal clustering.",
        "A Relational-Sequential dataset (or RS-dataset for short) contains records comprised of a patients values in demographic attributes and their sequence of diagnosis codes. The task of clustering an RS-dataset is helpful for analyses ranging from pattern mining to classification. However, existing methods are not appropriate to perform this task. Thus, we initiate a study of how an RS-dataset can be clustered effectively and efficiently. We formalize the task of clustering an RS-dataset as an optimization problem. At the heart of the problem is a distance measure we design to quantify the pairwise similarity between records of an RS-dataset. Our measure uses a tree structure that encodes hierarchical relationships between records, based on their demographics, as well as an edit-distance-like measure that captures both the sequentiality and the semantic similarity of diagnosis codes. We also develop an algorithm which first identifies k representative records (centers), for a given k, and then constructs clusters, each containing one center and the records that are closer to the center compared to other centers. Experiments using two Electronic Health Record datasets demonstrate that our algorithm constructs compact and well-separated clusters, which preserve meaningful relationships between demographics and sequences of diagnosis codes, while being efficient and scalable.",
        "AIMS: It is increasingly recognized that the presence of comorbidities substantially contributes to the disease burden in patients with heart failure (HF). Several reports have suggested that clustering of comorbidities can lead to improved characterization of the disease phenotypes, which may influence management of the individual patient. Therefore, we aimed to cluster patients with HF based on medical comorbidities and their treatment and, subsequently, compare the clinical characteristics between these clusters. METHODS AND RESULTS: A total of 603 patients with HF entering an outpatient HF rehabilitation programme were included [median age 65 years (interquartile range 56-71), 57% ischaemic origin of cardiomyopathy, and left ventricular ejection fraction 35% (26-45)]. Exercise performance, daily life activities, disease-specific health status, coping styles, and personality traits were assessed. In addition, the presence of 12 clinically relevant comorbidities was recorded, based on targeted diagnostics combined with applicable pharmacotherapies. Self-organizing maps (SOMs; www.viscovery.net) were used to visualize clusters, generated by using a hybrid algorithm that applies the classical hierarchical cluster method of Ward on top of the SOM topology. Five clusters were identified: (1) a least comorbidities cluster; (2) a cachectic/implosive cluster; (3) a metabolic diabetes cluster; (4) a metabolic renal cluster; and (5) a psychologic cluster. Exercise performance, daily life activities, disease-specific health status, coping styles, personality traits, and number of comorbidities were significantly different between these clusters. CONCLUSIONS: Distinct combinations of comorbidities could be identified in patients with HF. Therapy may be tailored based on these clusters as next step towards precision medicine. The effect of such an approach needs to be prospectively tested.",
        "Semantic mining is always a challenge for big biomedical text data. Ontology has been widely proved and used to extract semantic information. However, the process of ontology-based semantic similarity calculation is so complex that it cannot measure the similarity for big text data. To solve this problem, we propose a parallelized semantic similarity measurement method based on Hadoop MapReduce for big text data. At first, we preprocess and extract the semantic features from documents. Then, we calculate the document semantic similarity based on ontology network structure under MapReduce framework. Finally, based on the generated semantic document similarity, document clusters are generated via clustering algorithms. To validate the effectiveness, we use two kinds of open datasets. The experimental results show that the traditional methods can hardly work for more than ten thousand biomedical documents. The proposed method keeps efficient and accurate for big dataset and is of high parallelism and scalability.",
        "Recombinant human growth hormone (r-hGH) is an established therapy for growth hormone deficiency (GHD); yet, some patients fail to achieve their full height potential, with poor adherence and persistence with the prescribed regimen often a contributing factor. A data-driven clinical decision support system based on \"traffic light\" visualizations for adherence risk management of patients receiving r-hGH treatment was developed. This research was feasible thanks to data-sharing agreements that allowed the creation of these models using real-world data of r-hGH adherence from easypod connect; data was retrieved for 11,015 children receiving r-hGH therapy for >/=180 days. Patients' adherence to therapy was represented using four values (mean and standard deviation [SD] of daily adherence and hours to next injection). Cluster analysis was used to categorize adherence patterns using a Gaussian mixture model. Following a traffic lights-inspired visualization approach, the algorithm was set to generate three clusters: green, yellow, or red status, corresponding to high, medium, and low adherence, respectively. The area under the receiver operating characteristic curve (AUC-ROC) was used to find optimum thresholds for independent traffic lights according to each metric. The most appropriate traffic light used the SD of the hours to the next injection, with an AUC-ROC value of 0.85 when compared to the complex clustering algorithm. For the daily adherence-based traffic lights, optimum thresholds were >0.82 (SD, <0.37), 0.53-0.82 (SD, 0.37-0.61), and <0.53 (SD, >0.61) for high, medium, and low adherence, respectively. For hours to next injection, the corresponding optimum thresholds were <27.18 (SD, <10.06), 27.18-34.01 (SD, 10.06-29.63), and >34.01 (SD, >29.63). Our research indicates that implementation of a practical data-driven alert system based on recognised traffic-light coding would enable healthcare practitioners to monitor sub-optimally-adherent patients to r-hGH treatment for early intervention to improve treatment outcomes.",
        "Clustering Algorithms have just fascinated significant devotion in machine learning applications owing to their great competence. Nevertheless, the existing algorithms quite have approximately disputes that need to be further deciphered. For example, most existing algorithms transform one type of feature into another type, which disregards the explicit possessions of information. In addition, most of them deliberate whole features, which may lead to difficulty in calculation and effect in sub-optimal presentation. To address the above difficulties, this paper proposes a novel technique for clustering categorical and numerical features based on feature space clustering of mixed data with missing information (FSCMMI). The procedure involves three stages. Initially, FSCMMI divides the given dataset depending on missing information in instances and features types. The second stage uses the decision-tree procedure to identify the association between instances. Finally, the third stage is used for computing the closeness measure for numerical features and categorical features. Meanwhile, we propose a new training algorithm to cluster mixed datasets. Extensive experimental results on benchmark datasets show that the proposed FSCMMI outperforms several state-of-art clustering methods in terms of accuracy and efficiency.",
        "BACKGROUND: Hospitals in the public and private sectors tend to join larger organizations to form hospital groups. This increasingly frequent mode of functioning raises the question of how countries should organize their health system, according to the interactions already present between their hospitals. The objective of this study was to identify distinctive profiles of French hospitals according to their characteristics and their role in the French hospital network. METHODS: Data were extracted from the national hospital database for year 2016. The database was restricted to public hospitals that practiced medicine, surgery or obstetrics. Hospitals profiles were determined using the k-means method. The variables entered in the clustering algorithm were: the number of stays, the effective diversity of hospital activity, and a network-based mobility indicator (proportion of stays followed by another stay in a different hospital of the same Regional Hospital Group within 90 days). RESULTS: Three hospital groups were identified by the clustering algorithm. The first group was constituted of 34 large hospitals (median 82,100 annual stays, interquartile range 69,004 - 117,774) with a very diverse activity. The second group contained medium-sized hospitals (with a median of 258 beds, interquartile range 164 - 377). The third group featured less diversity regarding the type of stay (with a mean of 8 effective activity domains, standard deviation 2.73), a smaller size and a higher proportion of patients that subsequently visited other hospitals (11%). The most frequent type of patient mobility occurred from the hospitals in group 2 to the hospitals in group 1 (29%). The reverse direction was less frequent (19%). CONCLUSIONS: The French hospital network is organized around three categories of public hospitals, with an unbalanced and disassortative patient flow. This type of organization has implications for hospital planning and infectious diseases control.",
        "PURPOSE: Recent explorations of tennis-specific movements have developed contemporary methods for identifying and classifying changes of direction (COD) during match-play. The aim of this research was to employ these new analysis techniques to objectively explore individual nuance and style factors in the execution of COD movements in professional tennis. METHODS: Player tracking data from 62 male and 77 female players at the Australian Open Grand Slam were analysed for COD movements using a model algorithm, with a sample of 150,000 direction changes identified. Hierarchical clustering methods were employed on the time-motion and degree characteristics of these direction changes to identify groups of different COD performers. RESULTS: Five unique clusters, labelled 'Cutters', 'Gear Changers', 'Lateral Changers', 'Balanced Changers' and 'Passive Changers' were identified in accordance with their varying speed, acceleration, degree and directionality of change features. CONCLUSIONS: Player COD clustering challenge previously held assumptions regarding on-court movement style, highlighting the complexity and variation in the sport's locomotion demands. In practice, the speed, acceleration, directionality and degree of change characteristics of each COD style can facilitate athlete profiling and the specificity of training interventions.",
        "Single-cell RNA-sequencing (scRNA-seq) analyses typically begin by clustering a gene-by-cell expression matrix to empirically define groups of cells with similar expression profiles. We describe new methods and a new open source library, minicore, for efficient k-means++ center finding and k-means clustering of scRNA-seq data. Minicore works with sparse count data, as it emerges from typical scRNA-seq experiments, as well as with dense data from after dimensionality reduction. Minicore's novel vectorized weighted reservoir sampling algorithm allows it to find initial k-means++ centers for a 4-million cell dataset in 1.5 minutes using 20 threads. Minicore can cluster using Euclidean distance, but also supports a wider class of measures like Jensen-Shannon Divergence, Kullback-Leibler Divergence, and the Bhattachaiyya distance, which can be directly applied to count data and probability distributions. Further, minicore produces lower-cost centerings more efficiently than scikit-learn for scRNA-seq datasets with millions of cells. With careful handling of priors, minicore implements these distance measures with only minor (<2-fold) speed differences among all distances. We show that a minicore pipeline consisting of k-means++, localsearch++ and mini-batch k-means can cluster a 4-million cell dataset in minutes, using less than 10GiB of RAM. This memory-efficiency enables atlas-scale clustering on laptops and other commodity hardware. Finally, we report findings on which distance measures give clusterings that are most consistent with known cell type labels.",
        "Background: Studies evaluating the natural history of femoroacetabular impingement (FAI) are limited. Purpose: To stratify the risk of progression to osteoarthritis (OA) in patients with FAI using an unsupervised machine-learning algorithm, compare the characteristics of each subgroup, and validate the reproducibility of staging. Study Design: Cohort study (prognosis); Level of evidence, 2. Methods: A geographic database from the Rochester Epidemiology Project was used to identify patients with hip pain between 2000 and 2016. Medical charts were reviewed to obtain characteristic information, physical examination findings, and imaging details. The patient data were randomly split into 2 mutually exclusive sets: train set (70%) for model development and test set (30%) for validation. The data were transformed via Uniform Manifold Approximation and Projection and were clustered using Hierarchical Density-based Spatial Clustering of Applications with Noise. Results: The study included 1071 patients with a mean follow-up period of 24.7 +/- 12.5 years. The patients were clustered into 5 subgroups based on train set results: patients in cluster 1 were in their early 20s (20.9 +/- 9.6 years), female dominant (84%), with low body mass index (<19 ); patients in cluster 2 were in their early 20s (22.9 +/- 6.7 years), female dominant (95%), and pincer-type FAI (100%) dominant; patients in cluster 3 were in their mid 20s (26.4 +/- 9.7) and were mixed-type FAI dominant (92%); patients in cluster 4 were in their early 30s (32.7 +/- 7.8), with high body mass index (>/=29 ), and diabetes (17%); and patients in cluster 5 were in their early 30s (30.0 +/- 9.1), with a higher percentage of males (43%) compared with the other clusters and with limited internal rotation (14%). Mean survival for clusters 1 to 5 was 17.9 +/- 0.6, 18.7 +/- 0.3, 17.1 +/- 0.4, 15.0 +/- 0.5, and 15.6 +/- 0.5 years, respectively, in the train set. The survival difference was significant between clusters 1 and 4 (P = .02), 2 and 4 (P < .005), 2 and 5 (P = .01), and 3 and 4 (P < .005) in the train set and between clusters 2 and 5 (P = .03) and 3 and 4 (P = .01) in the test set. Cluster characteristics and prognosis was well reproduced in the test set. Conclusion: Using the clustering algorithm, it was possible to determine the prognosis for OA progression in patients with FAI in the presence of conflicting risk factors acting in combination.",
        "Clustering genes in similarity graphs is a popular approach for orthology prediction. Most algorithms group genes without considering their species, which results in clusters that contain several paralogous genes. Moreover, clustering is known to be problematic when in-paralogs arise from ancient duplications. Recently, we proposed a two-step process that avoids these problems. First, we infer clusters of only orthologs (i.e. with only genes from distinct species), and second, we infer the missing inter-cluster orthologs. In this paper, we focus on the first step, which leads to a problem we call Colorful Clustering. In general, this is as hard as classical clustering. However, in similarity graphs, the number of species is usually small, as well as the neighborhood size of genes in other species. We therefore study the problem of clustering in which the number of colors is bounded by [Formula: see text], and each gene has at most [Formula: see text] neighbors in another species. We show that the well-known cluster editing formulation remains NP-hard even when [Formula: see text] and [Formula: see text]. We then propose a fixed-parameter algorithm in [Formula: see text] to find the single best cluster in the graph. We implemented this algorithm and included it in the aforementioned two-step approach. Experiments on simulated data show that this approach performs favorably to applying only an unconstrained clustering step.",
        "Heart rate is one of the most important diagnostic bases for cardiovascular disease. This paper introduces a deep autoencoding strategy into feature extraction of electrocardiogram (ECG) signals, and proposes a beat-to-beat heart rate estimation method based on convolution autoencoding and Gaussian mixture clustering. The high-level heartbeat features were first extracted in an unsupervised manner by training the convolutional autoencoder network, and then the adaptive Gaussian mixture clustering was applied to detect the heartbeat locations from the extracted features, and calculated the beat-to-beat heart rate. Compared with the existing heartbeat classification/detection methods, the proposed unsupervised feature learning and heartbeat clustering method does not rely on accurate labeling of each heartbeat location, which could save a lot of time and effort in human annotations. Experimental results demonstrate that the proposed method maintains better accuracy and generalization ability compared with the existing ECG heart rate estimation methods and could be a robust long-time heart rate monitoring solution for wearable ECG devices.",
        "Human activity recognition plays a prominent role in numerous applications like smart homes, elderly healthcare and ambient intelligence. The complexity of human behavior leads to the difficulty of developing an accurate activity recognizer, especially in situations where different activities have similar sensor readings. Accordingly, how to measure the relationships among activities and construct an activity recognizer for better distinguishing the confusing activities remains critical. To this end, we in this study propose a clustering guided hierarchical framework to discriminate on-going human activities. Specifically, we first introduce a clustering-based activity confusion index and exploit it to automatically and quantitatively measure the confusion between activities in a data-driven way instead of relying on the prior domain knowledge. Afterwards, we design a hierarchical activity recognition framework under the guidance of the confusion relationships to reduce the recognition errors between similar activities. Finally, the simulations on the benchmark datasets are evaluated and results show the superiority of the proposed model over its competitors. In addition, we experimentally evaluate the key components of the framework comprehensively, which indicates its flexibility and stability.",
        "Many machine learning procedures, including clustering analysis are often affected by missing values. This work aims to propose and evaluate a Kernel Fuzzy C-means clustering algorithm considering the kernelization of the metric with local adaptive distances (VKFCM-K-LP) under three types of strategies to deal with missing data. The first strategy, called Whole Data Strategy (WDS), performs clustering only on the complete part of the dataset, i.e. it discards all instances with missing data. The second approach uses the Partial Distance Strategy (PDS), in which partial distances are computed among all available resources and then re-scaled by the reciprocal of the proportion of observed values. The third technique, called Optimal Completion Strategy (OCS), computes missing values iteratively as auxiliary variables in the optimization of a suitable objective function. The clustering results were evaluated according to different metrics. The best performance of the clustering algorithm was achieved under the PDS and OCS strategies. Under the OCS approach, new datasets were derive and the missing values were estimated dynamically in the optimization process. The results of clustering under the OCS strategy also presented a superior performance when compared to the resulting clusters obtained by applying the VKFCM-K-LP algorithm on a version where missing values are previously imputed by the mean or the median of the observed values.",
        "Due to the explosive growth of short text on various social media platforms, short text stream clustering has become an increasingly prominent issue. Unlike traditional text streams, short text stream data present the following characteristics: short length, weak signal, high volume, high velocity, topic drift, etc. Existing methods cannot simultaneously address two major problems very well: inferring the number of topics and topic drift. Therefore, we propose a dynamic clustering algorithm for short text streams based on the Dirichlet process (DCSS), which can automatically learn the number of topics in documents and solve the topic drift problem of short text streams. To solve the sparsity problem of short texts, DCSS considers the correlation of the topic distribution at neighbouring time points and uses the inferred topic distribution of past documents as a prior of the topic distribution at the current moment while simultaneously allowing newly streamed documents to change the posterior distribution of topics. We conduct experiments on two widely used datasets, and the results show that DCSS outperforms existing methods and has better stability.",
        "With the spread of COVID-19, there is an urgent need for a fast and reliable diagnostic aid. For the same, literature has witnessed that medical imaging plays a vital role, and tools using supervised methods have promising results. However, the limited size of medical images for diagnosis of CoVID19 may impact the generalization of such supervised methods. To alleviate this, a new clustering method is presented. In this method, a novel variant of a gravitational search algorithm is employed for obtaining optimal clusters. To validate the performance of the proposed variant, a comparative analysis among recent metaheuristic algorithms is conducted. The experimental study includes two sets of benchmark functions, namely standard functions and CEC2013 functions, belonging to different categories such as unimodal, multimodal, and unconstrained optimization functions. The performance comparison is evaluated and statistically validated in terms of mean fitness value, Friedman test, and box-plot. Further, the presented clustering method tested against three different types of publicly available CoVID19 medical images, namely X-ray, CT scan, and Ultrasound images. Experiments demonstrate that the proposed method is comparatively outperforming in terms of accuracy, precision, sensitivity, specificity, and F1-score.",
        "Cluster analysis of functional data is finding increasing application in the field of medical research and statistics. Here we introduce a functional version of the forward search methodology for the purpose of functional data clustering. The proposed forward search algorithm is based on the functional spatial ranks and is a data-driven non-parametric method. It does not require any preprocessing functional data steps, nor does it require any dimension reduction before clustering. The Forward Search Based on Functional Spatial Rank (FSFSR) algorithm identifies the number of clusters in the curves and provides the basis for the accurate assignment of each curve to its cluster. We apply it to three simulated datasets and two real medical datasets, and compare it with six other standard methods. Based on both simulated and real data, the FSFSR algorithm identifies the correct number of clusters. Furthermore, when compared with six standard methods used for clustering and classification, it records the lowest misclassification rate. We conclude that the FSFSR algorithm has the potential to cluster and classify functional data.",
        "This article extends the expectation-maximization (EM) formulation for the Gaussian mixture model (GMM) with a novel weighted dissimilarity loss. This extension results in the fusion of two different clustering methods, namely, centroid-based clustering and graph clustering in the same framework in order to leverage their advantages. The fusion of centroid-based clustering and graph clustering results in a simple ``soft'' asynchronous hybrid clustering method. The proposed algorithm may start as a pure centroid-based clustering algorithm (e.g., k-means), and as the time evolves, it may eventually and gradually turn into a pure graph clustering algorithm [e.g., basic greedy asynchronous distributed interference avoidance (GADIA) (Babadi and Tarokh, 2010)] as the algorithm converges and vice versa. The ``hard'' version of the proposed hybrid algorithm includes the standard Hopfield neural networks (and, thus, Bruck's Ln algorithm by (Bruck, 1990) and the Ising model in statistical mechanics), Babadi and Tarokh's basic GADIA in 2010, and the standard k-means (Steinhaus, 1956), (MacQueen, 1967) [i.e., the Lloyd algorithm (Lloyd, 1957, 1982)] as its special cases. We call the ``hard version'' of the proposed clustering as ``hybrid-nongreedy asynchronous clustering (H-NAC).'' We apply the H-NAC to various clustering problems using well-known benchmark datasets. The computer simulations confirm the superior performance of the H-NAC compared to the k-means clustering, k-GADIA, spectral clustering, and a very recent clustering algorithm structured graph learning (SGL) by Kang et al. (2021), which represents one of the state-of-the-art clustering algorithms.",
        "In mixed multi-view data, multiple sets of diverse features are measured on the same set of samples. By integrating all available data sources, we seek to discover common group structure among the samples that may be hidden in individualistic cluster analyses of a single data view. While several techniques for such integrative clustering have been explored, we propose and develop a convex formalization that enjoys strong empirical performance and inherits the mathematical properties of increasingly popular convex clustering methods. Specifically, our Integrative Generalized Convex Clustering Optimization (iGecco) method employs different convex distances, losses, or divergences for each of the different data views with a joint convex fusion penalty that leads to common groups. Additionally, integrating mixed multi-view data is often challenging when each data source is high-dimensional. To perform feature selection in such scenarios, we develop an adaptive shifted group-lasso penalty that selects features by shrinking them towards their loss-specific centers. Our so-called iGecco+ approach selects features from each data view that are best for determining the groups, often leading to improved integrative clustering. To solve our problem, we develop a new type of generalized multi-block ADMM algorithm using sub-problem approximations that more efficiently fits our model for big data sets. Through a series of numerical experiments and real data examples on text mining and genomics, we show that iGecco+ achieves superior empirical performance for high-dimensional mixed multi-view data.",
        "BACKGROUND/PURPOSE: Sport-specific adaptations of athlete's hearts are still under investigation. This study sought to 1) identify athlete groups with similar characteristics by clustering echocardiographic data; 2) externally validate the data-driven clusters with sport classifications of various dynamic or static loads to support the conventional hypothesis-driven approach in delineating the athlete's heart. METHODS: Anthropometric, echocardiographic and electrocardiographic assessments were collected during the 2017 Summer Universiade in Taiwan. Besides standard echocardiography and strain measurements, ventricular-arterial coupling (VAC) was assessed by the ratio of effective arterial elastance (Ea) to left ventricular end-systolic elastance (Ees) as calculated by a modified single-beat algorithm. RESULTS: We grouped 598 elite athletes (348 male, age 23 +/- 2.5 years, across 24 disciplines) using Mitchell's classification. The hypothesis-driven analysis showed dynamic training-related adaptations in heart rate and morphology, including ventricular size, mass, and stroke volume. In comparison, the unsupervised approach found two clusters for each sex. Male athletes participating in high dynamic-load exercises had larger chambers, supranormal diastolic functions, depressed Ees, lower Ea and preserved optimal VAC implicating the resting status of a reservoir-rich pump, which affirmed sport-specific adaptation. The female athletes could be clustered with more noticeable functional alterations, such as depressed biventricular strain. However, the imbalanced number between clusters impeded the validation of load-related remodeling. CONCLUSION: Hierarchical clustering could analyze complicated multiparametric interactions among numerous echocardiography-derived phenotypes to discern the adaptive propensity of the athlete's heart. The endorsement or generation of hypotheses by a data-driven approach can be applied to various domains.",
        "The Automatic Quasi-clique Merger algorithm is a new algorithm adapted from early work published under the name QCM (introduced by Ou and Zhang in 2007). The AQCM algorithm performs hierarchical clustering in any data set for which there is an associated similarity measure quantifying the similarity of any data i and data j. Importantly, the method exhibits two valuable performance properties: 1) the ability to automatically return either a larger or smaller number of clusters depending on the inherent properties of the data rather than on a parameter. 2) the ability to return a very large number of relatively small clusters automatically when such clusters are reasonably well defined in a data set. In this work we present the general idea of a quasi-clique agglomerative approach, provide the full details of the mathematical steps of the AQCM algorithm, and explain some of the motivation behind the new methodology. The main achievement of the new methodology is that the agglomerative process now unfolds adaptively according to the inherent structure unique to a given data set, and this happens without the time-costly parameter adjustment that drove the previous QCM algorithm. For this reason we call the new algorithm automatic. We provide a demonstration of the algorithm's performance at the task of community detection in a social media network of 22,900 nodes.",
        "The relevance of this study lies in improvement of machine learning models understanding. We present a method for interpreting clustering results and apply it to the case of clinical pathways modeling. This method is based on statistical inference and allows to get the description of the clusters, determining the influence of a particular feature on the difference between them. Based on the proposed approach, it is possible to determine the characteristic features for each cluster. Finally, we compare the method with the Bayesian inference explanation and with the interpretation of medical experts [1].",
        "Modern shotgun proteomics experiments generate gigabytes of spectra every hour, only a fraction of which were utilized to form biological conclusions. Instead of being stored as flat files in public data repositories, this large amount of data can be better organized to facilitate data reuse. Clustering these spectra by similarity can be helpful in building high-quality spectral libraries, correcting identification errors, and highlighting frequently observed but unidentified spectra. However, large-scale clustering is time-consuming. Here, we present ClusterSheep, a method utilizing Graphics Processing Units (GPUs) to accelerate the process. Unlike previously proposed algorithms for this purpose, our method performs true pairwise comparison of all spectra within a precursor mass-to-charge ratio tolerance, thereby preserving the full cluster structures. ClusterSheep was benchmarked against previously reported clustering tools, MS-Cluster, MaRaCluster, and msCRUSH. The software tool also functions as an interactive visualization tool with a persistent state, enabling the user to explore the resulting clusters visually and retrieve the clustering results as desired.",
        "Background: Blood pressure is a critical therapeutic goal in intensive care unit (ICU). One important factor influencing blood pressure are analgesia and sedation. Analgesic and sedative drugs are commonly used in critically ill patients. These drugs affect blood pressure by reducing the tension of the venous system, the cardiac preload, and cardiac output and inhibiting cardiac functions. Consequently, vasoactive agents are commonly used to increase blood pressure. The indications for the usage of vasoactive agents are unequivocal. However, opinions on when to stop raising blood pressure vary. This study explored the relationship between blood pressure and sedation. Methods: Patients in the Multiparameter Intelligent Monitoring in Intensive Care-III (MIMIC) database who had received mechanical ventilation, had been administered sedative analgesics during their ICU stay, and met the inclusion criteria were included in this study. The mean arterial pressure (MAP) tendency patterns were identified using spectral clustering and visualized using the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm. The 28-day mortality rates of patients with different MAP patterns during their first 24 hours in the ICU and their sedation levels were calculated in the crosstab. Results: Fourteen thousand seven hundred and eighty-five patients from the MIMIC-III database were included in this study. Three MAP patterns were identified by spectral clustering. The median MAP of the low, moderate, and high MAP groups was 71.2, 80.4, and 97.6 mmHg, respectively. The 28-day mortality rate of patients in the moderate MAP group (13.0%) was lower than that of patients in the low (16.6%) and high (15.6%) MAP groups. No difference was found in the 28-day mortality rate between the low and high MAP groups. Dynamic changes in blood pressure at different sedation depths were also examined. Notably, compared with light and moderate sedation level, patients in the deep sedation group, especially those in the high MAP group (48.5%), had a higher 28-day mortality rate (36.5%). Conclusions: Low MAP in the first 24 hours in ICU indicates a high possibility of poor prognosis for critically ill patients on mechanical ventilation. For patients under deep sedation, maintaining a high mean arterial pressure also indicates poor prognosis. A personalized MAP target should be determined according to the severity of illness and level of sedation for each patient.",
        "In this article, we advance divide-and-conquer strategies for solving the community detection problem in networks. We propose two algorithms that perform clustering on several small subgraphs and finally patch the results into a single clustering. The main advantage of these algorithms is that they significantly bring down the computational cost of traditional algorithms, including spectral clustering, semidefinite programs, modularity-based methods, likelihood-based methods, etc., without losing accuracy, and even improving accuracy at times. These algorithms are also, by nature, parallelizable. Since most traditional algorithms are accurate, and the corresponding optimization problems are much simpler in small problems, our divide-and-conquer methods provide an omnibus recipe for scaling traditional algorithms up to large networks. We prove the consistency of these algorithms under various subgraph selection procedures and perform extensive simulations and real-data analysis to understand the advantages of the divide-and-conquer approach in various settings.",
        "Precision medicine is a paradigm shift in healthcare relying heavily on genomics data. However, the complexity of biological interactions, the large number of genes and the lack of comparisons on the analysis of data, remain a tremendous bottleneck regarding clinical adoption. We introduce a novel, automatic and unsupervised framework to discover low-dimensional gene biomarkers. Our method is based on the center-based LP-Stability clustering algorithm. Our evaluation includes both mathematical and biological criteria. The recovered signature is applied to several biological tasks, including screening of biological pathways and functions, and tumor types and subtypes characterization. Quantitative comparisons among different distance metrics, commonly used clustering methods and a referential gene signature from literature, confirm the high performance of our approach. In particular, our signature, based on 27 genes, reports at least 30 times better mathematical significance (Dunn's Index) and 25% better biological significance (Enrichment in Protein-Protein Interaction) than those produced by other referential clustering methods. Our signature reports promising results on distinguishing immune-inflammatory and immune desert tumors, while reporting a high balanced accuracy of 92% on tumor types classification and averaged balanced accuracy of 68% on tumor subtypes, which represents 7% and 9% higher performance compared to the referential signature.",
        "Objective: The paper applies spatial fuzzy clustering algorithm to explore the role and value of neuroendoscopic assisted technology in the operation of tumors in the saddle region, and analyze the MRI image characteristics of tumors in the saddle region. Methods: The clinical data of 63 patients from our hospital who underwent neuroendoscopic assisted microscopy to remove tumors in the saddle area from 2017 to 2019 (neuroendoscopy-assisted group) were collected. Seventy six patients who occupied the saddle area by microscopic resection only in the same period (Simple microscope group) clinical data. By comparing the patient's tumor resection rate, postoperative complication rate and postoperative recurrence rate, the surgical effect was evaluated. Results: The total resection rates of the tumors in the neuroendoscopy-assisted group and the microscope-only group were 95.24% (60/63) and 80.26% (61/76). The incidence of postoperative vasospasm was 3.17% (2/63) and 13.16% (10/76), the incidence of nerve injury was 0 (0/63) and 6.58% (5/76), the difference was statistically significant (P <0.05). There was no significant difference in the incidence of postoperative infection, cerebrospinal fluid leakage and postoperative recurrence rate between the two groups (P> 0.05). Conclusion: Neuroendoscopy-assisted microscopy-based removal of the saddle area occupying space based on spatial fuzzy clustering algorithm can increase the total tumor resection rate and reduce the incidence of complications.",
        "Multi-view clustering has become an active topic in artificial intelligence. Yet, similar investigation for graph-structured data clustering has been absent so far. To fill this gap, we present a Multi-View Graph embedding Clustering network (MVGC). Specifically, unlike traditional multi-view construction methods, which are only suitable to describe Euclidean structure data, we leverage Euler transform to augment the node attribute, as a new view descriptor, for non-Euclidean structure data. Meanwhile, we impose block diagonal representation constraint, which is measured by the l1,2-norm, on self-expression coefficient matrix to well explore the cluster structure. By doing so, the learned view-consensus coefficient matrix well encodes the discriminative information. Moreover, we make use of the learned clustering labels to guide the learnings of node representation and coefficient matrix, where the latter is used in turn to conduct the subsequent clustering. In this way, clustering and representation learning are seamlessly connected, with the aim to achieve better clustering performance. Extensive experimental results indicate that MVGC is superior to 11 state-of-the-art methods on four benchmark datasets. In particular, MVGC achieves an Accuracy of 96.17% (53.31%) on the ACM (IMDB) dataset, which is an up to 2.85% (1.97%) clustering performance improvement compared with the strongest baseline.",
        "Recent advances in high throughput technologies have made large amounts of biomedical omics data accessible to the scientific community. Single omic data clustering has proved its impact in the biomedical and biological research fields. Multi-omic data clustering and multi-omic data integration techniques have shown improved clustering performance and biological insight. Cancer subtype clustering is an important task in the medical field to be able to identify a suitable treatment procedure and prognosis for cancer patients. State of the art multi-view clustering methods are based on non-convex objectives which only guarantee non-global solutions that are high in computational complexity. Only a few convex multi-view methods are present. However, their models do not take into account the intrinsic manifold structure of the data. In this paper, we introduce a convex graph regularized multi-view clustering method that is robust to outliers. We compare our algorithm to state of the art convex and non-convex multi-view and single view clustering methods and show its superiority in clustering cancer subtypes on publicly available cancer genomic datasets from the TCGA repository. We also show our method's better ability to potentially discover cancer subtypes compared to other state of the art multi-view methods.",
        "Multiomics data clustering is one of the major challenges in the field of precision medicine. Integration of multiomics data for cancer subtyping can improve the understanding on cancer and reveal systems-level insights. How to integrate multiomics data for accurate cancer subtyping is an interesting and challenging research problem. To capture the global and the local structure of omics data, a novel framework for integrating multiomics data is proposed for cancer subtyping. Multiview clustering with low-rank and sparsity constraints (MVCLRS) can measure the local similarities of samples in each omics data and obtain global consensus structures by integrating the multiomics data. The main insight provided by MVCLRS is that low-rank sparse subspace clustering for the construction of an affinity matrix can best capture the local similarities in omics data. Extensive testing is conducted on 10 real world cancer datasets with multiomics from The Cancer Genome Atlas. Compared with 10 state-of-the-art multiomics clustering algorithms, the MVCLRS performs better in the 10 cancer datasets by providing its clustering results with at least one enriched clinical label in nine of ten cancer subtypes, the most of any method.",
        "Three-dimensional (3D) reconstruction in single-particle cryo-electron microscopy (cryo-EM) is a significant technique for recovering the 3D structure of proteins or other biological macromolecules from their two-dimensional (2D) noisy projection images taken from unknown random directions. Class averaging in single-particle cryo-EM is an important procedure for producing high-quality initial 3D structures, where image alignment is a fundamental step. In this paper, an efficient image alignment algorithm using 2D interpolation in the frequency domain of images is proposed to improve the estimation accuracy of alignment parameters of rotation angles and translational shifts between the two projection images, which can obtain subpixel and subangle accuracy. The proposed algorithm firstly uses the Fourier transform of two projection images to calculate a discrete cross-correlation matrix and then performs the 2D interpolation around the maximum value in the cross-correlation matrix. The alignment parameters are directly determined according to the position of the maximum value in the cross-correlation matrix after interpolation. Furthermore, the proposed image alignment algorithm and a spectral clustering algorithm are used to compute class averages for single-particle 3D reconstruction. The proposed image alignment algorithm is firstly tested on a Lena image and two cryo-EM datasets. Results show that the proposed image alignment algorithm can estimate the alignment parameters accurately and efficiently. The proposed method is also used to reconstruct preliminary 3D structures from a simulated cryo-EM dataset and a real cryo-EM dataset and to compare them with RELION. Experimental results show that the proposed method can obtain more high-quality class averages than RELION and can obtain higher reconstruction resolution than RELION even without iteration.",
        "A modified distorted Born iterative method (DBIM), which includes clustering of reconstructed electrical properties (EPs) after certain iterations, is presented for brain imaging aiming at stroke detection and classification. For this approach to work, a rough estimation of number of different materials (or bio-tissues) in the imaged domain and their corresponding rough dielectric properties (permittivity and conductivity) are needed as a prior information. The proposed adaptive clustering DBIM (AC-DBIM) is compared with three conventional methods (DBIM, multiplicative regularized contrast source inversion (MR-CSI), and CSI for shape and location reconstruction (SL-CSI)) in two-dimensional scenario on a head phantom and numerical head model with different strokes. Three-dimensional simulations are also conducted to indicate the suitability of AC-DBIM in real-life brain imaging. Lastly, the proposed algorithm is assessed using a clinical electromagnetic head scanner developed on phantoms. The simulation and experimental results show superiority of AC-DBIM compared to conventional methods. AC-DBIM achieves significant improvement in the size and shape reconstruction and reduction in errors and standard deviation of the reconstructed _r and at clinical scenarios compared with conventional DBIM.",
        "BACKGROUND: Gene co-expression networks are widely studied in the biomedical field, with algorithms such as WGCNA and lmQCM having been developed to detect co-expressed modules. However, these algorithms have limitations such as insufficient granularity and unbalanced module size, which prevent full acquisition of knowledge from data mining. In addition, it is difficult to incorporate prior knowledge in current co-expression module detection algorithms. RESULTS: In this paper, we propose a novel module detection algorithm based on topology potential and spectral clustering algorithm to detect co-expressed modules in gene co-expression networks. By testing on TCGA data, our novel method can provide more complete coverage of genes, more balanced module size and finer granularity than current methods in detecting modules with significant overall survival difference. In addition, the proposed algorithm can identify modules by incorporating prior knowledge. CONCLUSION: In summary, we developed a method to obtain as much as possible information from networks with increased input coverage and the ability to detect more size-balanced and granular modules. In addition, our method can integrate data from different sources. Our proposed method performs better than current methods with complete coverage of input genes and finer granularity. Moreover, this method is designed not only for gene co-expression networks but can also be applied to any general fully connected weighted network.",
        "BACKGROUND: Finding significant genes or proteins from gene chip data for disease diagnosis and drug development is an important task. However, the challenge comes from the curse of the data dimension. It is of great significance to use machine learning methods to find important features from the data and build an accurate classification model. RESULTS: The proposed method has proved superior to the published advanced hybrid feature selection method and traditional feature selection method on different public microarray data sets. In addition, the biomarkers selected using our method show a match to those provided by the cooperative hospital in a set of clinical cleft lip and palate data. METHOD: In this paper, a feature selection algorithm ILRC based on clustering and improved L1 regularization is proposed. The features are firstly clustered, and the redundant features in the sub-clusters are deleted. Then all the remaining features are iteratively evaluated using ILR. The final result is given according to the cumulative weight reordering. CONCLUSION: The proposed method can effectively remove redundant features. The algorithm's output has high stability and classification accuracy, which can potentially select potential biomarkers.",
        "Functional modules can be predicted using genome-wide protein-protein interactions (PPIs) from a systematic perspective. Various graph clustering algorithms have been applied to PPI networks for this task. In particular, the detection of overlapping clusters is necessary because a protein is involved in multiple functions under different conditions. graph entropy (GE) is a novel metric to assess the quality of clusters in a large, complex network. In this study, the unweighted and weighted GE algorithm is evaluated to prove the validity of predicting function modules. To measure clustering accuracy, the clustering results are compared to protein complexes and Gene Ontology (GO) annotations as references. We demonstrate that the GE algorithm is more accurate in overlapping clusters than the other competitive methods. Moreover, we confirm the biological feasibility of the proteins that occur most frequently in the set of identified clusters. Finally, novel proteins for the additional annotation of GO terms are revealed.",
        "Along with advances in technology, matrix data, such as medical/industrial images, have emerged in many practical fields. These data usually have high dimensions and are not easy to cluster due to their intrinsic correlated structure among rows and columns. Most approaches convert matrix data to multi dimensional vectors and apply conventional clustering methods to them, and thus, suffer from an extreme high-dimensionality problem as well as a lack of interpretability of the correlated structure among row/column variables. Recently, a regularized model was proposed for clustering matrix-valued data by imposing a sparsity structure for the mean signal of each cluster. We extend their approach by regularizing further on the covariance to cope better with the curse of dimensionality for large size images. A penalized matrix normal mixture model with lasso-type penalty terms in both mean and covariance matrices is proposed, and then an expectation maximization algorithm is developed to estimate the parameters. The proposed method has the competence of both parsimonious modeling and reflecting the proper conditional correlation structure. The estimators are consistent, and their limiting distributions are derived. We applied the proposed method to simulated data as well as real datasets and measured its clustering performance with the clustering accuracy (ACC) and the adjusted rand index (ARI). The experiment results show that the proposed method performed better with higher ACC and ARI than those of conventional methods.",
        "Classical approaches in cluster analysis are typically based on a feature space analysis. However, many applications lead to datasets with additional spatial information and a ground truth with spatially coherent classes, which will not necessarily be reconstructed well by standard clustering methods. Motivated by applications in hyperspectral imaging, we introduce in this work clustering models based on Orthogonal Nonnegative Matrix Factorization (ONMF), which include an additional Total Variation (TV) regularization procedure on the cluster membership matrix to enforce the needed spatial coherence in the clusters. We propose several approaches with different optimization techniques, where the TV regularization is either performed as a subsequent post-processing step or included into the clustering algorithm. Finally, we provide a numerical evaluation of 12 different TV regularized ONMF methods on a hyperspectral dataset obtained from a matrix-assisted laser desorption/ionization imaging measurement, which leads to significantly better clustering results compared to classical clustering models.",
        "In clinical and epidemiological studies, there is a growing interest in studying the heterogeneity among patients based on longitudinal characteristics to identify subtypes of the study population. Compared to clustering a single longitudinal marker, simultaneously clustering multiple longitudinal markers allow additional information to be incorporated into the clustering process, which reveals co-existing longitudinal patterns and generates deeper biological insight. In the current study, we propose a Bayesian consensus clustering (BCC) model for multivariate longitudinal data. Instead of arriving at a single overall clustering, the proposed model allows each marker to follow marker-specific local clustering and these local clusterings are aggregated to find a global (consensus) clustering. To estimate the posterior distribution of model parameters, a Gibbs sampling algorithm is proposed. We apply our proposed model to the primary biliary cirrhosis study to identify patient subtypes that may be associated with their prognosis. We also perform simulation studies to compare the clustering performance between the proposed model and existing models under several scenarios. The results demonstrate that the proposed BCC model serves as a useful tool for clustering multivariate longitudinal data.",
        "Multiscale brings great benefits for people to observe objects or problems from different perspectives. Multiscale clustering has been widely studied in various disciplines. However, most of the research studies are only for the numerical dataset, which is a lack of research on the clustering of nominal dataset, especially the data are nonindependent and identically distributed (Non-IID). Aiming at the current research situation, this paper proposes a multiscale clustering framework based on Non-IID nominal data. Firstly, the benchmark-scale dataset is clustered based on coupled metric similarity measure. Secondly, it is proposed to transform the clustering results from benchmark scale to target scale that the two algorithms are named upscaling based on single chain and downscaling based on Lanczos kernel, respectively. Finally, experiments are performed using five public datasets and one real dataset of the Hebei province of China. The results showed that the method can provide us not only competitive performance but also reduce computational cost.",
        "BACKGROUND: RNA-seq is a tool for measuring gene expression and is commonly used to identify differentially expressed genes (DEGs). Gene clustering is used to classify DEGs with similar expression patterns for the subsequent analyses of data from experiments such as time-courses or multi-group comparisons. However, gene clustering has rarely been used for analyzing simple two-group data or differential expression (DE). In this study, we report that a model-based clustering algorithm implemented in an R package, MBCluster.Seq, can also be used for DE analysis. RESULTS: The input data originally used by MBCluster.Seq is DEGs, and the proposed method (called MBCdeg) uses all genes for the analysis. The method uses posterior probabilities of genes assigned to a cluster displaying non-DEG pattern for overall gene ranking. We compared the performance of MBCdeg with conventional R packages such as edgeR, DESeq2, and TCC that are specialized for DE analysis using simulated and real data. Our results showed that MBCdeg outperformed other methods when the proportion of DEG (PDEG) was less than 50%. However, the DEG identification using MBCdeg was less consistent than with conventional methods. We compared the effects of different normalization algorithms using MBCdeg, and performed an analysis using MBCdeg in combination with a robust normalization algorithm (called DEGES) that was not implemented in MBCluster.Seq. The new analysis method showed greater stability than using the original MBCdeg with the default normalization algorithm. CONCLUSIONS: MBCdeg with DEGES normalization can be used in the identification of DEGs when the PDEG is relatively low. As the method is based on gene clustering, the DE result includes information on which expression pattern the gene belongs to. The new method may be useful for the analysis of time-course and multi-group data, where the classification of expression patterns is often required.",
        "MOTIVATION: Clustering is a fundamental task in the analysis of nucleotide sequences. Despite the exponential increase in the size of sequence databases of homologous genes, few methods exist to cluster divergent sequences. Traditional clustering methods have mostly focused on optimizing high speed clustering of highly similar sequences. We develop a phylogenetic clustering method which infers ancestral sequences for a set of initial clusters and then uses a greedy algorithm to cluster sequences. RESULTS: We describe a clustering program AncestralClust, which is developed for clustering divergent sequences. We compare this method with other state-of-the-art clustering methods using datasets of homologous sequences from different species. We show that, in divergent datasets, AncestralClust has higher accuracy and more even cluster sizes than current popular methods. AVAILABILITY AND IMPLEMENTATION: AncestralClust is an Open Source program available at https://github.com/lpipes/ancestralclust. SUPPLEMENTARY INFORMATION: Supplementary figures and table are available online.",
        "In this work, we developed an efficient approach to compute ensemble averages in systems with pairwise-additive energetic interactions between the entities. Methods involving full enumeration of the configuration space result in exponential complexity. Sampling methods such as Markov Chain Monte Carlo (MCMC) algorithms have been proposed to tackle the exponential complexity of these problems; however, in certain scenarios where significant energetic coupling exists between the entities, the efficiency of the such algorithms can be diminished. We used a strategy to improve the efficiency of MCMC by taking advantage of the cluster structure in the interaction energy matrix to bias the sampling. We pursued two different schemes for the biased MCMC runs and show that they are valid MCMC schemes. We used both synthesized and real-world systems to show the improved performance of our biased MCMC methods when compared to the regular MCMC method. In particular, we applied these algorithms to the problem of estimating protonation ensemble averages and titration curves of residues in a protein.",
        "Diabetics are prone to postoperative cognitive dysfunction (POCD). The occurrence may be related to the damage of the prefrontal lobe. In this study, the prefrontal lobe was segmented based on an improved clustering algorithm in patients with diabetes, in order to evaluate the relationship between prefrontal lobe volume and COPD. In this study, a total of 48 diabetics who underwent selective noncardiac surgery were selected. Preoperative magnetic resonance imaging (MRI) images of the patients were segmented based on the improved clustering algorithm, and their prefrontal volume was measured. The correlation between the volume of the prefrontal lobe and Z-score or blood glucose was analyzed. Qualitative analysis shows that the gray matter, white matter, and cerebrospinal fluid based on the improved clustering algorithm were easy to distinguish. Quantitative evaluation results show that the proposed segmentation algorithm can obtain the optimal Jaccard coefficient and the least average segmentation time. There was a negative correlation between the volume of the prefrontal lobe and the Z-score. The cut-off value of prefrontal lobe volume for predicting POCD was <179.8, with the high specificity. There was a negative correlation between blood glucose and volume of the prefrontal lobe. From the results, we concluded that the segmentation of the prefrontal lobe based on an improved clustering algorithm before operation may predict the occurrence of POCD in diabetics.",
        "Chromatin accessibility, as measured by ATACseq, varies between hematopoietic cell types in different lineages of the hematopoietic differentiation tree, e.g. T cells vs. B cells, but methods that associate variation in chromatin accessibility to the lineage structure of the differentiation tree are lacking. Using an ATACseq dataset recently published by the ImmGen consortium, we construct associations between chromatin accessibility and hematopoietic cell types using a novel co-clustering approach that accounts for the structure of the hematopoietic, differentiation tree. Under a model in which all loci and cell types within a co-cluster have a shared accessibility state, we show that roughly 80% of cell type associated accessibility variation can be captured through 12 cell type clusters and 20 genomic locus clusters, with the cell type clusters reflecting coherent components of the differentiation tree. Using publicly available ChIPseq datasets, we show that our clustering reflects transcription factor binding patterns with implications for regulation across cell types. We show that traditional methods such as hierarchical and kmeans clusterings lead to cell type clusters that are more dispersed on the tree than our tree-based algorithm. We provide a python package, chromcocluster, that implements the algorithms presented.",
        "Background: Concussion subtypes are typically organized into commonly affected symptom areas or a combination of affected systems, an approach that may be flawed by bias in conceptualization or the inherent limitations of interdisciplinary expertise. Objective: The purpose of this study was to determine whether a bottom-up, unsupervised, machine learning approach, could more accurately support concussion subtyping. Methods: Initial patient intake data as well as objective outcome measures including, the Patient-Reported Outcomes Measurement Information System (PROMIS), Dizziness Handicap Inventory (DHI), Pain Catastrophizing Scale (PCS), and Immediate Post-Concussion Assessment and Cognitive Testing Tool (ImPACT) were retrospectively extracted from the Advance Concussion Clinic's database. A correlation matrix and principal component analysis (PCA) were used to reduce the dimensionality of the dataset. Sklearn's agglomerative clustering algorithm was then applied, and the optimal number of clusters within the patient database were generated. Between-group comparisons among the formed clusters were performed using a Mann-Whitney U test. Results: Two hundred seventy-five patients within the clinics database were analyzed. Five distinct clusters emerged from the data when maximizing the Silhouette score (0.36) and minimizing the Davies-Bouldin score (0.83). Concussion subtypes derived demonstrated clinically distinct profiles, with statistically significant differences (p < 0.05) between all five clusters. Conclusion: This machine learning approach enabled the identification and characterization of five distinct concussion subtypes, which were best understood according to levels of complexity, ranging from Extremely Complex to Minimally Complex. Understanding concussion in terms of Complexity with the utilization of artificial intelligence, could provide a more accurate concussion classification or subtype approach; one that better reflects the true heterogeneity and complex system disruptions associated with mild traumatic brain injury.",
        "This paper is mainly aimed at the decomposition of image quality assessment study by using Three Parameter Logistic Mixture Model and k-means clustering (TPLMM-k). This method is mainly used for the analysis of various images which were related to several real time applications and for medical disease detection and diagnosis with the help of the digital images which were generated by digital microscopic camera. Several algorithms and distribution models had been developed and proposed for the segmentation of the images. Among several methods developed and proposed, the Gaussian Mixture Model (GMM) was one of the highly used models. One can say that almost the GMM was playing the key role in most of the image segmentation research works so far noticed in the literature. The main drawback with the distribution model was that this GMM model will be best fitted with a kind of data in the dataset. To overcome this problem, the TPLMM-k algorithm is proposed. The image decomposition process used in the proposed algorithm had been analyzed and its performance was analyzed with the help of various performance metrics like the Variance of Information (VOI), Global Consistency Error (GCE) and Probabilistic Rand Index (PRI). According to the results, it is shown that the proposed algorithm achieves the better performance when compared with the previous results of the previous techniques. In addition, the decomposition of the images had been improved in the proposed algorithm.",
        "Distinct microbial signatures associated with specific human body sites can play a role in the identification of biological materials recovered from the crime scene, but at present, methods that have capability to predict origin of biological materials based on such signatures are limited. Metagenomic sequencing and machine learning (ML) offer a promising enhancement to current identification protocols. We use ML for forensic source body site identification using shotgun metagenomic sequenced data to verify the presence of microbiomic signatures capable of discriminating between source body sites and then show that accurate prediction is possible. The consistency between cluster membership and actual source body site (purity) exceeded 99% at the genus taxonomy using off-the-shelf ML clustering algorithms. Similar results were obtained at the family level. Accurate predictions were observed for genus, family, and order taxonomies, as well as with a core set of 51 genera. The accurate outcomes from our replicable process should encourage forensic scientists to seriously consider integrating ML predictors into their source body site identification protocols.",
        "The identification of protein complexes in protein-protein interaction networks is the most fundamental and essential problem for revealing the underlying mechanism of biological processes. However, most existing protein complexes identification methods only consider a network's topology structures, and in doing so, these methods miss the advantage of using nodes' feature information. In protein-protein interaction, both topological structure and node features are essential ingredients for protein complexes. The spectral clustering method utilizes the eigenvalues of the affinity matrix of the data to map to a low-dimensional space. It has attracted much attention in recent years as one of the most efficient algorithms in the subcategory of dimensionality reduction. In this paper, a new version of spectral clustering, named text-associated DeepWalk-Spectral Clustering (TADW-SC), is proposed for attributed networks in which the identified protein complexes have structural cohesiveness and attribute homogeneity. Since the performance of spectral clustering heavily depends on the effectiveness of the affinity matrix, our proposed method will use the text-associated DeepWalk (TADW) to calculate the embedding vectors of proteins. In the following, the affinity matrix will be computed by utilizing the cosine similarity between the two low dimensional vectors, which will be considerable to improve the accuracy of the affinity matrix. Experimental results show that our method performs unexpectedly well in comparison to existing state-of-the-art methods in both real protein network datasets and synthetic networks.",
        "Multiple kernel clustering (MKC) optimally utilizes a group of pre-specified base kernels to improve clustering performance. Among existing MKC algorithms, the recently proposed late fusion MKC methods demonstrate promising clustering performance in various applications and enjoy considerable computational acceleration. However, we observe that the kernel partition learning and late fusion processes are separated from each other in the existing mechanism, which may lead to suboptimal solutions and adversely affect the clustering performance. In this article, we propose a novel late fusion multiple kernel clustering with proxy graph refinement (LFMKC-PGR) framework to address these issues. First, we theoretically revisit the connection between late fusion kernel base partition and traditional spectral embedding. Based on this observation, we construct a proxy self-expressive graph from kernel base partitions. The proxy graph in return refines the individual kernel partitions and also captures partition relations in graph structure rather than simple linear transformation. We also provide theoretical connections and considerations between the proposed framework and the multiple kernel subspace clustering. An alternate algorithm with proved convergence is then developed to solve the resultant optimization problem. After that, extensive experiments are conducted on 12 multi-kernel benchmark datasets, and the results demonstrate the effectiveness of our proposed algorithm. The code of the proposed algorithm is publicly available at https://github.com/wangsiwei2010/graphlatefusion_MKC.",
        "We propose a dual system for unsupervised object segmentation in video, which brings together two modules with complementary properties: a space-time graph that discovers objects in videos and a deep network that learns powerful object features. The system uses an iterative knowledge exchange policy. A novel spectral space-time clustering process on the graph produces unsupervised segmentation masks passed to the network as pseudo-labels. The net learns to segment in single frames what the graph discovers in video and passes back to the graph strong image-level features that improve its node-level features in the next iteration. Knowledge is exchanged for several cycles until convergence. The graph has one node per each video pixel, but the object discovery is fast. It uses a novel power iteration algorithm computing the main space-time cluster as the principal eigenvector of a special Feature-Motion matrix without actually computing the matrix. The thorough experimental analysis validates our theoretical claims and proves the effectiveness of the cyclical knowledge exchange. We also perform experiments on the supervised scenario, incorporating features pretrained with human supervision. We achieve state-of-the-art level on unsupervised and supervised scenarios on four challenging datasets: DAVIS, SegTrack, YouTube-Objects, and DAVSOD. We will make our code publicly available.",
        "Clustering analysis is one of the most important technologies for single-cell data mining. It is widely used in the division of different gene sequences, the identification of functional genes, and the detection of new cell types. Although the traditional unsupervised clustering method does not require label data, the distribution of the original data, the setting of hyperparameters, and other factors all affect the effectiveness of the clustering algorithm. While in some cases the type of some cells is known, it is hoped to achieve high accuracy if the prior information about those cells is utilized sufficiently. In this study, we propose SCMAG (a semisupervised single-cell clustering method based on a matrix aggregation graph convolutional neural network) that takes into full consideration the prior information for single-cell data. To evaluate the performance of the proposed semisupervised clustering method, we test on different single-cell datasets and compare with the current semisupervised clustering algorithm in recognizing cell types on various real scRNA-seq data; the results show that it is a more accurate and significant model.",
        "Multivariate time-dependent data, where multiple features are observed over time for a set of individuals, are increasingly widespread in many application domains. To model these data, we need to account for relations among both time instants and variables and, at the same time, for subject heterogeneity. We propose a new co-clustering methodology for grouping individuals and variables simultaneously, designed to handle both functional and longitudinal data. Our approach borrows some concepts from the curve registration framework by embedding the shape invariant model in the latent block model, estimated via a suitable modification of the SEM-Gibbs algorithm. The resulting procedure allows for several user-defined specifications of the notion of cluster that can be chosen on substantive grounds and provides parsimonious summaries of complex time-dependent data by partitioning data matrices into homogeneous blocks. Along with the explicit modelling of time evolution, these aspects allow for an easy interpretation of the clusters, from which also low-dimensional settings may benefit.",
        "The IoT-enabled smart grid system provides smart meter data for electricity consumers to record their energy consumption behaviors, the typical features of which can be represented by the load patterns extracted from load data clustering. The changeability of consumption behaviors requires load pattern update for achieving accurate consumer segmentation and effective demand response. In order to save training time and reduce computation scale, we propose a novel incremental clustering algorithm with probability strategy, ICluster-PS, instead of overall load data clustering to update load patterns. ICluster-PS first conducts new load pattern extraction based on the existing load patterns and new data. Then, it intergrades new load patterns with the existing ones. Finally, it optimizes the intergraded load pattern sets by a further modification. Moreover, ICluster-PS can be performed continuously with new coming data due to parameter updating and generalization. Extensive experiments are implemented on real-world dataset containing diverse consumer types in various districts. The experimental results are evaluated by both clustering validity indices and accuracy measures, which indicate that ICluster-PS outperforms other related incremental clustering algorithm. Additionally, according to the further case studies on pattern evolution analysis, ICluster-PS is able to present any pattern drifts through its incremental clustering results.",
        "The analysis of energy loss near edge structures in EELS is a powerful method for a precise characterization of elemental oxidation states and local atomic coordination with an outstanding lateral resolution, down to the atomic scale. Given the complexity and sizes of the EELS spectrum images datasets acquired by the state-of-the-art instrumentation, methods with low convergence times are usually preferred for spectral unmixing in quantitative analysis, such as multiple linear least squares fittings. Nevertheless, non-linear least squares fitting may be a superior choice for analysis in some cases, as it eliminates the need of calibrated reference spectra and provides information for each of the individual components included in the fitted model. To avoid some of the problems that the non-linear least squares algorithms may suffer dealing with mixed-composition samples and, thus, a model comprised by a large number of individual curves we proposed the combination of clustering analysis for segmentation and non-linear least squares fitting for spectral analysis. Clustering analysis is capable of a fast classification of pixels in smaller subsets divided by their spectral characteristics, and thus increases the control over the model parameters in separated regions of the samples, classified by their specific compositions. Furthermore, along with this manuscript we provide access to a self-contained and expandable modular software solution called WhatEELS. It was specifically designed to facilitate the combined use of clustering and NLLS, and includes a set of tools for white-lines analysis and elemental quantification. We successfully demonstrated its capabilities with a control sample of mesoporous cerium oxide doped with praseodymium and gadolinium, which posed challenging case-study given its spectral characteristics.",
        "In this article, we focus on utilizing the idea of co-clustering algorithms to address the subspace clustering problem. In recent years, co-clustering methods have been developed greatly with many important applications, such as document clustering and gene expression analysis. Different from the traditional graph-based methods, co-clustering can utilize the bipartite graph to extract the duality relationship between samples and features. It means that the bipartite graph can obtain more information than other traditional graph methods. Therefore, we proposed a novel method to handle the subspace clustering problem by combining dictionary learning with a bipartite graph under the constraint of the (normalized) Laplacian rank. Besides, to avoid the effect of redundant information hiding in the data, the original data matrix is not used as the static dictionary in our model. By updating the dictionary matrix under the sparse constraint, we can obtain a better coefficient matrix to construct the bipartite graph. Based on Theorem 2 and Lemma 1, we further speed up our algorithm. Experimental results on both synthetic and benchmark datasets demonstrate the superior effectiveness and stability of our model.",
        "BACKGROUND: Trials of lumbar spondylolisthesis are difficult to compare because of the heterogeneity in the populations studied. OBJECTIVE: To define patterns of clinical presentation. METHODS: This is a study of the prospective Quality Outcomes Database spondylolisthesis registry, including patients who underwent single-segment surgery for grade 1 degenerative lumbar spondylolisthesis. Twenty-four-month patient-reported outcomes (PROs) were collected. A k-means clustering analysis-an unsupervised machine learning algorithm-was used to identify clinical presentation phenotypes. RESULTS: Overall, 608 patients were identified, of which 507 (83.4%) had 24-mo follow-up. Clustering revealed 2 distinct cohorts. Cluster 1 (high disease burden) was younger, had higher body mass index (BMI) and American Society of Anesthesiologist (ASA) grades, and globally worse baseline PROs. Cluster 2 (intermediate disease burden) was older and had lower BMI and ASA grades, and intermediate baseline PROs. Baseline radiographic parameters were similar (P > .05). Both clusters improved clinically (P < .001 all 24-mo PROs). In multivariable adjusted analyses, mean 24-mo Oswestry Disability Index (ODI), Numeric Rating Scale Back Pain (NRS-BP), Numeric Rating Scale Leg Pain, and EuroQol-5D (EQ-5D) were markedly worse for the high-disease-burden cluster (adjusted-P < .001). However, the high-disease-burden cluster demonstrated greater 24-mo improvements for ODI, NRS-BP, and EQ-5D (adjusted-P < .05) and a higher proportion reaching ODI minimal clinically important difference (MCID) (adjusted-P = .001). High-disease-burden cluster had lower satisfaction (adjusted-P = .02). CONCLUSION: We define 2 distinct phenotypes-those with high vs intermediate disease burden-operated for lumbar spondylolisthesis. Those with high disease burden were less satisfied, had a lower quality of life, and more disability, more back pain, and more leg pain than those with intermediate disease burden, but had greater magnitudes of improvement in disability, back pain, quality of life, and more often reached ODI MCID.",
        "The lowest-energy structures of AgCu nanoalloys are searched for by global optimization algorithms for sizes 100 and 200 atoms depending on composition. Even though the AgCu system is very weakly miscible in macroscopic samples, the mixing energy for these nanoalloys turns out to be clearly negative for both sizes, a result which is attributed to the stabilization of non-crystalline Cu@Ag core-shell structures at the nanoscale. The mixing energy is a quantity nowadays unknown in its functional form, so that its prediction may take advantage of machine learning techniques. A support vector regressor is then implemented to successfully predict the mixing energy of AgCu nanoalloys of both sizes. Moreover, with the help of unsupervised learning algorithms, it is shown that the automatic classification of such nanoalloys into different physically meaningful structural families is indeed possible. Finally, thanks to the harmonic superposition approximation, the temperature-dependent probabilities of such structural families are calculated.",
        "Identification of protein complexes from protein-protein interaction (PPI) networks is a key problem in PPI mining, solved by parameter-dependent approaches that suffer from small recall rates. Here we introduce GCC-v, a family of efficient, parameter-free algorithms to accurately predict protein complexes using the (weighted) clustering coefficient of proteins in PPI networks. Through comparative analyses with gold standards and PPI networks from Escherichia coli, Saccharomyces cerevisiae, and Homo sapiens, we demonstrate that GCC-v outperforms twelve state-of-the-art approaches for identification of protein complexes with respect to twelve performance measures in at least 85.71% of scenarios. We also show that GCC-v results in the exact recovery of approximately 35% of protein complexes in a pan-plant PPI network and discover 144 new protein complexes in Arabidopsis thaliana, with high support from GO semantic similarity. Our results indicate that findings from GCC-v are robust to network perturbations, which has direct implications to assess the impact of the PPI network quality on the predicted protein complexes.",
        "MOTIVATION: Recent advancements in fluorescence in situ hybridization (FISH) techniques enable them to concurrently obtain information on the location and gene expression of single cells. A key question in the initial analysis of such spatial transcriptomics data is the assignment of cell types. To date, most studies used methods that only rely on the expression levels of the genes in each cell for such assignments. To fully utilize the data and to improve the ability to identify novel sub-types we developed a new method, FICT, which combines both expression and neighborhood information when assigning cell types. RESULTS: FICT optimizes a probabilistic function that we formalize and for which we provide learning and inference algorithms. We used FICT to analyze both simulated and several real spatial transcriptomics data. As we show, FICT can accurately identify cell types and sub-types improving on expression only methods and other methods proposed for clustering spatial transcriptomics data. Some of the spatial subtypes identified by FICT provide novel hypotheses about the new functions for excitatory and inhibitory neurons. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics FICT is available at: https://github.com/haotianteng/FICT.",
        "In this article, we elaborate on a Kullback-Leibler (KL) divergence-based Fuzzy C-Means (FCM) algorithm by incorporating a tight wavelet frame transform and morphological reconstruction (MR). To make membership degrees of each image pixel closer to those of its neighbors, a KL divergence term on the partition matrix is introduced as a part of FCM, thus resulting in KL divergence-based FCM. To make the proposed FCM robust, a filtered term is augmented in its objective function, where MR is used for image filtering. Since tight wavelet frames provide redundant representations of images, the proposed FCM is performed in a feature space constructed by tight wavelet frame decomposition. To further improve its segmentation accuracy (SA), a segmented feature set is reconstructed by minimizing the inverse process of its objective function. Each reconstructed feature is reassigned to the closest prototype, thus modifying abnormal features produced in the reconstruction process. Moreover, a segmented image is reconstructed by using tight wavelet frame reconstruction. Finally, supporting experiments coping with synthetic, medical, and real-world images are reported. The experimental results exhibit that the proposed algorithm works well and comes with better segmentation performance than other peers. In a quantitative fashion, its average SA improvements over its peers are 4.06%, 3.94%, and 4.41%, respectively, when segmenting synthetic, medical, and real-world images. Moreover, the proposed algorithm requires less time than most of the FCM-related algorithms.",
        "Background: Machine learning is one kind of machine intelligence technique that learns from data and detects inherent patterns from large, complex datasets. Due to this capability, machine learning techniques are widely used in medical applications, especially where large-scale genomic and proteomic data are used. Cancer classification based on bio-molecular profiling data is a very important topic for medical applications since it improves the diagnostic accuracy of cancer and enables a successful culmination of cancer treatments. Hence, machine learning techniques are widely used in cancer detection and prognosis. Methods: In this article, a new ensemble machine learning classification model named Multiple Filtering and Supervised Attribute Clustering algorithm based Ensemble Classification model (MFSAC-EC) is proposed which can handle class imbalance problem and high dimensionality of microarray datasets. This model first generates a number of bootstrapped datasets from the original training data where the oversampling procedure is applied to handle the class imbalance problem. The proposed MFSAC method is then applied to each of these bootstrapped datasets to generate sub-datasets, each of which contains a subset of the most relevant/informative attributes of the original dataset. The MFSAC method is a feature selection technique combining multiple filters with a new supervised attribute clustering algorithm. Then for every sub-dataset, a base classifier is constructed separately, and finally, the predictive accuracy of these base classifiers is combined using the majority voting technique forming the MFSAC-based ensemble classifier. Also, a number of most informative attributes are selected as important features based on their frequency of occurrence in these sub-datasets. Results: To assess the performance of the proposed MFSAC-EC model, it is applied on different high-dimensional microarray gene expression datasets for cancer sample classification. The proposed model is compared with well-known existing models to establish its effectiveness with respect to other models. From the experimental results, it has been found that the generalization performance/testing accuracy of the proposed classifier is significantly better compared to other well-known existing models. Apart from that, it has been also found that the proposed model can identify many important attributes/biomarker genes.",
        "Feature selection (marker gene selection) is widely believed to improve clustering accuracy, and is thus a key component of single cell clustering pipelines. Existing feature selection methods perform inconsistently across datasets, occasionally even resulting in poorer clustering accuracy than without feature selection. Moreover, existing methods ignore information contained in gene-gene correlations. Here, we introduce DUBStepR (Determining the Underlying Basis using Stepwise Regression), a feature selection algorithm that leverages gene-gene correlations with a novel measure of inhomogeneity in feature space, termed the Density Index (DI). Despite selecting a relatively small number of genes, DUBStepR substantially outperformed existing single-cell feature selection methods across diverse clustering benchmarks. Additionally, DUBStepR was the only method to robustly deconvolve T and NK heterogeneity by identifying disease-associated common and rare cell types and subtypes in PBMCs from rheumatoid arthritis patients. DUBStepR is scalable to over a million cells, and can be straightforwardly applied to other data types such as single-cell ATAC-seq. We propose DUBStepR as a general-purpose feature selection solution for accurately clustering single-cell data.",
        "Since the hippocampus is of small size, low contrast, and irregular shape, a novel hippocampus segmentation method based on subspace patch-sparsity clustering in brain MRI is proposed to improve the segmentation accuracy, which requires that the representation coefficients in different subspaces should be as sparse as possible, while the representation coefficients in the same subspace should be as average as possible. By restraining the coefficient matrix with the patch-sparse constraint, the coefficient matrix contains a patch-sparse structure, which is helpful to the hippocampus segmentation. The experimental results show that our proposed method is effective in the noisy brain MRI data, which can well deal with hippocampus segmentation problem.",
        "The present work reports an efficient way of capturing real-time crack propagation in concrete structures. The modified spectral analysis based algorithm and finite element modeling (FEM) were utilised for crack detection and quantitative analysis of crack propagation. Crack propagation was captured in cement-based composite (CBC) containing saw dust and M20 grade concrete under compressive loading using a simple and inexpensive 8-megapixel mobile phone camera. The randomly selected images showing crack initiation and propagation in CBCs demonstrated the crack capturing capability of developed algorithm. A measure of oriented energy was provided at crack edges to develop a similarity spatial relationship among the pairwise pixels. FE modelling was used for distress anticipation, by analysing stresses during the compressive test in constituents of CBCs. FE modeling jointly with the developed algorithm, can provide real-time inputs from the crack-prone areas and useful in early crack detection of concrete structures for preventive support and management.",
        "BACKGROUND: Globally, gastric cancer (GC) is the fifth most common tumor. It is necessary to identify novel molecular subtypes to guide patient selection for specific target therapeutic benefits. METHODS: Multi-omics data, including transcriptomics RNA-sequencing (mRNA, LncRNA, miRNA), DNA methylation, and gene mutations in the TCGA-STAD cohort were used for the clustering. Ten classical clustering algorithms were executed to recognize patients with different molecular features using the \"MOVICS\" package in R. The activated signaling pathways were evaluated using the single-sample gene set enrichment analysis. The differential distribution of gene mutations, copy number alterations, and tumor mutation burden was compared, and potential responses to immunotherapy and chemotherapy were also assessed. RESULTS: Two molecular subtypes (CS1 and CS2) were recognized by ten clustering algorithms with consensus ensembles. Patients in the CS1 group had a shorter average overall survival time (28.5 vs. 68.9 months, P = 0.016), and progression-free survival (19.0 vs. 63.9 months, P = 0.008) as compared to those in the CS2 group. Extracellular associated biological process activation was higher in the CS1 group, while the CS2 group displayed the enhanced activation of cell cycle-associated pathways. Significantly higher total mutation numbers and neoantigens were observed in the CS2 group, along with specific mutations in TTN, MUC16, and ARID1A. Higher infiltration of immunocytes was also observed in the CS2 group, reflective of the potential immunotherapeutic benefits. Moreover, the CS2 group could also respond to 5-fluorouracil, cisplatin, and paclitaxel. The similar diversity in clinical outcomes between CS1 and CS2 groups was successfully validated in the external cohorts, GSE62254, GSE26253, GSE15459, and GSE84437. CONCLUSION: The findings provided novel insights into the GC subtypes through integrative analysis of five -omics data by ten clustering algorithms. These could provide potential clinical therapeutic targets based on the specific molecular features.",
        "BACKGROUND: The classification of motor imagery electroencephalogram (MI-EEG) is a pivotal task in the biosignal classification process in the brain-computer interface (BCI) applications. Currently, this bio-engineering-based technology is being employed by researchers in various fields to develop cutting-edge applications. The classification of real-time MI-EEG signals is the most challenging task in these applications. The prediction performance of the existing classification methods is still limited due to the high dimensionality and dynamic behaviors of the real-time EEG data. PROPOSED METHOD: To enhance the classification performance of real-time BCI applications, this paper presents a new clustering-based ensemble technique called CluSem to mitigate this problem. We also develop a new brain game called CluGame using this method to evaluate the classification performance of real-time motor imagery movements. In this game, real-time EEG signal classification and prediction tabulation through animated balls are controlled via threads. By playing this game, users can control the movements of the balls via the brain signals of motor imagery movements without using any traditional input devices. RESULTS: Our results demonstrate that CluSem is able to improve the classification accuracy between 5% and 15% compared to the existing methods on our collected as well as the publicly available EEG datasets. The source codes used to implement CluSem and CluGame are publicly available at https://github.com/MdOchiuddinMiah/MI-BCI_ML.",
        "With the increasing number of samples, the manual clustering of COVID-19 and medical disease data samples becomes time-consuming and requires highly skilled labour. Recently, several algorithms have been used for clustering medical datasets deterministically; however, these definitions have not been effective in grouping and analysing medical diseases. The use of evolutionary clustering algorithms may help to effectively cluster these diseases. On this presumption, we improved the current evolutionary clustering algorithm star (ECA*), called iECA*, in three manners: (i) utilising the elbow method to find the correct number of clusters; (ii) cleaning and processing data as part of iECA* to apply it to multivariate and domain-theory datasets; (iii) using iECA* for real-world applications in clustering COVID-19 and medical disease datasets. Experiments were conducted to examine the performance of iECA* against state-of-the-art algorithms using performance and validation measures (validation measures, statistical benchmarking, and performance ranking framework). The results demonstrate three primary findings. First, iECA* was more effective than other algorithms in grouping the chosen medical disease datasets according to the cluster validation criteria. Second, iECA* exhibited the lower execution time and memory consumption for clustering all the datasets, compared to the current clustering methods analysed. Third, an operational framework was proposed to rate the effectiveness of iECA* against other algorithms in the datasets analysed, and the results indicated that iECA* exhibited the best performance in clustering all medical datasets. Further research is required on real-world multi-dimensional data containing complex knowledge fields for experimental verification of iECA* compared to evolutionary algorithms.",
        "Multiple kernel alignment (MKA) maximization criterion has been widely applied into multiple kernel clustering (MKC) and many variants have been recently developed. Though demonstrating superior clustering performance in various applications, it is observed that none of them can effectively handle incomplete MKC, where parts or all of the pre-specified base kernel matrices are incomplete. To address this issue, we propose to integrate the imputation of incomplete kernel matrices and MKA maximization for clustering into a unified learning framework. The clustering of MKA maximization guides the imputation of incomplete kernel elements, and the completed kernel matrices are in turn combined to conduct the subsequent MKC. These two procedures are alternately performed until convergence. By this way, the imputation and MKC processes are seamlessly connected, with the aim to achieve better clustering performance. Besides theoretically analyzing the clustering generalization error bound, we empirically evaluate the clustering performance on five multiple kernel learning (MKL) benchmark datasets, and the results indicate the superiority of our algorithm over existing state-of-the-art counterparts. Our codes and data are publicly available at \\url{https://xinwangliu.github.io/}.",
        "Due to the popularity of social media and online fora, such as Twitter, Reddit, Facebook, and Wechat, short text stream clustering has gained significant attention in recent years. However, most existing short text stream clustering approaches usually work on static data and tend to cause a ``term ambiguity'' problem due to the sparse word representation. Beyond, they often exploit short text streams in a batch way and are difficult to find evolving topics in term-changing subspaces. In this article, we propose an online semantic-enhanced graphical model for evolving short text stream clustering (OSGM), by exploiting the word-occurrence semantic information and dynamically maintaining evolving active topics in term-changing subspaces in an online way. Compared to the existing approaches, our online model is not only free of determining the optimal batch size but also lends itself to handling large-scale data streams efficiently. It is also able to handle the ``term ambiguity'' problem without incorporating features from external resources. More importantly, to the best of our knowledge, it is the first work to extract evolving topics in term-changing subspaces automatically in an online way. Extensive experiments demonstrate that the proposed model yields better performance compared to many state-of-the-art algorithms on both synthetic and real-world datasets.",
        "Background and Objectives: Despite the association between hyperchloremia and adverse outcomes, mortality risks among patients with hyperchloremia have not consistently been observed among all studies with different patient populations with hyperchloremia. The objective of this study was to characterize hyperchloremic patients at hospital admission into clusters using an unsupervised machine learning approach and to evaluate the mortality risk among these distinct clusters. Materials and Methods: We performed consensus cluster analysis based on demographic information, principal diagnoses, comorbidities, and laboratory data among 11,394 hospitalized adult patients with admission serum chloride of >108 mEq/L. We calculated the standardized mean difference of each variable to identify each cluster's key features. We assessed the association of each hyperchloremia cluster with hospital and one-year mortality. Results: There were three distinct clusters of patients with admission hyperchloremia: 3237 (28%), 4059 (36%), and 4098 (36%) patients in clusters 1 through 3, respectively. Cluster 1 was characterized by higher serum chloride but lower serum sodium, bicarbonate, hemoglobin, and albumin. Cluster 2 was characterized by younger age, lower comorbidity score, lower serum chloride, and higher estimated glomerular filtration (eGFR), hemoglobin, and albumin. Cluster 3 was characterized by older age, higher comorbidity score, higher serum sodium, potassium, and lower eGFR. Compared with cluster 2, odds ratios for hospital mortality were 3.60 (95% CI 2.33-5.56) for cluster 1, and 4.83 (95% CI 3.21-7.28) for cluster 3, whereas hazard ratios for one-year mortality were 4.49 (95% CI 3.53-5.70) for cluster 1 and 6.96 (95% CI 5.56-8.72) for cluster 3. Conclusions: Our cluster analysis identified three clinically distinct phenotypes with differing mortality risks in hospitalized patients with admission hyperchloremia.",
        "OBJECTIVE: Deep significance clustering (DICE) is a self-supervised learning framework. DICE identifies clinically similar and risk-stratified subgroups that neither unsupervised clustering algorithms nor supervised risk prediction algorithms alone are guaranteed to generate. MATERIALS AND METHODS: Enabled by an optimization process that enforces statistical significance between the outcome and subgroup membership, DICE jointly trains 3 components, representation learning, clustering, and outcome prediction while providing interpretability to the deep representations. DICE also allows unseen patients to be predicted into trained subgroups for population-level risk stratification. We evaluated DICE using electronic health record datasets derived from 2 urban hospitals. Outcomes and patient cohorts used include discharge disposition to home among heart failure (HF) patients and acute kidney injury among COVID-19 (Cov-AKI) patients, respectively. RESULTS: Compared to baseline approaches including principal component analysis, DICE demonstrated superior performance in the cluster purity metrics: Silhouette score (0.48 for HF, 0.51 for Cov-AKI), Calinski-Harabasz index (212 for HF, 254 for Cov-AKI), and Davies-Bouldin index (0.86 for HF, 0.66 for Cov-AKI), and prediction metric: area under the Receiver operating characteristic (ROC) curve (0.83 for HF, 0.78 for Cov-AKI). Clinical evaluation of DICE-generated subgroups revealed more meaningful distributions of member characteristics across subgroups, and higher risk ratios between subgroups. Furthermore, DICE-generated subgroup membership alone was moderately predictive of outcomes. DISCUSSION: DICE addresses a gap in current machine learning approaches where predicted risk may not lead directly to actionable clinical steps. CONCLUSION: DICE demonstrated the potential to apply in heterogeneous populations, where having the same quantitative risk does not equate with having a similar clinical profile.",
        "The guided filter is a novel explicit image filtering method, which implements a smoothing filter on \"flat patch\" regions and ensures edge preserving on \"high variance\" regions. Recently, the guided filter has been successfully incorporated into the process of fuzzy c-means (FCM) to boost the clustering results of noisy images. However, the adaptability of the existing guided filter-based FCM methods to different images is deteriorated, as the factor epsilon of the guided filter is fixed to a scalar. To solve this issue, this paper proposes a new guided filter-based FCM method (IFCM_GF), in which the guidance image of the guided filter is adjusted by a newly defined influence factor rho. By dynamically changing the impact factor rho, the IFCM_GF acquires excellent segmentation results on various noisy images. Furthermore, to promote the segmentation accuracy of images with heavy noise and simplify the selection of the influence factor rho, we further propose a morphological reconstruction-based improved FCM clustering algorithm with guided filter (MRIFCM_GF). In this approach, the original noisy image is reconstructed by the morphological reconstruction (MR) before clustering, and the IFCM_GF is performed on the reconstructed image by utilizing the adjusted guidance image. Due to the efficiency of the MR to remove noise, the MRIFCM_GF achieves better segmentation results than the IFCM_GF on images with heavy noise and the selection of the influence factor for the MRIFCM_GF is simple. Experiments demonstrate the effectiveness of the presented methods.",
        "The improvement of teachers' educational technology ability is one of the main methods to improve the management efficiency of colleges and universities in China, and the scientific evaluation of teachers' ability is of great significance. In view of this, this study proposes an evaluation model of teachers' educational technology ability based on the fuzzy clustering generalized regression neural network. Firstly, the comprehensive evaluation structure system of teachers' educational technology ability is constructed, and then the prediction method of teachers' ability based on fuzzy clustering algorithm is analysed. On this basis, the optimization prediction method of fuzzy clustering generalized regression neural network is proposed. Finally, the application effect of fuzzy clustering generalized regression neural network in the evaluation of teachers' educational technology ability is analysed. The results show that the evaluation system of teachers' educational technology ability proposed in this study is scientific and reasonable; fuzzy clustering generalized regression neural network model can better accurately predict the ability of teachers' educational technology and can quickly realize global optimization. According to the fitness analysis results of the fuzzy clustering generalized regression neural network model, the model converges after the 20th iteration and the fitness value remains about 1.45. Therefore, the fuzzy clustering generalized regression neural network has stronger adaptability and has been optimized to a certain extent. The average evaluation accuracy of fuzzy clustering generalized regression neural network model is 98.44%, and the evaluation results of the model are better than other algorithms. It is hoped that this study can provide some reference value for the evaluation of teachers' educational technology ability in colleges and universities in China.",
        "Benchmark datasets with predefined cluster structures and high-dimensional biomedical datasets outline the challenges of cluster analysis: clustering algorithms are limited in their clustering ability in the presence of clusters defining distance-based structures resulting in a biased clustering solution. Data sets might not have cluster structures. Clustering yields arbitrary labels and often depends on the trial, leading to varying results. Moreover, recent research indicated that all partition comparison measures can yield the same results for different clustering solutions. Consequently, algorithm selection and parameter optimization by unsupervised quality measures (QM) are always biased and misleading. Only if the predefined structures happen to meet the particular clustering criterion and QM, can the clusters be recovered. Results are presented based on 41 open-source algorithms which are particularly useful in biomedical scenarios. Furthermore, comparative analysis with mirrored density plots provides a significantly more detailed benchmark than that with the typically used box plots or violin plots.",
        "The development of single-cell ribonucleic acid (RNA) sequencing (scRNA-seq) technology has led to great opportunities for the identification of heterogeneous cell types in complex tissues. Clustering algorithms are of great importance to effectively identify different cell types. In addition, the definition of the distance between each two cells is a critical step for most clustering algorithms. In this study, we found that different distance measures have considerably different effects on clustering algorithms. Moreover, there is no specific distance measure that is applicable to all datasets. In this study, we introduce a new single-cell clustering method called SD-h, which generates an applicable distance measure for different kinds of datasets by optimally synthesizing commonly used distance measures. Then, hierarchical clustering is performed based on the new distance measure for more accurate cell-type clustering. SD-h was tested on nine frequently used scRNA-seq datasets and it showed great superiority over almost all the compared leading single-cell clustering algorithms.",
        "It is a vital task to design an integrated machine learning model to discover cancer subtypes and understand the heterogeneity of cancer based on multiple omics data. In recent years, some multi-view clustering algorithms have been proposed and applied to the prediction of cancer subtypes. Among them, the multi-view clustering methods based on graph learning are widely concerned. These multi-view approaches usually have one or more of the following problems. Many multi-view algorithms use the original omics data matrix to construct the similarity matrix and ignore the learning of the similarity matrix. They separate the data clustering process from the graph learning process, resulting in a highly dependent clustering performance on the predefined graph. In the process of graph fusion, these methods simply take the average value of the affinity graph of multiple views to represent the result of the fusion graph, and the rich heterogeneous information is not fully utilized. To solve the above problems, in this paper, a Multi-view Spectral Clustering Based on Multi-smooth Representation Fusion (MRF-MSC) method was proposed. Firstly, MRF-MSC constructs a smooth representation for each data type, which can be viewed as a sample (patient) similarity matrix. The smooth representation can explicitly enhance the grouping effect. Secondly, MRF-MSC integrates the smooth representation of multiple omics data to form a similarity matrix containing all biological data information through graph fusion. In addition, MRF-MSC adaptively gives weight factors to the smooth regularization representation of each omics data by using the self-weighting method. Finally, MRF-MSC imposes constrained Laplacian rank on the fusion similarity matrix to get a better cluster structure. The above problems can be transformed into spectral clustering for solving, and the clustering results can be obtained. MRF-MSC unifies the above process of graph construction, graph fusion and spectral clustering under one framework, which can learn better data representation and high-quality graphs, so as to achieve better clustering effect. In the experiment, MRF-MSC obtained good experimental results on the TCGA cancer data sets.",
        "Tujia brocades are important carriers of Chinese Tujia national culture and art. It records the most detailed and real cultural history of Tujia nationality and is one of the National Intangible Cultural Heritage. Classic graphic elements are separated from Tujia brocade patterns to establish the Tujia brocade graphic element database, which is used for the protection and inheritance of traditional national culture. Tujia brocade dataset collected a total of more than 200 clear Tujia brocade patterns and was divided into seven categories, according to traditional meanings. The weave texture of a Tujia brocade is coarse, and the textural features of the background are obvious, so classical segmentation algorithms cannot achieve good segmentation effects. At the same time, deep learning technology cannot be used because there is no standard Tujia brocade dataset. Based on the above problems, this study proposes a method based on an unsupervised clustering algorithm for the segmentation of Tujia brocades. First, the cluster number K is calculated by fusing local binary patterns (LBP) and gray-level co-occurrence matrix (GLCM) characteristic values. Second, clustering and segmentation are conducted on each input Tujia brocade image by adopting a Gaussian mixture model (GMM) to obtain a preliminary segmentation image, wherein the image yielded after preliminary segmentation is rough. Then, a method based on voting optimization and dense conditional random field (DenseCRF) (CRF denotes conditional random filtering) is adopted to optimize the image after preliminary segmentation and obtain the image segmentation results. Finally, the desired graphic element contour is extracted through interactive cutting. The contributions of this study include: (1) a calculation method for the cluster number K wherein the experimental results show that the effect of the clustering number K chosen in this paper is ideal; (2) an optimization method for the noise points of Tujia brocade patterns based on voting, which can effectively eliminate isolated noise points from brocade patterns.",
        "Fiber clustering methods are typically used in brain research to study the organization of white matter bundles from large diffusion MRI tractography datasets. These methods enable exploratory bundle inspection using visualization and other methods that require identifying brain white matter structures in individuals or a population. Some applications, such as real-time visualization and inter-subject clustering, need fast and high-quality intra-subject clustering algorithms. This work proposes a parallel algorithm using a General Purpose Graphics Processing Unit (GPGPU) for fiber clustering based on the FFClust algorithm. The proposed GPGPU implementation exploits data parallelism using both multicore and GPU fine-grained parallelism present in commodity architectures, including current laptops and desktop computers. Our approach implements all FFClust steps in parallel, improving execution times in all of them. In addition, our parallel approach includes a parallel Kmeans++ algorithm implementation and defines a new variant of Kmeans++ to reduce the impact of choosing outliers as initial centroids. The results show that our approach provides clustering quality results very similar to FFClust, and it requires an execution time of 3.5 s for processing about a million fibers, achieving a speedup of 11.5 times compared to FFClust.",
        "In recent years, severe air pollution has frequently occurred in China at the regional scale. The clustering method to define joint control regions is an effective approach to address severe regional air pollution. However, current cluster analysis research on the determination of joint control areas relies on the Pearson correlation coefficient as a similarity measure. Due to nonlinearity and outliers in air pollution data, the correlation coefficient cannot accurately reveal the similarity in air quality between different cities. To bridge this gap, we proposed a method to delineate spatial patterns of PM2.5 pollution and regional boundaries of polluted areas using the frequent itemset clustering approach. The frequent itemsets between cities were first mined, and the support values were employed as interestingness metrics to describe the significance of similar variation patterns between cities. Then, the hierarchical clustering method was applied to identify appropriate areas for joint pollution control. The proposed clustering algorithm exhibits the advantages of not requiring model assumptions and a robustness to the outliers, which is a cost-effective approach to define joint control regions. By analysing urban PM2.5 pollution in China from 2015 to 2018, we obtained results demonstrating that the frequent itemset clustering approach can efficiently determine pollution patterns and can effectively identify regional divisions. The clustering approach could facilitate a greater understanding of PM2.5 spatiotemporal aggregation to design joint control measures among areas. The findings and methodology of this research have important implications for the formulation of clean air policies in China.",
        "Medical data analysis is an important part of intelligent medicine, and clustering analysis is a commonly used method for data analysis of Traditional Chinese Medicine (TCM); however, the classical K-Means algorithm is greatly affected by the selection of initial clustering center, which is easy to fall into the local optimal solution. To avoid this problem, an improved differential evolution clustering algorithm is proposed in this paper. The proposed algorithm selects the initial clustering center randomly, optimizes and locates the clustering center in the process of evolution iteration, and improves the mutation mode of differential evolution to enhance the overall optimization ability, so that the clustering effect can reach the global optimization as far as possible. Three University of California, Irvine (UCI), data sets are selected to compare the clustering effect of the classical K-Means algorithm, the standard DE-K-Means algorithm, the K-Means++ algorithm, and the proposed algorithm. The experimental results show that, in terms of global optimization, the proposed algorithm is obviously superior to the other three algorithms, and in terms of convergence speed, the proposed algorithm is better than DE-K-Means algorithm. Finally, the proposed algorithm is applied to analyze the drug data of Traditional Chinese Medicine in the treatment of pulmonary diseases, and the analysis results are consistent with the theory of Traditional Chinese Medicine.",
        "In recent years, a lot of excellent multi-view clustering methods have been proposed. Because most of them need to fuse all views at one time, they are infeasible as the number of views increases over time. If the present multi-view clustering methods are employed directly to re-fuse all views at each time, it is too expensive to store all historical views. In this paper, we proposed an efficient incremental multi-view spectral clustering method with sparse and connected graph learning (SCGL). In our method, only one consensus similarity matrix is stored to represent the structural information of all historical views. Once the newly collected view is available, the consensus similarity matrix is reconstructed by learning from its previous version and the current new view. To further improve the incremental multi-view clustering performance, the sparse graph learning and the connected graph learning are integrated into our model, which can not only reduce the noises, but also preserve the correct connections within clusters. Experiments on several multi-view datasets demonstrate that our method is superior to traditional methods in clustering accuracy, and is more suitable to deal with the multi-view clustering with the number of views increasing over time.",
        "In numerical computation, locating multiple roots of nonlinear equations (NESs) in a single run is a challenging work. In order to solve the problem of population grouping and parameters settings during the evolutionary, a clustering-based adaptive speciation differential evolution, referred to as CASDE, is presented to deal with NESs. CASDE offers three advantages: 1) the clustering with dynamic clustering sizes is used to set clustering sizes for different problems; 2) adaptive parameter control at the niche level is proposed to enhance the search ability and efficiency; 3) re-initialization mechanism motivates the algorithm to search new roots and saves computing resources. To evaluate the performance of CASDE, we select 30 problems with different features as test suite. Experimental results indicate that the speciation clustering with dynamic clustering sizes, niche adaptive parameter control, and re-initialization mechanism when combined together in a synergistic manner can improve the ability to find multiple roots in a single run. Additionally, our method is also compared with other state-of-the-art methods, which is capable of obtaining better results in terms of peak ratio and success rate. Finally, two practical mechanical problems are used to verify the performance of CASDE, and it also demonstrates superior results.",
        "In this work, the partitioning clustering of COVID-19 data using c-Means (cM) and Fuzy c-Means (Fc-M) algorithms is carried out. Based on the data available from January 2020 with respect to location, i.e., longitude and latitude of the globe, the confirmed daily cases, recoveries, and deaths are clustered. In the analysis, the maximum cluster size is treated as a variable and is varied from 5 to 50 in both algorithms to find out an optimum number. The performance and validity indices of the clusters formed are analyzed to assess the quality of clusters. The validity indices to understand all the COVID-19 clusters' quality are analysed based on the Zahid SC (Separation Compaction) index, Xie-Beni Index, Fukuyama-Sugeno Index, Validity function, PC (performance coefficient), and CE (entropy) indexes. The analysis results pointed out that five clusters were identified as a major centroid where the pandemic looks concentrated. Additionally, the observations revealed that mainly the pandemic is distributed easily at any global location, and there are several centroids of COVID-19, which primarily act as epicentres. However, the three main COVID-19 clusters identified are 1) cases with value <50,000, 2) cases with a value between 0.1 million to 2 million, and 3) cases above 2 million. These centroids are located in the US, Brazil, and India, where the rest of the small clusters of the pandemic look oriented. Furthermore, the Fc-M technique seems to provide a much better cluster than the c-M algorithm.",
        "scRNA-seq data analysis enables new possibilities for identification of novel cells, specific characterization of known cells and study of cell heterogeneity. The performance of most clustering methods especially developed for scRNA-seq is greatly influenced by user input. We propose a centrality-clustering method named UICPC and compare its performance with 9 state-of-the-art clustering methods on 11 real-world scRNA-seq datasets to demonstrate its effectiveness and usefulness in discovering cell groups. Our method does not require user input. However, it requires settings of threshold, which are benchmarked after performing extensive experiments. We observe that most compared approaches show poor performance due to high heterogeneity and large dataset dimensions. However, UICPC shows excellent performance in terms of NMI, Purity and ARI, respectively. UICPC is available as an R package and can be downloaded by clicking the link https://sites.google.com/view/hussinchowdhury/software.",
        "Unsupervised Domain Adaptation (UDA) aims to learn a classifier for the unlabeled target domain by leveraging knowledge from a labeled source domain with a different but related distribution. Many existing approaches typically learn a domain-invariant representation space by directly matching the marginal distributions of the two domains. However, they ignore exploring the underlying discriminative features of the target data and align the cross-domain discriminative features, which may lead to suboptimal performance. To tackle these two issues simultaneously, this paper presents a Joint Clustering and Discriminative Feature Alignment (JCDFA) approach for UDA, which is capable of naturally unifying the mining of discriminative features and the alignment of class-discriminative features into one single framework. Specifically, in order to mine the intrinsic discriminative information of the unlabeled target data, JCDFA jointly learns a shared encoding representation for two tasks: supervised classification of labeled source data, and discriminative clustering of unlabeled target data, where the classification of the source domain can guide the clustering learning of the target domain to locate the object category. We then conduct the cross-domain discriminative feature alignment by separately optimizing two new metrics: 1) an extended supervised contrastive learning, i.e., semi-supervised contrastive learning 2) an extended Maximum Mean Discrepancy (MMD), i.e., conditional MMD, explicitly minimizing the intra-class dispersion and maximizing the inter-class compactness. When these two procedures, i.e., discriminative features mining and alignment are integrated into one framework, they tend to benefit from each other to enhance the final performance from a cooperative learning perspective. Experiments are conducted on four real-world benchmarks (e.g., Office-31, ImageCLEF-DA, Office-Home and VisDA-C). All the results demonstrate that our JCDFA can obtain remarkable margins over state-of-the-art domain adaptation methods. Comprehensive ablation studies also verify the importance of each key component of our proposed algorithm and the effectiveness of combining two learning strategies into a framework.",
        "Automated detection of brain tumor location is essential for both medical and analytical uses. In this paper, we clustered brain MRI images to detect tumor location. To obtain perfect results, we presented an unsupervised robust PCA algorithm to clustered images. The proposed method clusters brain MR image pixels to four leverages. The algorithm is implemented for five brain diseases such as glioma, Huntington, meningioma, Pick, and Alzheimer's. We used ten images of each disease to validate the optimal identification rate. According to the results obtained, 2% of the data in the bad leverage part of the image were determined, which acceptably discerned the tumor. Results show that this method has the potential to detect tumor location for brain disease with high sensitivity. Moreover, results show that the method for the Glioma images has approximately better results than others. However, according to the ROC curve for all selected diseases, the present method can find lesion location.",
        "The rapid growth in the industrial sector has required the development of more productive and reliable machinery, and therefore, leads to complex systems. In this regard, the automatic detection of unknown events in machinery represents a greater challenge, since uncharacterized catastrophic faults can occur. However, the existing methods for anomaly detection present limitations when dealing with highly complex industrial systems. For that purpose, a novel fault diagnosis methodology is developed to face the anomaly detection. An unsupervised anomaly detection framework named deep-autoencoder-compact-clustering one-class support-vector machine (DAECC-OC-SVM) is presented, which aims to incorporate the advantages of automatically learnt representation by deep neural network to improved anomaly detection performance. The method combines the training of a deep-autoencoder with clustering compact model and a one-class support-vector-machine function-based outlier detection method. The addressed methodology is applied on a public rolling bearing faults experimental test bench and on multi-fault experimental test bench. The results show that the proposed methodology it is able to accurately to detect unknown defects, outperforming other state-of-the-art methods.",
        "In metal-cutting processes, the interaction between the tool and workpiece is highly nonlinear and is very sensitive to small variations in the process parameters. This causes difficulties in controlling and predicting the resulting surface finish quality of the machined surface. In this work, vibration signals along the major cutting force direction in the turning process are measured at different combinations of cutting speeds, feeds, and depths of cut using a piezoelectric accelerometer. The signals are processed to extract features in the time and frequency domains. These include statistical quantities, Fast Fourier spectral signatures, and various wavelet analysis extracts. Various feature selection methods are applied to the extracted features for dimensionality reduction, followed by applying several outlier-resistant unsupervised clustering algorithms on the reduced feature set. The objective is to ascertain if partitions created by the clustering algorithms correspond to experimentally obtained surface roughness data for specific combinations of cutting conditions. We find 75% accuracy in predicting surface finish from the Noise Clustering Fuzzy C-Means (NC-FCM) and the Density-Based Spatial Clustering Applications with Noise (DBSCAN) algorithms, and upwards of 80% accuracy in identifying outliers. In general, wrapper methods used for feature selection had better partitioning efficacy than filter methods for feature selection. These results are useful when considering real-time steel turning process monitoring systems.",
        "Temporal networks are ubiquitous in nature and society, and tracking the dynamics of networks is fundamental for investigating the mechanisms of systems. Dynamic communities in temporal networks simultaneously reflect the topology of the current snapshot (clustering accuracy) and historical ones (clustering drift). Current algorithms are criticized for their inability to characterize the dynamics of networks at the vertex level, independence of feature extraction and clustering, and high time complexity. In this study, we solve these problems by proposing a novel joint learning model for dynamic community detection in temporal networks (also known as jLMDC) via joining feature extraction and clustering. This model is formulated as a constrained optimization problem. Vertices are classified into dynamic and static groups by exploring the topological structure of temporal networks to fully exploit their dynamics at each time step. Then, jLMDC updates the features of dynamic vertices by preserving features of static ones during optimization. The advantage of jLMDC is that features are extracted under the guidance of clustering, promoting performance, and saving the running time of the algorithm. Finally, we extend jLMDC to detect the overlapping dynamic community in temporal networks. The experimental results on 11 temporal networks demonstrate that jLMDC improves accuracy up to 8.23% and saves 24.89% of running time on average compared to state-of-the-art methods.",
        "In this study, we propose a novel algorithm to encode the cluster structure by incorporating ensemble clustering (EC) into subspace clustering (SC). First, the low-rank representation (LRR) is learned from a higher order data relationship induced by ensemble K-means coding, which exploits the cluster structure in a co-association matrix of basic partitions (i.e., clustering results). Second, to provide a fast predictive coding mechanism, an encoding function parameterized by neural networks is introduced to predict the LRR derived from partitions. These two steps are jointly proceeded to seamlessly integrate partition information and original features and thus deliver better representations than the ones obtained from each single source. Moreover, an alternating optimization framework is developed to learn the LRR, train the encoding function, and fine-tune the higher order relationship. Extensive experiments on eight benchmark datasets validate the effectiveness of the proposed algorithm on several clustering tasks compared with state-of-the-art EC and SC methods.",
        "Here we propose a novel unsupervised feature selection by combining hierarchical feature clustering with singular value decomposition (SVD). The proposed algorithm first generates several feature clusters by adopting hierarchical clustering on the feature space and then applies SVD to each of these feature clusters to identify the feature that contributes most to the SVD-entropy. The proposed feature selection method selects an optimal feature subset that not only minimizes the mutual dependency among the selected features but also maximizes mutual dependency of the selected features against their nearest neighbor non-selected features. Each of the selected features also contributes the maximum SVD-entropy among all features of the same feature cluster. The experimental results demonstrate that proposed algorithm performs well against state-of-the-art methods of feature selection in terms of various evaluation criteria. The superiority of the proposed algorithm is demonstrated through analysis of Acute Myeloid Leukemia (AML) multi-omics data that consist of five datasets: gene expression, exon expression, methylation, microRNA, and pathway activity dataset (paradigm IPLs) from The Cancer Genome Atlas (TCGA). Our analysis pinpoints a candidate gene-marker, EREG for AML with an integrative omics evidence. EREG is targeted by two top ranked microRNAs, hsa-miR-1286 and hsa-miR-1976 in the datasets.",
        "Advancements in dermoscopy techniques have elucidated identifiable characteristics of melanoma which revolve around the asymmetrical constitution of melanocytic lesions consequent of unfettered proliferative growth as a malignant lesion. This study explores the applications of hierarchical density-based spatial clustering of applications with noise (HDBSCAN) in terms of the direct diagnostic implications of applying agglomerative clustering in the spectroscopic analysis of malignant melanocytic lesions and benign dermatologic spots. 100 images of benign (n = 50) and malignant moles (n = 50) were sampled from the International Skin Imaging Collaboration Archive and processed through two separate Python algorithms. The first of which deconvolutes the three-digit tupled integer identifiers of pixel color in image composition into three separate matrices corresponding to the red, green and blue color channel. Statistical characterization of integer variance was utilized to determine the optimal channel for comparative analysis between malignant and benign image groups. The second applies HDBSCAN to the matrices, identifying agglomerative clustering in the dataset. The results indicate the potential diagnostic applications of HDBSCAN analysis in fast-processing dermoscopy, as optimization of clustering parameters according to a binary search strategy produced an accuracy of 85% in the classification of malignant and benign melanocytic lesions.",
        "Machine learning (ML) techniques have been recently employed to facilitate the development of novel two-dimensional (2D) materials. Among various synthesis approaches, chemical vapor deposition (CVD) has demonstrated tremendous potential in producing high-quality 2D flakes with good controllability, enabling large-scale production at a relatively low cost. Traditionally, the quality of CVD-grown samples can be manually evaluated based on optical images which is labor-intensive and time-consuming. In this paper, we explored a data-driven unsupervised quality assessment strategy based on image clustering via integrating self-organizing map (SOM) and k-means methods for optical image analysis of CVD-grown 2D materials. The high matching rate between the clustering results and material experts' labels indicated a good accuracy of the proposed clustering algorithm. The proposed unsupervised ML methodology will provide materials scientists with an effective tool kit for efficient evaluation of CVD-grown materials' quality and has a broad applicability for various material systems.",
        "Clustering and cell type classification are a vital step of analyzing scRNA-seq data to reveal the complexity of the tissue (e.g. the number of cell types and the transcription characteristics of the respective cell type). Recently, deep learning-based single-cell clustering algorithms become popular since they integrate the dimensionality reduction with clustering. But these methods still have unstable clustering effects for the scRNA-seq datasets with high dropouts or noise. In this study, a novel single-cell RNA-seq deep embedding clustering via convolutional autoencoder embedding and soft K-means (scCAEs) is proposed by simultaneously learning the feature representation and clustering. It integrates the deep learning with convolutional autoencoder to characterize scRNA-seq data and proposes a regularized soft K-means algorithm to cluster cell populations in a learned latent space. Next, a novel constraint is introduced to the clustering objective function to iteratively optimize the clustering results, and more importantly, it is theoretically proved that this objective function optimization ensures the convergence. Moreover, it adds the reconstruction loss to the objective function combining the dimensionality reduction with clustering to find a more suitable embedding space for clustering. The proposed method is validated on a variety of datasets, in which the number of clusters in the mentioned datasets ranges from 4 to 46, and the number of cells ranges from 90 to 30 302. The experimental results show that scCAEs is superior to other state-of-the-art methods on the mentioned datasets, and it also keeps the satisfying compatibility and robustness. In addition, for single-cell datasets with the batch effects, scCAEs can ensure the cell separation while removing batch effects.",
        "Multivariate time-series (MTS) clustering is a fundamental technique in data mining with a wide range of real-world applications. To date, though some approaches have been developed, they suffer from various drawbacks, such as high computational cost or loss of information. Most existing approaches are single-view methods without considering the benefits of mutual-support multiple views. Moreover, due to its data structure, MTS data cannot be handled well by most multiview clustering methods. Toward this end, we propose a consistent and specific non-negative matrix factorization-based multiview clustering (CSMVC) method for MTS clustering. The proposed method constructs a multilayer graph to represent the original MTS data and generates multiple views with a subspace technique. The obtained multiview data are processed through a novel non-negative matrix factorization (NMF) method, which can explore the view-consistent and view-specific information simultaneously. Furthermore, an alternating optimization scheme is proposed to solve the corresponding optimization problem. We conduct extensive experiments on 13 benchmark datasets and the results demonstrate the superiority of our proposed method against other state-of-the-art algorithms under a wide range of evaluation metrics.",
        "Spectral clustering is a well-known clustering algorithm for unsupervised learning, and its improved algorithms have been successfully adapted for many real-world applications. However, traditional spectral clustering algorithms are still facing many challenges to the task of unsupervised learning for large-scale datasets because of the complexity and cost of affinity matrix construction and the eigen-decomposition of the Laplacian matrix. From this perspective, we are looking forward to finding a more efficient and effective way by adaptive neighbor assignments for affinity matrix construction to address the above limitation of spectral clustering. It tries to learn an affinity matrix from the view of global data distribution. Meanwhile, we propose a deep learning framework with fully connected layers to learn a mapping function for the purpose of replacing the traditional eigen-decomposition of the Laplacian matrix. Extensive experimental results have illustrated the competitiveness of the proposed algorithm. It is significantly superior to the existing clustering algorithms in the experiments of both toy datasets and real-world datasets.",
        "The hidden Markov model (HMM) is a broadly applied generative model for representing time-series data, and clustering HMMs attract increased interest from machine learning researchers. However, the number of clusters (K) and the number of hidden states (S) for cluster centers are still difficult to determine. In this article, we propose a novel HMM-based clustering algorithm, the variational Bayesian hierarchical EM algorithm, which clusters HMMs through their densities and priors and simultaneously learns posteriors for the novel HMM cluster centers that compactly represent the structure of each cluster. The numbers K and S are automatically determined in two ways. First, we place a prior on the pair (K,S) and approximate their posterior probabilities, from which the values with the maximum posterior are selected. Second, some clusters and states are pruned out implicitly when no data samples are assigned to them, thereby leading to automatic selection of the model complexity. Experiments on synthetic and real data demonstrate that our algorithm performs better than using model selection techniques with maximum likelihood estimation.",
        "Coronavirus Disease 2019 (COVID-19) has been considered one of the most critical diseases of the 21st century. Only early detection can aid in the prevention of personal transmission of the disease. Recent scientific research reports indicate that computed tomography (CT) images of COVID-19 patients exhibit acute infections and lung abnormalities. However, analyzing these CT scan images is very difficult because of the presence of noise and low-resolution. Therefore, this study suggests the development of a new early detection method to detect abnormalities in chest CT scan images of COVID-19 patients. By this motivation, a novel image clustering algorithm, called ambiguous D-means fusion clustering algorithm (ADMFCA), is introduced in this study. This algorithm is based on the newly proposed ambiguous set theory and associated concepts. The ambiguous set is used in the proposed technique to characterize the ambiguity associated with grayscale values of pixels as true, false, true-ambiguous and false-ambiguous. The proposed algorithm performs the clustering operation on the CT scan images based on the entropies of different grayscale values. Finally, a final outcome image is obtained from the clustered images by image fusion operation. The experiment is carried out on 40 different CT scan images of COVID-19 patients. The clustered images obtained by the proposed algorithm are compared to five well-known clustering methods. The comparative study based on statistical metrics shows that the proposed ADMFCA is more efficient than the five existing clustering methods.",
        "Clustering is a form of unsupervised learning that aims to uncover latent groups within data based on similarity across a set of features. A common application of this in biomedical research is in delineating novel cancer subtypes from patient gene expression data, given a set of informative genes. However, it is typically unknown a priori what genes may be informative in discriminating between clusters, and what the optimal number of clusters are. Few methods exist for performing unsupervised clustering of RNA-seq samples, and none currently adjust for between-sample global normalization factors, select cluster-discriminatory genes, or account for potential confounding variables during clustering. To address these issues, we propose the Feature Selection and Clustering of RNA-seq (FSCseq): a model-based clustering algorithm that utilizes a finite mixture of regression (FMR) model and the quadratic penalty method with a Smoothly-Clipped Absolute Deviation (SCAD) penalty. The maximization is done by a penalized Classification EM algorithm, allowing us to include normalization factors and confounders in our modeling framework. Given the fitted model, our framework allows for subtype prediction in new patients via posterior probabilities of cluster membership, even in the presence of batch effects. Based on simulations and real data analysis, we show the advantages of our method relative to competing approaches.",
        "In this research, pure deterministic system has been established by a new Distributed Energy Efficient Clustering Protocol with Enhanced Threshold (DEECET) by clustering sensor nodes to originate the wireless sensor network. The DEECET is very dynamic, highly distributive, self-confessed and much energy efficient as compared to most of the other existing protocols. The MATLAB simulation provides aim proved result by means of energy dissipation being emulated in the networks lifespan for homogeneous as well as heterogeneous sensor network, which when contrasted for other traditional protocols. An enhanced result has been obtained for equitable energy dissipation for systematized networks using DEECET.",
        "To efficiently lower virus infectivity and combat virus epidemics or pandemics, it is important to discover broadly acting antivirals. Here, we investigated two naturally occurring polyphenols, Epigallocatechin gallate (EGCG) and Resveratrol (RES), and polyphenol-functionalized nanoparticles for their antiviral efficacy. Concentrations in the low micromolar range permanently inhibited the infectivity of high doses of enteroviruses (10(7) PFU/mL). Sucrose gradient separation of radiolabeled viruses, dynamic light scattering, transmission electron microscopic imaging and an in-house developed real-time fluorescence assay revealed that polyphenols prevented infection mainly through clustering of the virions into very stable assemblies. Clustering and stabilization were not compromised even in dilute virus solutions or after diluting the polyphenols-clustered virions by 50-fold. In addition, the polyphenols lowered virus binding on cells. In silico docking experiments of these molecules against 2- and 3-fold symmetry axes of the capsid, using an algorithm developed for this study, discovered five binding sites for polyphenols, out of which three were novel binding sites. Our results altogether suggest that polyphenols exert their antiviral effect through binding to multiple sites on the virion surface, leading to aggregation of the virions and preventing RNA release and reducing cell surface binding.",
        "This paper presents a novel approach for anomaly detection in industrial processes. The system solely relies on unlabeled data and employs a 1D-convolutional neural network-based deep autoencoder architecture. As a core novelty, we split the autoencoder latent space in discriminative and reconstructive latent features and introduce an auxiliary loss based on k-means clustering for the discriminatory latent variables. We employ a Top-K clustering objective for separating the latent space, selecting the most discriminative features from the latent space. We use the approach to the benchmark Tennessee Eastman data set to prove its applicability. We provide different ablation studies and analyze the method concerning various downstream tasks, including anomaly detection, binary and multi-class classification. The obtained results show the potential of the approach to improve downstream tasks compared to standard autoencoder architectures.",
        "The synergy between Artificial Intelligence and the Edge Computing paradigm promises to transfer decision-making processes to the periphery of sensor networks without the involvement of central data servers. For this reason, we recently witnessed an impetuous development of devices that integrate sensors and computing resources in a single board to process data directly on the collection place. Due to the particular context where they are used, the main feature of these boards is the reduced energy consumption, even if they do not exhibit absolute computing powers comparable to modern high-end CPUs. Among the most popular Artificial Intelligence techniques, clustering algorithms are practical tools for discovering correlations or affinities within data collected in large datasets, but a parallel implementation is an essential requirement because of their high computational cost. Therefore, in the present work, we investigate how to implement clustering algorithms on parallel and low-energy devices for edge computing environments. In particular, we present the experiments related to two devices with different features: the quad-core UDOO X86 Advanced+ board and the GPU-based NVIDIA Jetson Nano board, evaluating them from the performance and the energy consumption points of view. The experiments show that they realize a more favorable trade-off between these two requirements than other high-end computing devices.",
        "The early detection of skin cancer, especially through the examination of lesions with malignant characteristics, has been reported to significantly decrease the potential fatalities. Segmentation of the regions that contain the actual lesions is one of the most widely used steps for achieving an automated diagnostic process of skin lesions. However, accurate segmentation of skin lesions has proven to be a challenging task in medical imaging because of the intrinsic factors such as the existence of undesirable artifacts and the complexity surrounding the seamless acquisition of lesion images. In this paper, we have introduced a novel algorithm based on gamma correction with clustering of keypoint descriptors for accurate segmentation of lesion areas in dermoscopy images. The algorithm was tested on dermoscopy images acquired from the publicly available dataset of Pedro Hispano hospital to achieve compelling equidistant sensitivity, specificity, and accuracy scores of 87.29%, 99.54%, and 96.02%, respectively. Moreover, the validation of the algorithm on a subset of heavily noised skin lesion images collected from the public dataset of International Skin Imaging Collaboration has yielded the equidistant sensitivity, specificity, and accuracy scores of 80.59%, 100.00%, and 94.98%, respectively. The performance results are propitious when compared to those obtained with existing modern algorithms using the same standard benchmark datasets and performance evaluation indices.",
        "Background: Glucosinolates (GSLs) are plant secondary metabolites that contain nitrogen-containing compounds. They are important in the plant defense system and known to provide protection against cancer in humans. Currently, increasing the amount of data generated from various omics technologies serves as a hotspot for new gene discovery. However, sometimes sequence similarity searching approach is not sufficiently effective to find these genes; hence, we adapted a network clustering approach to search for potential GSLs genes from the Arabidopsis thaliana co-expression dataset. Methods: We used known GSL genes to construct a comprehensive GSL co-expression network. This network was analyzed with the DPClusOST algorithm using a density of 0.5. 0.6. 0.7, 0.8, and 0.9. Generating clusters were evaluated using Fisher's exact test to identify GSL gene co-expression clusters. A significance score (SScore) was calculated for each gene based on the generated p-value of Fisher's exact test. SScore was used to perform a receiver operating characteristic (ROC) study to classify possible GSL genes using the ROCR package. ROCR was used in determining the AUC that measured the suitable density value of the cluster for further analysis. Finally, pathway enrichment analysis was conducted using ClueGO to identify significant pathways associated with the GSL clusters. Results: The density value of 0.8 showed the highest area under the curve (AUC) leading to the selection of thirteen potential GSL genes from the top six significant clusters that include IMDH3, MVP1, T19K24.17, MRSA2, SIR, ASP4, MTO1, At1g21440, HMT3, At3g47420, PS1, SAL1, and At3g14220. A total of Four potential genes (MTO1, SIR, SAL1, and IMDH3) were identified from the pathway enrichment analysis on the significant clusters. These genes are directly related to GSL-associated pathways such as sulfur metabolism and valine, leucine, and isoleucine biosynthesis. This approach demonstrates the ability of the network clustering approach in identifying potential GSL genes which cannot be found from the standard similarity search.",
        "Atomic force microscopy-single-molecule force spectroscopy (AFM-SMFS) is a powerful methodology to probe intermolecular and intramolecular interactions in biological systems because of its operability in physiological conditions, facile and rapid sample preparation, versatile molecular manipulation, and combined functionality with high-resolution imaging. Since a huge number of AFM-SMFS force-distance curves are collected to avoid human bias and errors and to save time, numerous algorithms have been developed to analyze the AFM-SMFS curves. Nevertheless, there is still a need to develop new algorithms for the analysis of AFM-SMFS data since the current algorithms cannot specify an unbinding force to a corresponding/each binding site due to the lack of networking functionality to model the relationship between the unbinding forces. To address this challenge, herein, we develop an unsupervised method, i.e., a network-based automatic clustering algorithm (NASA), to decode the details of specific molecules, e.g., the unbinding force of each binding site, given the input of AFM-SMFS curves. Using the interaction of heparan sulfate (HS)-antithrombin (AT) on different endothelial cell surfaces as a model system, we demonstrate that NASA is able to automatically detect the peak and calculate the unbinding force. More importantly, NASA successfully identifies three unbinding force clusters, which could belong to three different binding sites, for both Ext1(f/f) and Ndst1(f/f) cell lines. NASA has great potential to be applied either readily or slightly modified to other AFM-based SMFS measurements that result in \"saw-tooth\"-shaped force-distance curves showing jumps related to the force unbinding, such as antibody-antigen interaction and DNA-protein interaction.",
        "Accurate forecasting of air pollutant concentration is of great importance since it is an essential part of the early warning system. However, it still remains a challenge due to the limited information of emission source and high uncertainties of the dynamic processes. In order to improve the accuracy of air pollutant concentration forecast, this study proposes a novel hybrid model using clustering, feature selection, real-time decomposition by empirical wavelet transform, and deep learning neural network. First, all air pollutant time series are decomposed by empirical wavelet transform based on real-time decomposition, and subsets of output data are constructed by combining corresponding decomposed components. Second, each subset of output data is classified into several clusters by clustering algorithm, and then appropriate inputs are selected by feature selection method. Third, a deep learning-based predictor, which uses three dimensional convolutional neural network and bidirectional long short-term memory neural network, is applied to predict decomposition components of each cluster. Last, air pollutant concentration forecast for each monitoring station is obtained by reconstructing predicted values of all the decomposition components. PM2.5 concentration data of Beijing, China is used to validate and test our model. Results show that the proposed model outperforms other models used in this study. In our model, mean absolute percentage error for 1, 6, 10 h ahead PM2.5 concentration prediction is 4.03%, 6.87%, and 8.98%, respectively. These outcomes demonstrate that the proposed hybrid model is a powerful tool to provide highly accurate forecast for air pollutant concentration.",
        "Purpose: This study used self-reported gender among trans and gender diverse people in Australia to identify and describe broad, overarching gender categories that encompass the expansive ways in which gender can be defined and expressed. Methods: Data were collected as part of the Australian Trans and Gender Diverse Sexual Health Survey hosted in October 2018. Participant self-identification with nonexclusive gender categories were analyzed using algorithm-based hierarchical clustering; factors associated with gender clusters were identified using logistic regression analyses. Results: Usable data were collected from 1613 trans and gender diverse people in Australia, of whom 71.0% used two or more labels to describe their gender. Three nonexclusive clusters were identified: (i) women/trans women, (ii) men/trans men, and (iii) nonbinary. In total, 33.8% of participants defined their gender in exclusively binary terms (i.e., men/women, trans men/trans women), 40.1% in nonbinary terms, and 26.0% in both binary and nonbinary terms. The following factors were associated with selecting nonbinary versus binary gender labels: presumed female gender at birth (adjusted odds ratio [aOR]=2.02, 95% confidence interval [CI]=1.60-2.54, p<0.001), having a majority of sexual and/or gender minority friends (aOR=2.46, 95% CI=1.49-3.10, p<0.001), and having spent more than half of one's life identifying as trans and/or gender diverse (aOR=1.75, 95% CI=1.37-2.23, p<0.001). Conclusion: Trans and gender diverse people take up diverse and often multiple gender labels, which can be broadly categorized as binary and nonbinary. Systems of health care and research must be adapted to include nonbinary people while remaining amenable to further adaptation.",
        "Objective: Hyperglycemia has emerged as an important clinical manifestation of coronavirus disease 2019 (COVID-19) in diabetic and nondiabetic patients. Whether these glycemic changes are specific to a subgroup of patients and persist following COVID-19 resolution remains to be elucidated. This work aimed to characterize longitudinal random blood glucose in a large cohort of nondiabetic patients diagnosed with COVID-19. Materials and Methods: De-identified electronic medical records of 7502 patients diagnosed with COVID-19 without prior diagnosis of diabetes between January 1, 2020, and November 18, 2020, were accessed through the TriNetX Research Network. Glucose measurements, diagnostic codes, medication codes, laboratory values, vital signs, and demographics were extracted before, during, and after COVID-19 diagnosis. Unsupervised time-series clustering algorithms were trained to identify distinct clusters of glucose trajectories. Cluster associations were tested for demographic variables, COVID-19 severity, glucose-altering medications, glucose values, and new-onset diabetes diagnoses. Results: Time-series clustering identified a low-complexity model with 3 clusters and a high-complexity model with 19 clusters as the best-performing models. In both models, cluster membership differed significantly by death status, COVID-19 severity, and glucose levels. Clusters membership in the 19 cluster model also differed significantly by age, sex, and new-onset diabetes mellitus. Discussion and Conclusion: This work identified distinct longitudinal blood glucose changes associated with subclinical glucose dysfunction in the low-complexity model and increased new-onset diabetes incidence in the high-complexity model. Together, these findings highlight the utility of data-driven techniques to elucidate longitudinal glycemic dysfunction in patients with COVID-19 and provide clinical evidence for further evaluation of the role of COVID-19 in diabetes pathogenesis.",
        "MOTIVATION: Classical Molecular Dynamics is a standard computational approach to model time-dependent processes at the atomic level. The inherent sparsity of increasingly huge generated trajectories demands clustering algorithms to reduce other post-simulation analysis complexity. The quality threshold (QT) variant is an appealing one from the vast number of available clustering methods. It guarantees that all members of a particular cluster will maintain a collective similarity established by a user-defined threshold. Unfortunately, its high computational cost for processing big data limits its application in the molecular simulation field. RESULTS: In the present work, we propose a methodological parallel between QT clustering and another well-known algorithm in the field of Graph Theory, the Maximum Clique Problem. Molecular trajectories are represented as graphs whose nodes designate conformations, while unweighted edges indicate mutual similarity between nodes. The use of a binary-encoded RMSD matrix coupled to the exploitation of bitwise operations to extract clusters significantly contributes to reaching a very affordable algorithm compared to the few implementations of QT for Molecular Dynamics available in the literature. Our alternative provides results in good agreement with the exact one while strictly preserving the collective similarity of clusters.The source code and documentation of BitQT are free and publicly available on GitHub (https://github.com/LQCT/BitQT.git) and ReadTheDocs (https://bitqt.readthedocs.io/en/latest/) respectively. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "In recent years, the application of single cell RNA-seq (scRNA-seq) has become more and more popular in fields such as biology and medical research. Analyzing scRNA-seq data can discover complex cell populations and infer single-cell trajectories in cell development. Clustering is one of the most important methods to analyze scRNA-seq data. In this paper, we focus on improving scRNA-seq clustering through gene selection, which also reduces the dimensionality of scRNA-seq data. Studies have shown that gene selection for scRNA-seq data can improve clustering accuracy. Therefore, it is important to select genes with cell type specificity. Gene selection not only helps to reduce the dimensionality of scRNA-seq data, but also can improve cell type identification in combination with clustering methods. Here, we proposed RFCell, a supervised gene selection method, which is based on permutation and random forest classification. We first use RFCell and three existing gene selection methods to select gene sets on 10 scRNA-seq data sets. Then, three classical clustering algorithms are used to cluster the cells obtained by these gene selection methods. We found that the gene selection performance of RFCell was better than other gene selection methods.",
        "Objective: This study aimed to explore the application of different prenatal corticosteroids in the assessment of neurological deficits and prognosis in premature infants through Magnetic Reasoning Imaging (MRI) under optimized cluster algorithm. Methods: 100 pregnant women with threatened preterm labor were retrospectively analyzed, in which 38 pregnant women with lasting threatened preterm labor (group A) were treated with multiple courses of antenatal corticosteroids (dexamethasone treatment) and 62 cases of pregnant women with threatened preterm labor (group B) were treated with single course of dexamethasone treatment. Craniocerebral MRI images based on optimal clustering algorithm were used to examine neonates. Neonatal hypoxic-ischemic encephalopathy (HIE) rate, serum neuron-specific enolase (NSE) concentration, neonatal behavioral neurological score (NBNA), respiratory distress syndrome (RDS) rate, perinatal mortality, neonatal birth weight, and maternal complications rate of two groups were compared. Results: Compared with other traditional image segmentation algorithms, this algorithm had the best segmentation effect, the shortest running time (1.43 s), the least number of iterations (5 times), and the highest segmentation accuracy (97.98%). There was no significant difference in the HIE rate, serum NSE concentration, NBNA score, RDS score, and perinatal mortality in group A and group B (P > 0.05). Compared with group B, neonates' body weight in group A was decreased, while the maternal complication rate in group A was increased (P < 0.05). Conclusion: MRI images based on optimized clustering algorithm can be used in the diagnosis of neonatal hypoxic-ischemic encephalopathy. There is no significant difference in the application of different antenatal corticosteroids affecting premature nerve function defect and prognosis, but multiple courses of antenatal corticosteroids can affect neonatal body mass and increased maternal complications to a certain extent; therefore, before threatened premature delivery treatment, the pros and cons of multiple courses of antenatal corticosteroids should fully be considered and in the treatment, measures should be actively taken to alleviate the side effect.",
        "Active learning is aimed to sample the most informative data from the unlabeled pool, and diverse clustering methods have been applied to it. However, the distance-based clustering methods usually cannot perform well in high dimensions and even begin to fail. In this paper, we propose a new active learning method combined with variational autoencoder (VAE) and density-based spatial clustering of applications with noise (DBSCAN). It overcomes the difficulty of distance representation in high dimensions and prevents the distance concentration phenomenon from occurring in the computational learning literature with respect to high-dimensional p-norms. Finally, we compare our method with four common active learning methods and two other clustering algorithms combined with VAE on three datasets. The results demonstrate that our approach achieves competitive performance, and it is a new batch mode active learning algorithm designed for neural networks with a relatively small query batch size.",
        "We propose the hierarchical Projective Adaptive Resonance Theory (PART) algorithm for classification of gene expression data. This algorithm is realized by combing transposed quasi-supervised PART and unsupervised PART. We develop the corresponding validation statistics for each process and compare it with other clustering algorithms in a case study of tuberculosis (TB). First, we use sample-based transposed quasi-supervised PART to obtain optimal clustering results of samples distinguished by time post-infection and the representative genes for each cluster including up-regulated, down-regulated and stable genes. The up- and down-regulated genes show more than 90% similarity to the result derived from Linear Models for Microarray Data and are verified by weighted k-nearest neighbor model on TB projection. Second, we use gene-based unsupervised PART algorithm to cluster these representative genes where functional enrichment analysis is conducted in each cluster. We further confirm the main immune response of human macrophage-like THP-1 cells against TB within 2 days is type I interferon-mediated innate immunity. This study demonstrates how hierarchical PART algorithm analyzes microarray data. The sample-based quasi-supervised PART extracts representative genes and narrows down the shortlist of disease-relevant genes and gene-based unsupervised PART classifies representative genes that help to interpret immune response against TB.",
        "BACKGROUND AND OBJECTIVE: Pulmonary nodules have different shapes and uneven density, and some nodules adhere to blood vessels, pleura and other anatomical structures, which increase the difficulty of nodule segmentation. The purpose of this paper is to use multiscale residual U-Net to accurately segment lung nodules with complex geometric shapes, while comparing it with fuzzy C-means clustering and manual segmentation. METHOD: We selected 58 computed tomography (CT) scan images of patients with different lung nodules for image segmentation. This paper proposes an automatic segmentation algorithm for lung nodules based on multiscale residual U-Net. In order to verify the accuracy of the method, we also conducted comparative experiments, while comparing it with fuzzy C-means clustering. RESULTS: Compared with the other two methods, the segmentation of lung nodules based on multiscale residual U-Net has a higher accuracy, with an accuracy rate of 94.57%. This method not only maintains a high accuracy rate, but also shortens the recognition time significantly with a segmentation time of 3.15 s. CONCLUSIONS: The diagnosis method of lung nodules combined with deep learning has a good market prospect and can improve the efficiency of doctors in diagnosing benign and malignant lung nodules.",
        "BACKGROUND: Within the most commonly used neurofeedback training methods, a threshold has been defined for each EEG feature wherein subjects' status during training can be assessed according to the given value. In the present study, a neurofeedback training method based on feature-space clustering was proposed in order to assess subjects' status more accurately. NEW METHOD: Neural gas algorithm was employed for feature space clustering. Then, the clusters were labeled as initial clusters (where the EEG features were placed prior to training) and target (where the EEG features should be shifted towards during training) ones. A scoring index was defined whose value was determined according to subjects' brain activity. This method was simulated in two versions: soft-boundary and hard-boundary based methods. RESULTS: The results of the present simulation showed that the proposed hard-boundary based version could guide the subjects towards the boundaries of the target clusters and even their status would be stabilized in case of too many changes in subjects' EEG features. In the proposed soft-boundary based version, in case of too many changes in training features, the subjects would not be encouraged and they could be guided towards the target boundaries. CONCLUSION: The proposed hard-boundary based version could be effective in guiding a subject towards being placed within the boundaries of target clusters and even beyond them if no specific limits exited for EEG features. As well, the soft-boundary based version could be useful when controlling EEG features within a limit.",
        "Clustering with variable selection is a challenging yet critical task for modern small-n-large-p data. Existing methods based on sparse Gaussian mixture models or sparse $K$-means provide solutions to continuous data. With the prevalence of RNA-seq technology and lack of count data modeling for clustering, the current practice is to normalize count expression data into continuous measures and apply existing models with a Gaussian assumption. In this article, we develop a negative binomial mixture model with lasso or fused lasso gene regularization to cluster samples (small $n$) with high-dimensional gene features (large $p$). A modified EM algorithm and Bayesian information criterion are used for inference and determining tuning parameters. The method is compared with existing methods using extensive simulations and two real transcriptomic applications in rat brain and breast cancer studies. The result shows the superior performance of the proposed count data model in clustering accuracy, feature selection, and biological interpretation in pathways.",
        "Given the rapid increase in the incidence of cardiometabolic conditions, there is an urgent need for better approaches to prevent as many cases as possible and move from a one-size-fits-all approach to a precision cardiometabolic prevention strategy in the general population. We used data from ORISCAV-LUX 2, a nationwide, cross-sectional, population-based study. On the 1356 participants, we used a machine learning semi-supervised cluster method guided by body mass index (BMI) and glycated hemoglobin (HbA1c), and a set of 29 cardiometabolic variables, to identify subgroups of interest for cardiometabolic health. Cluster stability was assessed with the Jaccard similarity index. We have observed 4 clusters with a very high stability (ranging between 92 and 100%). Based on distinctive features that deviate from the overall population distribution, we have labeled Cluster 1 (N = 729, 53.76%) as \"Healthy\", Cluster 2 (N = 508, 37.46%) as \"Family history-Overweight-High Cholesterol \", Cluster 3 (N = 91, 6.71%) as \"Severe Obesity-Prediabetes-Inflammation\" and Cluster 4 (N = 28, 2.06%) as \"Diabetes-Hypertension-Poor CV Health\". Our work provides an in-depth characterization and thus, a better understanding of cardiometabolic health in the general population. Our data suggest that such a clustering approach could now be used to define more targeted and tailored strategies for the prevention of cardiometabolic diseases at a population level. This study provides a first step towards precision cardiometabolic prevention and should be externally validated in other contexts.",
        "Despite the scRNA-seq analytic algorithms developed, their performance for cell clustering cannot be quantified due to the unknown \"true\" clusters. Referencing the transcriptomic heterogeneity of cell clusters, a \"true\" mRNA number matrix of cell individuals was defined as ground truth. Based on the matrix and the actual data generation procedure, a simulation program (SSCRNA) for raw data was developed. Subsequently, the consistency between simulated data and real data was evaluated. Furthermore, the impact of sequencing depth and algorithms for analyses on cluster accuracy was quantified. As a result, the simulation result was highly consistent with that of the actual data. Among the clustering algorithms, the Gaussian normalization method was the more recommended. As for the clustering algorithms, the K-means clustering method was more stable than K-means plus Louvain clustering. In conclusion, the scRNA simulation algorithm developed restores the actual data generation process, discovers the impact of parameters on classification, compares the normalization/clustering algorithms, and provides novel insight into scRNA analyses.",
        "Data of the commercial parameters of Pleurotus ostreatus and Pleurotus djamor were analyzed using the data mining technique: K-means clustering algorithm. The parameters evaluated were: biological efficiency, crop yield ratio, productivity rate, nutritional composition, antioxidant and antimicrobial activities in the production of fruit bodies of 50 strains of Pleurotus ostreatus and 50 strains of Pleurotus djamor, cultivated on the most representative agricultural wastes from the province of Guayas: 80% sugarcane bagasse and 20% wheat straw (M1), and 60% wheat straw and 40% sugarcane bagasse (M2). The database of the parameters obtained in experimental procedures was grouped into three clusters, providing a visualization of the strains with a higher relation to each parameter (vector) measured.",
        "Detecting outliers is a widely studied problem in many disciplines, including statistics, data mining, and machine learning. All anomaly detection activities are aimed at identifying cases of unusual behavior compared to most observations. There are many methods to deal with this issue, which are applicable depending on the size of the data set, the way it is stored, and the type of attributes and their values. Most of them focus on traditional datasets with a large number of quantitative attributes. The multitude of solutions related to detecting outliers in quantitative sets, a large and still has a small number of research solutions is a problem detecting outliers in data containing only qualitative variables. This article was designed to compare three different categorical data clustering algorithms: K-modes algorithm taken from MacQueen's K-means algorithm and the STIRR and ROCK algorithms. The comparison concerned the method of dividing the set into clusters and, in particular, the outliers detected by algorithms. During the research, the authors analyzed the clusters detected by the indicated algorithms, using several datasets that differ in terms of the number of objects and variables. They have conducted experiments on the parameters of the algorithms. The presented study made it possible to check whether the algorithms similarly detect outliers in the data and how much they depend on individual parameters and parameters of the set, such as the number of variables, tuples, and categories of a qualitative variable.",
        "A number of literature reports have shown that multi-view clustering can acquire a better performance on complete multi-view data. However, real-world data usually suffers from missing some samples in each view and has a small number of labeled samples. Additionally, almost all existing multi-view clustering models do not execute incomplete multi-view data well and fail to fully utilize the labeled samples to reduce computational complexity, which precludes them from practical application. In view of these problems, this paper proposes a novel framework called Semi-supervised Multi-View Clustering with Weighted Anchor Graph Embedding (SMVC_WAGE), which is conceptually simple and efficiently generates high-quality clustering results in practice. Specifically, we introduce a simple and effective anchor strategy. Based on selected anchor points, we can exploit the intrinsic and extrinsic view information to bridge all samples and capture more reliable nonlinear relations, which greatly enhances efficiency and improves stableness. Meanwhile, we construct the global fused graph compatibly across multiple views via a parameter-free graph fusion mechanism which directly coalesces the view-wise graphs. To this end, the proposed method can not only deal with complete multi-view clustering well but also be easily extended to incomplete multi-view cases. Experimental results clearly show that our algorithm surpasses some state-of-the-art competitors in clustering ability and time cost.",
        "This paper aimed to discuss the denoising ability of magnetic resonance imaging (MRI) images based on fuzzy C-means clustering (FCM) algorithm and the influence of Butylphthalide combined with Edaravone treatment on nerve function and vascular endothelial function in patients with acute cerebral infarction (ACI). Based on FCM algorithm, Markov Random Field (MRF) model algorithm was introduced to obtain a novel algorithm (NFCM), which was compared with FCM and MRF algorithm in terms of misclassification rate (MCR) and difference of Kappa index (KI). 90 patients with ACI diagnosed in hospital from December 2018 to December 2019 were selected as subjects, who were divided into combined treatment group (conventional treatment + Edaravone + Butylphthalide) and Edaravone group (conventional treatment + Edaravone) randomly, each consisting of 45 cases. The National Institutes of Health Stroke Scale (NIHSS) score and endothelial function index level such as plasma nitric oxide (NO), human endothelin-1 (ET-1), and vascular endothelial cell growth factor (VEGF) were compared before and after treatment between the two groups. The results showed that the MCR of NFCM was evidently inferior to FCM and MRF, and the KI was notably higher relative to the other two algorithms. After treatment, the NIHSS score of the combined treatment group was (9.09 +/- 1.86) points and that of Edaravone group was (14.97 +/- 3.44) points, with evident difference between the two groups (P < 0.05). After treatment, the NO of the combined treatment was (54.63 +/- 4.85), and that of Edaravone group was (41.54 +/- 5.27), which was considerably different (P < 0.01), and the VEGF and ET-1 of combined treatment group were greatly inferior to Edaravone group (P < 0.01). It was revealed that the novel algorithm based on FCM can obtain more favorable quality and segmentation accuracy of MRI images. Moreover, Butylphthalide combined with Edaravone treatment can effectively improve nerve function, vascular endothelial function, and short-term prognosis in ACI, which was safe and worthy of clinical adoption.",
        "Since water supply association analysis plays an important role in attribution analysis of water supply fluctuation, how to carry out effective association analysis has become a critical problem. However, the current techniques and methods used for association analysis are not very effective because they are based on continuous data. In general, there is different degrees of monotone relationship between continuous data, which makes the analysis results easily affected by monotone relationship. The multicollinearity between continuous data distorts these analytical methods and may generate incorrect results. Meanwhile, we cannot know the association rules and value interval between features and water supply. Therefore, the lack of an effective analysis method hinders the water supply association analysis. Association rules and value interval of features obtained from association analysis are helpful to grasp cause of water supply fluctuation and know the fluctuation interval of water supply, so as to provide better support for water supply dispatching. But the association rules and value interval between features and water supply are not fully understood. In this study, a data mining method coupling kmeans clustering discretization and apriori algorithm was proposed. The kmeans was used for data discretization to obtain the one-hot encoding that can be recognized by apriori, and the discretization can also avoid the influence of monotone relationship and multicollinearity on analysis results. All the rules eventually need to be validated in order to filter out spurious rules. The results show that the method in this study is an effective association analysis method. The method can not only obtain the valid strong association rules between features and water supply, but also understand whether the association relationship between features and water supply is direct or indirect. Meanwhile, the method can also obtain value interval of features, the association degree between features and confidence probability of rules.",
        "Graph clustering, a fundamental technique in network science for understanding structures in complex systems, presents inherent problems. Though studied extensively in the literature, graph clustering in large systems remains particularly challenging because massive graphs incur a prohibitively large computational load. The heat kernel PageRank provides a quantitative ranking of nodes, and a local cluster can be efficiently found by performing a sweep over the heat kernel PageRank vector. But computing an exact heat kernel PageRank vector may be expensive, and approximate algorithms are often used instead. Most approximate algorithms compute the heat kernel PageRank vector on the whole graph, and thus are dependent on global structures. In this paper, we present an algorithm for approximating the heat kernel PageRank on a local subgraph. Moreover, we show that the number of computations required by the proposed algorithm is sublinear in terms of the expected size of the local cluster of interest, and that it provides a good approximation of the heat kernel PageRank, with approximation errors bounded by a probabilistic guarantee. Numerical experiments verify that the local clustering algorithm using our approximate heat kernel PageRank achieves state-of-the-art performance.",
        "Similarity in T-cell receptor (TCR) sequences implies shared antigen specificity between receptors, and could be used to discover novel therapeutic targets. However, existing methods that cluster T-cell receptor sequences by similarity are computationally inefficient, making them impractical to use on the ever-expanding datasets of the immune repertoire. Here, we developed GIANA (Geometric Isometry-based TCR AligNment Algorithm) a computationally efficient tool for this task that provides the same level of clustering specificity as TCRdist at 600 times its speed, and without sacrificing accuracy. GIANA also allows the rapid query of large reference cohorts within minutes. Using GIANA to cluster large-scale TCR datasets provides candidate disease-specific receptors, and provides a new solution to repertoire classification. Querying unseen TCR-seq samples against an existing reference differentiates samples from patients across various cohorts associated with cancer, infectious and autoimmune disease. Our results demonstrate how GIANA could be used as the basis for a TCR-based non-invasive multi-disease diagnostic platform.",
        "Implicit in the k-means algorithm is a way to assign a value, or utility, to a cluster of points. It works by taking the centroid of the points and the value of the cluster is the sum of distances from the centroid to each point in the cluster. The aim in this paper is to introduce an alternative way to assign a value to a cluster. Motivation is provided. Moreover, whereas the k-means algorithm does not have a natural way to determine k if it is unknown, we can use our method of evaluating a cluster to find good clusters in a sequential manner. The idea uses optimizations over permutations and clusters are set by the cyclic groups; generated by the Hungarian algorithm.",
        "Despite widely and regularly used therapy asthma in children is not fully controlled. Recognizing the complexity of asthma phenotypes and endotypes imposed the concept of precision medicine in asthma treatment. By applying machine learning algorithms assessed with respect to their accuracy in predicting treatment outcome, we have successfully identified 4 distinct clusters in a pediatric asthma cohort with specific treatment outcome patterns according to changes in lung function (FEV1 and MEF50), airway inflammation (FENO) and disease control likely affected by discrete phenotypes at initial disease presentation, differing in the type and level of inflammation, age of onset, comorbidities, certain genetic and other physiologic traits. The smallest and the largest of the 4 clusters- 1 (N = 58) and 3 (N = 138) had better treatment outcomes compared to clusters 2 and 4 and were characterized by more prominent atopic markers and a predominant allelic (A allele) effect for rs37973 in the GLCCI1 gene previously associated with positive treatment outcomes in asthmatics. These patients also had a relatively later onset of disease (6 + yrs). Clusters 2 (N = 87) and 4 (N = 64) had poorer treatment success, but varied in the type of inflammation (predominantly neutrophilic for cluster 4 and likely mixed-type for cluster 2), comorbidities (obesity for cluster 2), level of systemic inflammation (highest hsCRP for cluster 2) and platelet count (lowest for cluster 4). The results of this study emphasize the issues in asthma management due to the overgeneralized approach to the disease, not taking into account specific disease phenotypes.",
        "The essential problem of multi-view spectral clustering is to learn a good common representation by effectively utilizing multi-view information. A popular strategy for improving the quality of the common representation is utilizing global and local information jointly. Most existing methods capture local manifold information by graph regularization. However, once local graphs are constructed, they do not change during the whole optimization process. This may lead to a degenerated common representation in the case of existing unreliable graphs. To address this problem, rather than directly using fixed local representations, we propose a dynamic strategy to construct a common local representation. Then, we impose a fusion term to maximize the common structure of the local and global representations so that they can boost each other in a mutually reinforcing manner. With this fusion term, we integrate local and global representation learning in a unified framework and design an alternative iteration based optimization procedure to solve it. Extensive experiments conducted on a number of benchmark datasets support the superiority of our algorithm over several state-of-the-art methods.",
        "We present a novel algorithm to compute the distance between synthetic routes based on tree edit distances. Such distances can be used to cluster synthesis routes generated using a retrosynthesis prediction tool. We show that the clustering of selected routes from a retrosynthesis analysis is performed in less than 10 s on average and only constitutes seven percent of the total time (prediction + clustering). Furthermore, we are able to show that representative routes from each cluster can be used to reduce the set of predicted routes. Finally, we show with a number of examples that the algorithm gives intuitive clusters that can be easily rationalized and that the routes in a cluster tend to use similar chemistry. The algorithm is included in the latest version of open-source AiZynthFinder software (https://github.com/MolecularAI/aizynthfinder) and as a separate package (https://github.com/MolecularAI/route-distances).",
        "The current unsupervised domain adaptation person re-identification (re-ID) method aims to solve the domain shift problem and applies prior knowledge learned from labelled data in the source domain to unlabelled data in the target domain for person re-ID. At present, the unsupervised domain adaptation person re-ID method based on pseudolabels has obtained state-of-the-art performance. This method obtains pseudolabels via a clustering algorithm and uses these pseudolabels to optimize a CNN model. Although it achieves optimal performance, the model cannot be further optimized due to the existence of noisy labels in the clustering process. In this paper, we propose a stable median centre clustering (SMCC) for the unsupervised domain adaptation person re-ID method. SMCC adaptively mines credible samples for optimization purposes and reduces the impact of label noise and outliers on training to improve the performance of the resulting model. In particular, we use the intracluster distance confidence measure of the sample and its K-reciprocal nearest neighbour cluster proportion in the clustering process to select credible samples and assign different weights according to the intracluster sample distance confidence of samples to measure the distances between different clusters, thereby making the clustering results more robust. The experiments show that our SMCC method can select credible and stable samples for training and improve performance of the unsupervised domain adaptation model. Our code is available at https://github.com/sunburst792/SMCC-method/tree/master.",
        "The World Health Organization (WHO) has declared Coronavirus Disease 2019 (COVID-19) as one of the highly contagious diseases and considered this epidemic as a global health emergency. Therefore, medical professionals urgently need an early diagnosis method for this new type of disease as soon as possible. In this research work, a new early screening method for the investigation of COVID-19 pneumonia using chest CT scan images has been introduced. For this purpose, a new image segmentation method based on K-means clustering algorithm (KMC) and novel fast forward quantum optimization algorithm (FFQOA) is proposed. The proposed method, called FFQOAK (FFQOA+KMC), initiates by clustering gray level values with the KMC algorithm and generating an optimal segmented image with the FFQOA. The main objective of the proposed FFQOAK is to segment the chest CT scan images so that infected regions can be accurately detected. The proposed method is verified and validated with different chest CT scan images of COVID-19 patients. The segmented images obtained using FFQOAK method are compared with various benchmark image segmentation methods. The proposed method achieves mean squared error, peak signal-to-noise ratio, Jaccard similarity coefficient and correlation coefficient of 712.30, 19.61, 0.90 and 0.91 in case of four experimental sets, namely Experimental_Set_1, Experimental_Set_2, Experimental_Set_3 and Experimental_Set_4, respectively. These four performance evaluation metrics show the effectiveness of FFQOAK method over these existing methods.",
        "The transcriptomic diversity of cell types in the human body can be analysed in unprecedented detail using single cell (SC) technologies. Unsupervised clustering of SC transcriptomes, which is the default technique for defining cell types, is prone to group cells by technical, rather than biological, variation. Compared to de-novo (unsupervised) clustering, we demonstrate using multiple benchmarks that supervised clustering, which uses reference transcriptomes as a guide, is robust to batch effects and data quality artifacts. Here, we present RCA2, the first algorithm to combine reference projection (batch effect robustness) with graph-based clustering (scalability). In addition, RCA2 provides a user-friendly framework incorporating multiple commonly used downstream analysis modules. RCA2 also provides new reference panels for human and mouse and supports generation of custom panels. Furthermore, RCA2 facilitates cell type-specific QC, which is essential for accurate clustering of data from heterogeneous tissues. We demonstrate the advantages of RCA2 on SC data from human bone marrow, healthy PBMCs and PBMCs from COVID-19 patients. Scalable supervised clustering methods such as RCA2 will facilitate unified analysis of cohort-scale SC datasets.",
        "COVID-19 has widely spread around the world, impacting the health systems of several countries in addition to the collateral damage that societies will face in the next years. Although the comparison between countries is essential for controlling this disease, the main challenge is the fact of countries are not simultaneously affected by the virus. Therefore, from the COVID-19 dataset by the Johns Hopkins University Center for Systems Science and Engineering, we present a temporal analysis on the number of new cases and deaths among countries using artificial intelligence. Our approach incrementally models the cases using a hierarchical clustering that emphasizes country transitions between infection groups over time. Then, one can compare the current situation of a country against others that have already faced previous waves. By using our approach, we designed a transition index to estimate the most probable countries' movements between infectious groups to predict next wave trends. We draw two important conclusions: (1) we show the historical infection path taken by specific countries and emphasize changing points that occur when countries move between clusters with small, medium, or large number of cases; (2) we estimate new waves for specific countries using the transition index.",
        "Detecting clusters over attributed graphs is a fundamental task in the graph analysis field. The goal is to partition nodes into dense clusters based on both their attributes and structures. Modern graph neural networks provide facilitation to jointly capture the above information in attributed graphs with a feature aggregation manner, and have achieved great success in attributed graph clustering. However, existing methods mainly focus on capturing the proximity information in graphs and often fail to learn cluster-friendly features during the training of models. Besides, similar to many deep clustering frameworks, current methods based on graph neural networks require a preassigned cluster number before estimating the clusters. To address these limitations, we propose in this paper a deep attributed clustering method based on self-separated graph neural networks and parameter-free cluster estimation. First, to learn cluster-friendly features, we jointly optimize a jumping graph convolutional auto-encoder with a self-separation regularizer, which learns clusters with changing sizes while keeping dense intra-cluster structures and sparse inter structures. Second, an additional softmax auto-encoder is trained to determine the natural cluster number from the data. The hidden units capture cluster structures and can be used to estimate the number of clusters. Extensive experiments show the effectiveness of the proposed model.",
        "OBJECTIVE: Lupus is a chronic complex autoimmune disease. Non-adherence to treatment can affect patient outcomes. Considering patients' preferences into medical decisions may increase acceptance to their medication. The PREFERLUP study used unsupervised clustering analysis to identify profiles of patients with similar treatment preferences in an online community of French lupus patients. METHODS: An online survey was conducted in adult lupus patients from the Carenity community between August 2018 and April 2019. Multiple Correspondence Analysis (MCA) was used with three unsupervised clustering methods (hierarchical, kmeans and partitioning around medoids). Several indicators (measure of connectivity, Dunn index and Silhouette width) were used to select the best clustering algorithm and choose the number of clusters. RESULTS: The 268 participants were mostly female (96%), with a mean age of 44.3 years 83% fulfilled the American College of Rheumatology (ACR) self-reported diagnostic criteria for systemic lupus erythematosus. Overall, the preferred route of administration was oral (62%) and the most important feature of an ideal drug was a low risk of side-effects (32%). Hierarchical clustering identified three clusters. Cluster 1 (59%) comprised patients with few comorbidities and a poor ability to identify oncoming flares; 84% of these patients desired oral treatments with limited side-effects. Cluster 2 (13%) comprised younger patients, who had already participated in a clinical trial, were willing to use implants and valued the compatibility of treatments with pregnancy. Cluster 3 (28%) comprised patients with a longer lupus duration, poorer control of the disease and more comorbidities; these patients mainly valued implants and injections and expected a reduction of corticosteroid intake. CONCLUSIONS: Different profiles of lupus patients were identified according to their drug preferences. These clusters could help physicians tailor their therapeutic proposals to take into account individual patient preferences, which could have a positive impact on treatment acceptance and then adherence. The study highlights the value of data acquired directly from patient communities.",
        "Fast-developing single-cell technologies create unprecedented opportunities to reveal cell heterogeneity and diversity. Accurate classification of single cells is a critical prerequisite for recovering the mechanisms of heterogeneity. However, the scRNA-seq profiles we obtained at present have high dimensionality, sparsity, and noise, which pose challenges for existing clustering methods in grouping cells that belong to the same subpopulation based on transcriptomic profiles. Although many computational methods have been proposed developing novel and effective computational methods to accurately identify cell types remains a considerable challenge. We present a new computational framework to identify cell types by integrating low-rank representation (LRR) and nonnegative matrix factorization (NMF); this framework is named NMFLRR. The LRR captures the global properties of original data by using nuclear norms, and a locality constrained graph regularization term is introduced to characterize the data's local geometric information. The similarity matrix and low-dimensional features of data can be simultaneously obtained by applying the alternating direction method of multipliers (ADMM) algorithm to handle each variable alternatively in an iterative way. We finally obtained the predicted cell types by using a spectral algorithm based on the optimized similarity matrix. Nine real scRNA-seq datasets were used to test the performance of NMFLRR and fifteen other competitive methods, and the accuracy and robustness of the simulation results suggest the NMFLRR is a promising algorithm for the classification of single cells. The simulation code is freely available at: https://github.com/wzhangwhu/NMFLRR_code.",
        "Cluster analysis is an important technique in data analysis. However, there is no encompassing theory on scatterplots to evaluate clustering. Human visual perception is regarded as a gold standard to evaluate clustering. The cluster analysis based on human visual perception requires the participation of many probands, to obtain diverse data, and hence is a challenge to do. We contribute an empirical and data-driven study on human perception for visual clustering of large scatterplot data. First, we systematically construct and label a large, publicly available scatterplot dataset. Second, we carry out a qualitative analysis based on the dataset and summarize the influence of visual factors on clustering perception. Third, we use the labeled datasets to train a deep neural network for modeling human visual clustering perception. Our experiments show that the data-driven model successfully models the human visual perception, and outperforms conventional clustering algorithms in synthetic and real datasets.",
        "BACKGROUND: The corpus callosum in the midsagittal plane plays a crucial role in the early diagnosis of diseases. When the anisotropy of the diffusion tensor in the midsagittal plane is calculated, the anisotropy of corpus callosum is close to that of the fornix, which leads to blurred boundary of the segmentation region. OBJECTIVE: To apply a fuzzy clustering algorithm combined with new spatial information to achieve accurate segmentation of the corpus callosum in the midsagittal plane in diffusion tensor images. METHODS: In this algorithm, a fixed region of interest is selected from the midsagittal plane, and the anisotropic filtering algorithm based on tensor is implemented by replacing the gradient direction of the structural tensor with an eigenvector, thus filtering the diffusion tensor of region of interest. Then, the iterative clustering center based on K-means clustering is used as the initial clustering center of tensor fuzzy clustering algorithm. Taking filtered diffusion tensor as input data and different metrics as similarity measures, the neighborhood diffusion tensor voxel calculation method of Log Euclidean framework is introduced in the membership function calculation, and tensor fuzzy clustering algorithm is proposed. In this study, MGH35 data from the Human Connectome Project (HCP) are tested and the variance, accuracy and specificity of the experimental results are discussed. RESULTS: Segmentation results of three groups of subjects in MGH35 data are reported. The average segmentation accuracy is 97.34%, and the average specificity is 98.43%. CONCLUSIONS: When segmenting the corpus callosum of diffusion tensor imaging, our method cannot only effective denoise images, but also achieve high accuracy and specificity.",
        "Wireless Sensor Networks (WSNs) continue to face two major challenges: energy and security. As a consequence, one of the WSN-related security tasks is to protect them from Denial of Service (DoS) and Distributed DoS (DDoS) attacks. Machine learning-based systems are the only viable option for these types of attacks, as traditional packet deep scan systems depend on open field inspection in transport layer security packets and the open field encryption trend. Moreover, network data traffic will become more complex due to increases in the amount of data transmitted between WSN nodes as a result of increasing usage in the future. Therefore, there is a need to use feature selection techniques with machine learning in order to determine which data in the DoS detection process are most important. This paper examined techniques for improving DoS anomalies detection along with power reservation in WSNs to balance them. A new clustering technique was introduced, called the CH_Rotations algorithm, to improve anomaly detection efficiency over a WSN's lifetime. Furthermore, the use of feature selection techniques with machine learning algorithms in examining WSN node traffic and the effect of these techniques on the lifetime of WSNs was evaluated. The evaluation results showed that the Water Cycle (WC) feature selection displayed the best average performance accuracy of 2%, 5%, 3%, and 3% greater than Particle Swarm Optimization (PSO), Simulated Annealing (SA), Harmony Search (HS), and Genetic Algorithm (GA), respectively. Moreover, the WC with Decision Tree (DT) classifier showed 100% accuracy with only one feature. In addition, the CH_Rotations algorithm improved network lifetime by 30% compared to the standard LEACH protocol. Network lifetime using the WC + DT technique was reduced by 5% compared to other WC + DT-free scenarios.",
        "In 2016, the British government acknowledged the importance of reducing antimicrobial prescriptions to avoid the long-term harmful effects of overprescription. Prescription needs are highly dependent on the factors that have a spatiotemporal component, such as bacterial outbreaks and urban densities. In this context, density-based clustering algorithms are flexible tools to analyze data by searching for group structures and therefore identifying peer groups of GPs with similar behavior. The case of Scotland presents an additional challenge due to the diversity of population densities under the area of study. We propose here a spatiotemporal clustering approach for modeling the behavior of antimicrobial prescriptions in Scotland. Particularly, we consider the density-based spatial clustering of applications with noise algorithm (DBSCAN) due to its ability to include both spatial and temporal data. We extend this approach into two directions. For the temporal analysis, we use dynamic time warping to measure the dissimilarity between time series while taking into account effects such as seasonality. For the spatial component, we propose a new way of weighting spatial distances with continuous weights derived from a Kernel density estimation-based process. This makes our approach suitable for cases with different local densities, which presents a well-known challenge for the original DBSCAN. We apply our approach to antibiotic prescription data in Scotland, demonstrating how the findings can be used to compare antimicrobial prescription behavior within a group of similar peers and detect regions of extreme behaviors.",
        "Identifying relationships between genetic variations and their clinical presentations has been challenged by the heterogeneous causes of a disease. It is imperative to unveil the relationship between the high-dimensional genetic manifestations and the clinical presentations, while taking into account the possible heterogeneity of the study subjects.We proposed a novel supervised clustering algorithm using penalized mixture regression model, called component-wise sparse mixture regression (CSMR), to deal with the challenges in studying the heterogeneous relationships between high-dimensional genetic features and a phenotype. The algorithm was adapted from the classification expectation maximization algorithm, which offers a novel supervised solution to the clustering problem, with substantial improvement on both the computational efficiency and biological interpretability. Experimental evaluation on simulated benchmark datasets demonstrated that the CSMR can accurately identify the subspaces on which subset of features are explanatory to the response variables, and it outperformed the baseline methods. Application of CSMR on a drug sensitivity dataset again demonstrated the superior performance of CSMR over the others, where CSMR is powerful in recapitulating the distinct subgroups hidden in the pool of cell lines with regards to their coping mechanisms to different drugs. CSMR represents a big data analysis tool with the potential to resolve the complexity of translating the clinical representations of the disease to the real causes underpinning it. We believe that it will bring new understanding to the molecular basis of a disease and could be of special relevance in the growing field of personalized medicine.",
        "MOTIVATION: Scalable clustering algorithms are needed to analyse millions of cells in single cell RNA-seq (scRNA-seq) data. RESULTS: Here we present an open source python package called FlowGrid that can integrate into the Scanpy workflow to perform clustering on very large scRNA-seq data sets. FlowGrid implements a fast density-based clustering algorithm originally designed for flow cytometry data analysis. We introduce a new automated parameter tuning procedure, and show that FlowGrid can achieve comparable clustering accuracy as state-of-the-art clustering algorithms but at a substantially reduced run time for very large single cell RNA-seq data sets. For example, FlowGrid can complete a 1-hour clustering task for one million cells in about 5 minutes. AVAILABILITY: https://github.com/holab-hku/FlowGrid. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Gas-path anomalies account for more than 90% of all civil aero-engine anomalies. It is essential to develop accurate gas-path anomaly detection methods. Therefore, a weakly supervised gas-path anomaly detection method for civil aero-engines based on mapping relationship mining of gas-path parameters and improved density peak clustering is proposed. First, the encoder-decoder, composed of an attention mechanism and a long short-term memory neural network, is used to construct the mapping relationship mining model among gas-path parameters. The predicted values of gas-path parameters under the restriction of mapping relationships are obtained. The deviation degree from the original values to the predicted values is regarded as the feature. To force the extracted features to better reflect the anomalies and make full use of weakly supervised labels, a weakly supervised cross-entropy loss function under extreme class imbalance is deployed. This loss function can be combined with a simple classifier to significantly improve the feature extraction results, in which anomaly samples are more different from normal samples and do not reduce the mining precision. Finally, an anomaly detection method is deployed based on improved density peak clustering and a weakly supervised clustering parameter adjustment strategy. In the improved density peak clustering method, the local density is enhanced by K-nearest neighbors, and the clustering effect is improved by a new outlier threshold determination method and a new outlier treatment method. Through these settings, the accuracy of dividing outliers and clustering can be improved, and the influence of outliers on the clustering process reduced. By introducing weakly supervised label information and automatically iterating according to clustering and anomaly detection results to update the hyperparameter settings, a weakly supervised anomaly detection method without complex parameter adjustment processes can be implemented. The experimental results demonstrate the superiority of the proposed method.",
        "The coronavirus has a high basic reproduction number ( R 0 ) and has caused the global COVID-19 pandemic. Governments are implementing lockdowns that are leading to economic fallout in many countries. Policy makers can take better decisions if provided with the indicators connected with the disease spread. This study is aimed to cluster the countries using social, economic, health and environmental related metrics affecting the disease spread so as to implement the policies to control the widespread of disease. Thus, countries with similar factors can take proactive steps to fight against the pandemic. The data is acquired for 79 countries and 18 different feature variables (the factors that are associated with COVID-19 spread) are selected. Pearson Product Moment Correlation Analysis is performed between all the feature variables with cumulative death cases and cumulative confirmed cases individually to get an insight of relation of these factors with the spread of COVID-19. Unsupervised k-means algorithm is used and the feature set includes economic, environmental indicators and disease prevalence along with COVID-19 variables. The learning model is able to group the countries into 4 clusters on the basis of relation with all 18 feature variables. We also present an analysis of correlation between the selected feature variables, and COVID-19 confirmed cases and deaths. Prevalence of underlying diseases shows strong correlation with COVID-19 whereas environmental health indicators are weakly correlated with COVID-19.",
        "With the rapid development of bioinformatics, researchers have applied community detection algorithms to detect functional modules in protein-protein interaction (PPI) networks that can predict the function of unknown proteins at the molecular level and further reveal the regularity of cell activity. Clusters in a PPI network may overlap where a protein is involved in multiple functional modules. To identify overlapping structures in protein functional modules, this paper proposes a novel overlapping community detection algorithm based on the neighboring local clustering coefficient (NLC). The contributions of the NLC algorithm are threefold: (i) Combine the edge-based community detection method with local expansion in seed selection and the local clustering coefficient of neighboring nodes to improve the accuracy of seed selection; (ii) A method of measuring the distance between edges is improved to make the result of community division more accurate; (iii) A community optimization strategy for the excessive overlapping nodes makes the overlapping structure more reasonable. The experimental results on standard networks, Lancichinetti-Fortunato-Radicchi (LFR) benchmark networks and PPI networks show that the NLC algorithm can improve the Extended modularity (EQ) value and Normalized Mutual Information (NMI) value of the community division, which verifies that the algorithm can not only detect reasonable communities but also identify overlapping structures in networks.",
        "Single-cell RNA sequencing (scRNA-seq) data has been widely used to profile cellular heterogeneities with a high-resolution picture. Clustering analysis is a crucial step of scRNA-seq data analysis because it provides a chance to identify and uncover undiscovered cell types. Most methods for clustering scRNA-seq data use an unsupervised learning strategy. Since the clustering step is separated from the cell annotation and labeling step, it is not uncommon for a totally exotic clustering with poor biological interpretability to be generated-a result generally undesired by biologists. To solve this problem, we proposed an active learning (AL) framework for clustering scRNA-seq data. The AL model employed a learning algorithm that can actively query biologists for labels, and this manual labeling is expected to be applied to only a subset of cells. To develop an optimal active learning approach, we explored several key parameters of the AL model in the experiments with four real scRNA-seq datasets. We demonstrate that the proposed AL model outperformed state-of-the-art unsupervised clustering methods with less than 1000 labeled cells. Therefore, we conclude that AL model is a promising tool for clustering scRNA-seq data that allows us to achieve a superior performance effectively and efficiently.",
        "With the widely application of cluster analysis, the number of clusters is gradually increasing, as is the difficulty in selecting the judgment indicators of cluster numbers. Also, small clusters are crucial to discovering the extreme characteristics of data samples, but current clustering algorithms focus mainly on analyzing large clusters. In this paper, a bidirectional clustering algorithm based on local density (BCALoD) is proposed. BCALoD establishes the connection between data points based on local density, can automatically determine the number of clusters, is more sensitive to small clusters, and can reduce the adjusted parameters to a minimum. On the basis of the robustness of cluster number to noise, a denoising method suitable for BCALoD is proposed. Different cutoff distance and cutoff density are assigned to each data cluster, which results in improved clustering performance. Clustering ability of BCALoD is verified by randomly generated datasets and city light satellite images.",
        "Multiple kernel clustering (MKC) has recently achieved remarkable progress in fusing multisource information to boost the clustering performance. However, the O( n(2)) memory consumption and O( n(3) ) computational complexity prohibit these methods from being applied into median- or large-scale applications, where n denotes the number of samples. To address these issues, we carefully redesign the formulation of subspace segmentation-based MKC, which reduces the memory and computational complexity to O( n) and O( n(2) ), respectively. The proposed algorithm adopts a novel sampling strategy to enhance the performance and accelerate the speed of MKC. Specifically, we first mathematically model the sampling process and then learn it simultaneously during the procedure of information fusion. By this way, the generated anchor point set can better serve data reconstruction across different views, leading to improved discriminative capability of the reconstruction matrix and boosted clustering performance. Although the integrated sampling process makes the proposed algorithm less efficient than the linear complexity algorithms, the elaborate formulation makes our algorithm straightforward for parallelization. Through the acceleration of GPU and multicore techniques, our algorithm achieves superior performance against the compared state-of-the-art methods on six datasets with comparable time cost to the linear complexity algorithms.",
        "Protein complexes are of great significance to provide valuable insights into the mechanisms of biological processes of proteins. A variety of computational algorithms have thus been proposed to identify protein complexes in a protein-protein interaction network. However, few of them can perform their tasks by taking into account both network topology and protein attribute information in a unified fuzzy-based clustering framework. Since proteins in the same complex are similar in terms of their attribute information and the consideration of fuzzy clustering can also make it possible for us to identify overlapping complexes, we target to propose such a novel fuzzy-based clustering framework, namely FCAN-PCI, for an improved identification accuracy. To do so, the semantic similarity between the attribute information of proteins is calculated and we then integrate it into a well-established fuzzy clustering model together with the network topology. After that, a momentum method is adopted to accelerate the clustering procedure. FCAN-PCI finally applies a heuristical search strategy to identify overlapping protein complexes. A series of extensive experiments have been conducted to evaluate the performance of FCAN-PCI by comparing it with state-of-the-art identification algorithms and the results demonstrate the promising performance of FCAN-PCI.",
        "We provide a systematic approach to validate the results of clustering methods on weighted networks, in particular for the cases where the existence of a community structure is unknown. Our validation of clustering comprises a set of criteria for assessing their significance and stability. To test for cluster significance, we introduce a set of community scoring functions adapted to weighted networks, and systematically compare their values to those of a suitable null model. For this we propose a switching model to produce randomized graphs with weighted edges while maintaining the degree distribution constant. To test for cluster stability, we introduce a non parametric bootstrap method combined with similarity metrics derived from information theory and combinatorics. In order to assess the effectiveness of our clustering quality evaluation methods, we test them on synthetically generated weighted networks with a ground truth community structure of varying strength based on the stochastic block model construction. When applying the proposed methods to these synthetic ground truth networks' clusters, as well as to other weighted networks with known community structure, these correctly identify the best performing algorithms, which suggests their adequacy for cases where the clustering structure is not known. We test our clustering validation methods on a varied collection of well known clustering algorithms applied to the synthetically generated networks and to several real world weighted networks. All our clustering validation methods are implemented in R, and will be released in the upcoming package clustAnalytics.",
        "Hypergraphs are a natural modeling paradigm for networked systems with multiway interactions. A standard task in network analysis is the identification of closely related or densely interconnected nodes. We propose a probabilistic generative model of clustered hypergraphs with heterogeneous node degrees and edge sizes. Approximate maximum likelihood inference in this model leads to a clustering objective that generalizes the popular modularity objective for graphs. From this, we derive an inference algorithm that generalizes the Louvain graph community detection method, and a faster, specialized variant in which edges are expected to lie fully within clusters. Using synthetic and empirical data, we demonstrate that the specialized method is highly scalable and can detect clusters where graph-based methods fail. We also use our model to find interpretable higher-order structure in school contact networks, U.S. congressional bill cosponsorship and committees, product categories in copurchasing behavior, and hotel locations from web browsing sessions.",
        "BACKGROUND: Facing the diversity of omics data and the difficulty of selecting one result over all those produced by several methods, consensus strategies have the potential to reconcile multiple inputs and to produce robust results. RESULTS: Here, we introduce ClustOmics, a generic consensus clustering tool that we use in the context of cancer subtyping. ClustOmics relies on a non-relational graph database, which allows for the simultaneous integration of both multiple omics data and results from various clustering methods. This new tool conciliates input clusterings, regardless of their origin, their number, their size or their shape. ClustOmics implements an intuitive and flexible strategy, based upon the idea of evidence accumulation clustering. ClustOmics computes co-occurrences of pairs of samples in input clusters and uses this score as a similarity measure to reorganize data into consensus clusters. CONCLUSION: We applied ClustOmics to multi-omics disease subtyping on real TCGA cancer data from ten different cancer types. We showed that ClustOmics is robust to heterogeneous qualities of input partitions, smoothing and reconciling preliminary predictions into high-quality consensus clusters, both from a computational and a biological point of view. The comparison to a state-of-the-art consensus-based integration tool, COCA, further corroborated this statement. However, the main interest of ClustOmics is not to compete with other tools, but rather to make profit from their various predictions when no gold-standard metric is available to assess their significance. AVAILABILITY: The ClustOmics source code, released under MIT license, and the results obtained on TCGA cancer data are available on GitHub: https://github.com/galadrielbriere/ClustOmics .",
        "[This corrects the article DOI: 10.3389/fpsyg.2020.00944.].",
        "Monitoring of an underwater environment and communication is essential for many applications, such as sea habitat monitoring, offshore investigation and mineral exploration, but due to underwater current, low bandwidth, high water pressure, propagation delay and error probability, underwater communication is challenging. In this paper, we proposed a sensor node clustering technique for UWSNs named as adaptive node clustering technique (ANC-UWSNs). It uses a dragonfly optimization (DFO) algorithm for selecting ideal measure of clusters needed for routing. The DFO algorithm is inspired by the swarming behavior of dragons. The proposed methodology correlates with other algorithms, for example the ant colony optimizer (ACO), comprehensive learning particle swarm optimizer (CLPSO), gray wolf optimizer (GWO) and moth flame optimizer (MFO). Grid size, transmission range and nodes density are used in a performance matrix, which varies during simulation. Results show that DFO outperform the other algorithms. It produces a higher optimized number of clusters as compared to other algorithms and hence optimizes overall routing and increases the life span of a network.",
        "Pupil segmentation is critical for line-of-sight estimation based on the pupil center method. Due to noise and individual differences in human eyes, the quality of eye images often varies, making pupil segmentation difficult. In this paper, we propose a pupil segmentation method based on fuzzy clustering of distributed information, which first preprocesses the original eye image to remove features such as eyebrows and shadows and highlight the pupil area; then the Gaussian model is introduced into global distribution information to enhance the classification fuzzy affiliation for the local neighborhood, and an adaptive local window filter that fuses local spatial and intensity information is proposed to suppress the noise in the image and preserve the edge information of the pupil details. Finally, the intensity histogram of the filtered image is used for fast clustering to obtain the clustering center of the pupil, and this binarization process is used to segment the pupil for the next pupil localization. Experimental results show that the method has high segmentation accuracy, sensitivity, and specificity. It can accurately segment the pupil when there are interference factors such as light spots, light reflection, and contrast difference at the edge of the pupil, which is an important contribution to improving the stability and accuracy of the line-of-sight tracking.",
        "Understanding the energy landscape and the conformational dynamics is crucial for studying many biological or chemical processes, such as protein-protein interaction and RNA folding. Molecular Dynamics (MD) simulations have been a major source of dynamic structure. Although many methods were proposed for learning metastable states from MD data, some key problems are still in need of further investigation. Here, we give a brief review on recent progresses in this field, with an emphasis on some popular methods belonging to a two-step clustering framework, and hope to draw more researchers to contribute to this area.",
        "Negotiation scoring systems are fundamental tools used in negotiation support to facilitate parties searching for negotiation agreement and analyzing its efficiency and fairness. Such a scoring system is obtained in prenegotiation by implementing selected multiple criteria decision-aiding methods to elicit the negotiator's preferences precisely and ensure that the support is reliable. However, the methods classically used in the preference elicitation require much cognitive effort from the negotiators, and hence, do not prevent them from using heuristics and making simple errors that result in inaccurate scoring systems. This paper aims to develop an alternative tool that allows scoring the negotiation offers by implementing a sorting approach and the reference set of limiting profiles defined individually by the negotiators in the form of complete packages. These limiting profiles are evaluated holistically and verbally by the negotiator. Then the fuzzy decision model is built that uses the notion of increasing the preference granularity by introducing a series of limiting sub-profiles for corresponding sub-categories of offers. This process is performed automatically by the support algorithm and does not require any additional preferential information from the negotiator. A new method of generating reference fuzzy scores to allow a detailed assignment of any negotiation offer from feasible negotiation space to clusters and sub-clusters is proposed. Finally, the efficient frontier and Nash's fair division are used to identify the recommended packages for negotiation in the bargaining phase. This new approach allows negotiators to obtain economically efficient, fair, balanced, and reciprocated agreements while minimizing information needs and effort.",
        "Wi-Fi-based indoor positioning systems have a simple layout and a low cost, and they have gradually become popular in both academia and industry. However, due to the poor stability of Wi-Fi signals, it is difficult to accurately decide the position based on a received signal strength indicator (RSSI) by using a traditional dataset and a deep learning classifier. To overcome this difficulty, we present a clustering-based noise elimination scheme (CNES) for RSSI-based datasets. The scheme facilitates the region-based clustering of RSSIs through density-based spatial clustering of applications with noise. In this scheme, the RSSI-based dataset is preprocessed and noise samples are removed by CNES. This experiment was carried out in a dynamic environment, and we evaluated the lab simulation results of CNES using deep learning classifiers. The results showed that applying CNES to the test database to eliminate noise will increase the success probability of fingerprint location. The lab simulation results show that after using CNES, the average positioning accuracy of margin-zero (zero-meter error), margin-one (two-meter error), and margin-two (four-meter error) in the database increased by 17.78%, 7.24%, and 4.75%, respectively. We evaluated the simulation results with a real time testing experiment, where the result showed that CNES improved the average positioning accuracy to 22.43%, 9.15%, and 5.21% for margin-zero, margin-one, and margin-two error, respectively.",
        "Computer numerical control (CNC) is a machine used in the manufacturing industry to produce components quickly for the engineering field or the desired shape. In the milling process carried out by CNC machines, sometimes vibrations occur that cause unwanted cracks or damage, which if left unchecked, will cause more severe damage. For this reason, this study describes how to monitor and analyze the sound produced by CNC during the milling process. This study uses six sound sample videos from YouTube, and there are two modes: (1) the operating mode is three different shapes with XY, XZ, and XYZ axes, and the second (2) is based on material differences. Namely, wood, Styrofoam, and plastic. The sound generated from all samples of the CNC milling processes will be detected using a sound detection program that has been designed in the LabVIEW using a simple microphone. The resulting sound frequency will be analyzed using the fast Fourier transform (FFT) process in spectral measurements, which will produce the amplitude and frequency of the detected sound in real time in the form of a graph. All frequency results that have been obtained from the sound detection monitoring tool in the CNC milling machine will be imported into the K-means clustering algorithm where the different frequencies between the resonant frequency and noise will be classified. Based on the experiments conducted, the sound detection program can detect sounds with a significant level of sensitivity.",
        "Unmanned aerial vehicles (UAVs) in the role of flying anchor nodes have been proposed to assist the localisation of terrestrial Internet of Things (IoT) sensors and provide relay services in the context of the upcoming 6G networks. This paper considered the objective of tracing a mobile IoT device of unknown location, using a group of UAVs that were equipped with received signal strength indicator (RSSI) sensors. The UAVs employed measurements of the target's radio frequency (RF) signal power to approach the target as quickly as possible. A deep learning model performed clustering in the UAV network at regular intervals, based on a graph convolutional network (GCN) architecture, which utilised information about the RSSI and the UAV positions. The number of clusters was determined dynamically at each instant using a heuristic method, and the partitions were determined by optimising an RSSI loss function. The proposed algorithm retained the clusters that approached the RF source more effectively, removing the rest of the UAVs, which returned to the base. Simulation experiments demonstrated the improvement of this method compared to a previous deterministic approach, in terms of the time required to reach the target and the total distance covered by the UAVs.",
        "A k-means algorithm is a method for clustering that has already gained a wide range of acceptability. However, its performance extremely depends on the opening cluster centers. Besides, due to weak exploration capability, it is easily stuck at local optima. Recently, a new metaheuristic called Moth Flame Optimizer (MFO) is proposed to handle complex problems. MFO simulates the moths intelligence, known as transverse orientation, used to navigate in nature. In various research work, the performance of MFO is found quite satisfactory. This paper suggests a novel heuristic approach based on the MFO to solve data clustering problems. To validate the competitiveness of the proposed approach, various experiments have been conducted using Shape and UCI benchmark datasets. The proposed approach is compared with five state-of-art algorithms over twelve datasets. The mean performance of the proposed algorithm is superior on 10 datasets and comparable in remaining two datasets. The analysis of experimental results confirms the efficacy of the suggested approach.",
        "The segmentation of high-grade gliomas (HGG) using magnetic resonance imaging (MRI) data is clinically meaningful in neurosurgical practice, but a challenging task. Currently, most segmentation methods are supervised learning with labeled training sets. Although these methods work well in most cases, they typically require time-consuming manual labeling and pre-trained models. In this work, we propose an automatically unsupervised segmentation toolbox based on the clustering algorithm and morphological processing, named AUCseg. With our toolbox, the whole tumor was first extracted by clustering on T2-FLAIR images. Then, based on the mask acquired with whole tumor segmentation, the enhancing tumor was segmented on the post-contrast T1-weighted images (T1-CE) using clustering methods. Finally, the necrotic regions were segmented by morphological processing or clustering on T2-weighted images. Compared with K-means, Mini-batch K-means, and Fuzzy C Means (FCM), the Gaussian Mixture Model (GMM) clustering performs the best in our toolbox. We did a multi-sided evaluation of our toolbox in the BraTS2018 dataset and demonstrated that the whole tumor, tumor core, and enhancing tumor can be automatically segmented using default hyper-parameters with Dice score 0.8209, 0.7087, and 0.7254, respectively. The computing time of our toolbox for each case is around 22 seconds, which is at least 3 times faster than other state-of-the-art unsupervised methods. In addition, our toolbox has an option to perform semi-automatic segmentation via manually setup hyper-parameters, which could improve the segmentation performance. Our toolbox, AUCseg, is publicly available on Github. (https://github.com/Haifengtao/AUCseg).",
        "The problems of data abnormalities and missing data are puzzling the traditional multi-modal heterogeneous big data clustering. In order to solve this issue, a multi-view heterogeneous big data clustering algorithm based on improved Kmeans clustering is established in this paper. At first, for the big data which involve heterogeneous data, based on multi view data analyzing, we propose an advanced Kmeans algorithm on the base of multi view heterogeneous system to determine the similarity detection metrics. Then, a BP neural network method is used to predict the missing attribute values, complete the missing data and restore the big data structure in heterogeneous state. Last, we ulteriorly propose a data denoising algorithm to denoise the abnormal data. Based on the above methods, we construct a framework namely BPK-means to resolve the problems of data abnormalities and missing data. Our solution approach is evaluated through rigorous performance evaluation study. Compared with the original algorithm, both theoretical verification and experimental results show that the accuracy of the proposed method is greatly improved.",
        "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering has gained popularity due to its flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the lower dimensional latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on the MNIST benchmark data set and challenging real-world tasks of clustering mouse organs from single-cell RNA-sequencing measurements and defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines as well as competitor methods.",
        "Multiview subspace clustering is one of the most widely used methods for exploiting the internal structures of multiview data. Most previous studies have performed the task of learning multiview representations by individually constructing an affinity matrix for each view without simultaneously exploiting the intrinsic characteristics of multiview data. In this article, we propose a multiview low-rank representation (MLRR) method to comprehensively discover the correlation of multiview data for multiview subspace clustering. MLRR considers symmetric low-rank representations (LRRs) to be an approximately linear spatial transformation under the new base, that is, the multiview data themselves, to fully exploit the angular information of the principal directions of LRRs, which is adopted to construct an affinity matrix for multiview subspace clustering, under a symmetric condition. MLRR takes full advantage of LRR techniques and a diversity regularization term to exploit the diversity and consistency of multiple views, respectively, and this method simultaneously imposes a symmetry constraint on LRRs. Hence, the angular information of the principal directions of rows is consistent with that of columns in symmetric LRRs. The MLRR model can be efficiently calculated by solving a convex optimization problem. Moreover, we present an intuitive fusion strategy for symmetric LRRs from the perspective of spectral clustering to obtain a compact representation, which can be shared by multiple views and comprehensively represents the intrinsic features of multiview data. Finally, the experimental results based on benchmark datasets demonstrate the effectiveness and robustness of MLRR compared with several state-of-the-art multiview subspace clustering algorithms.",
        "Finding a consensus embedding from multiple views is the mainstream task in multiview graph-based clustering, in which the key problem is to handle the inconsistence among multiple views. In this article, we consider clustering effectiveness and practical applicability collectively, and propose a parameter-free model to alleviate the inconsistence of multiple views cleverly. To be specific, the proposed model considers the diversities of multiple views as two-layers. The first layer considers the inconsistence among different features of each view and the second layer considers linking the preembeddings of multiple views attentively. By this way, a consensus embedding can be learned via kernel method effectively and the whole learning procedure is parameter-free. To solve the optimization problem involved in the proposed model, we propose an alternative algorithm which is efficient and easy to implement in practice. In the experiments, we evaluate the proposed model on synthetic and real datasets and the experimental results demonstrate its effectiveness.",
        "BACKGROUND: Comparison of gene expression algorithms may be beneficial for obtaining disease pattern or grouping patients based on the gene expression profile. The current study aimed to investigate whether the knowledge within these data is able to group the ovarian cancer patients with similar disease pattern. METHODS: Four different clustering methods were applied on 20 genes expression data of 37 women with ovarian cancer. All selected genes in this study had prominent roles in the control of the activity of the immune system, as well as the chemotaxis, angiogenesis, apoptosis, and etc. Comparison of different clustering methods such as K-means, Hierarchical, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Expectation-Maximization (EM) algorithm was the other aim of the present study. In addition, the percentage of correct prediction, Robustness-Performance Trade-off (RPT), and Silhouette criteria were used to evaluate the performance of clustering methods. RESULTS: Six out of 20 genes (IFN-gamma, Foxp3, IL-4, BCL-2, Oct4 and survivin) selected by the Laplacian score showed key roles in the development of ovarian cancer and their prognostic values were clinically and statistically confirmed. The results indicated proper capability of the expression pattern of these genes in grouping the patients with similar prognosis, i.e. patients alive after 5 years or dead (62.12%). CONCLUSION: The results revealed the better performance for k-means and hierarchical clustering methods, and confirmed the fact that by using the expression profile of these genes, patients with similar behavior can be grouped in the same cluster with acceptable accuracy level. Certainly, the useful information from these data may contribute to the prediction of prognosis in ovarian cancer patients along with other features of patients.<br />.",
        "A central nervous system (CNS) disease affecting the insulating myelin sheaths around the brain axons is called multiple sclerosis (MS). In today's world, MS is extensively diagnosed and monitored using the MRI, because of the structural MRI sensitivity in dissemination of white matter lesions with respect to space and time. The main aim of this study is to propose Multiple Sclerosis Lesion Segmentation in Brain MRI imaging using Optimized Deep Convolutional Neural Network and Super-pixel Clustering. Three stages included in the proposed methodology are: (a) preprocessing, (b) segmentation of super-pixel, and (c) classification of super-pixel. In the first stage, image enhancement and skull stripping is done through performing a preprocessing step. In the second stage, the MS lesion and Non-MS lesion regions are segmented through applying SLICO algorithm over each slice of the volume. In the fourth stage, a CNN training and classification is performed using this segmented lesion and non-lesion regions. To handle this complex task, a newly developed Improved Particle Swarm Optimization (IPSO) based optimized convolutional neural network classifier is applied. On clinical MS data, the approach exhibits a significant increase in the accuracy segmenting of WM lesions when compared with the rest of evaluated methods.",
        "A variety of medical imaging procedures, cadaver experiments, and computer models have been utilized to capture, depict, and understand the motion of the human lumbar spine. Particular interest lies in assessing the relative movement between two adjacent vertebrae, which can be represented by a temporal evolution of finite helical axes (FHA). Mathematically, this FHA evolution constitutes a seven-dimensional quantity: one dimension for the time, two for the (normalized) direction vector, another two for the (unique) position vector, as well as one for each the angle of rotation around and the amount of translation along the axis. Predominantly in the literature, however, movements are assumed to take place in certain physiological planes on which FHA are projected. The resulting three-dimensional quantity - the so-called centrode - is easily presentable but leaves out substantial pieces of available data. Here, we investigate and assess several possibilities to visualize subsets of FHA data of increasing dimensionality. Finally, we utilize an agglomerative hierarchical clustering algorithm and propose a novel visualization technique, namely the quiver principal axis plot (QPAP), to depict the entirety of information inherent to hundreds or thousands of FHA. The QPAP method is applied to flexion-extension, lateral bending, and axial rotation movements of a lumbar spine within both a reduced model as well as a complex upper body system.",
        "Here, we report density functional theory calculations combined with the k-means clustering algorithm and the Spearman rank correlation analysis to investigate the stability mechanisms of eight-atom binary metal AB clusters, where A and B are Fe, Co, Ni, Cu, Ga, Al, and Zn (7 unary and 21 binary clusters). Based on the excess energy analysis, the six most stable binary clusters are NiAl, NiGa, CoAl, FeNi, NiZn, and FeAl, and except for FeNi, their highest energetic stabilities can be explained by the hybridization of the d- and sp-states, which is maximized at the 50% composition, i.e., A4B4. Based on the Spearman correlation analysis, the energetic stability of the binary clusters increases with an increase in the highest occupied molecule orbital-lowest unoccupied molecular orbital (HOMO-LUMO) energy separation, which can be considered as a global descriptor. Furthermore, reducing the total magnetic moment values increases the stability for binary clusters without the Fe, Co, and Ni species, while the binary FeB, CoB, and NiB clusters increase their energetic stability with a decrease in the cluster radius, respectively, i.e., an energetic preference for compact structures.",
        "Elucidation of cell subpopulations at high resolution is a key and challenging goal of single-cell ribonucleic acid (RNA) sequencing (scRNA-seq) data analysis. Although unsupervised clustering methods have been proposed for de novo identification of cell populations, their performance and robustness suffer from the high variability, low capture efficiency and high dropout rates which are characteristic of scRNA-seq experiments. Here, we present a novel unsupervised method for Single-cell Clustering by Enhancing Network Affinity (SCENA), which mainly employed three strategies: selecting multiple gene sets, enhancing local affinity among cells and clustering of consensus matrices. Large-scale validations on 13 real scRNA-seq datasets show that SCENA has high accuracy in detecting cell populations and is robust against dropout noise. When we applied SCENA to large-scale scRNA-seq data of mouse brain cells, known cell types were successfully detected, and novel cell types of interneurons were identified with differential expression of gamma-aminobutyric acid receptor subunits and transporters. SCENA is equipped with CPU + GPU (Central Processing Units + Graphics Processing Units) heterogeneous parallel computing to achieve high running speed. The high performance and running speed of SCENA combine into a new and efficient platform for biological discoveries in clustering analysis of large and diverse scRNA-seq datasets.",
        "Clustering algorithms based on deep neural networks have been widely studied for image analysis. Most existing methods require partial knowledge of the true labels, namely, the number of clusters, which is usually not available in practice. In this article, we propose a Bayesian nonparametric framework, deep nonparametric Bayes (DNB), for jointly learning image clusters and deep representations in a doubly unsupervised manner. In doubly unsupervised learning, we are dealing with the problem of ``unknown unknowns,'' where we estimate not only the unknown image labels but also the unknown number of labels as well. The proposed algorithm alternates between generating a potentially unbounded number of clusters in the forward pass and learning the deep networks in the backward pass. With the help of the Dirichlet process mixtures, the proposed method is able to partition the latent representations space without specifying the number of clusters a priori. An important feature of this work is that all the estimation is realized with an end-to-end solution, which is very different from the methods that rely on post hoc analysis to select the number of clusters. Another key idea in this article is to provide a principled solution to the problem of ``trivial solution'' for deep clustering, which has not been much studied in the current literature. With extensive experiments on benchmark datasets, we show that our doubly unsupervised method achieves good clustering performance and outperforms many other unsupervised image clustering methods.",
        "The rapid growth in virtualization solutions has driven the widespread adoption of cloud computing paradigms among various industries and applications. This has led to a growing need for XaaS solutions and equipment to enable teleworking. To meet this need, cloud operators and datacenters have to overtake several challenges related to continuity, the quality of services provided, data security, and anomaly detection issues. Mainly, anomaly detection methods play a critical role in detecting virtual machines' abnormal behaviours that can potentially violate service level agreements established with users. Unsupervised machine learning techniques are among the most commonly used technologies for implementing anomaly detection systems. This paper introduces a novel clustering approach for analyzing virtual machine behaviour while running workloads in a system based on resource usage details (such as CPU utilization and downtime events). The proposed algorithm is inspired by the intuitive mechanism of flocking birds in nature to form reasonable clusters. Each starling movement's direction depends on self-information and information provided by other close starlings during the flight. Analogically, after associating a weight with each data sample to guide the formation of meaningful groups, each data element determines its next position in the feature space based on its current position and surroundings. Based on a realistic dataset and clustering validity indices, the experimental evaluation shows that the new weighted fuzzy c-means algorithm provides interesting results and outperforms the corresponding standard algorithm (weighted fuzzy c-means).",
        "This paper aims to propose a new model for time series forecasting that combines forecasting with clustering algorithm. It introduces a new scheme to improve the forecasting results by grouping the time series data using k-means clustering algorithm. It utilizes the clustering result to get the forecasting data. There are usually some user-defined parameters affecting the forecasting results, therefore, a learning-based procedure is proposed to estimate the parameters that will be used for forecasting. This parameter value is computed in the algorithm simultaneously. The result of the experiment compared to other forecasting algorithms demonstrates good results for the proposed model. It has the smallest mean squared error of 13,007.91 and the average improvement rate of 19.83%.",
        "The accumulated omic data poses a challenge for the integrative analysis of them. Although great efforts have been devoted to address this issue, the performance of current algorithms is not desirable because of the complexity and heterogeneity of data. The ultimate goal of this study is to propose an algorithm (aka NMF-DEC) to integrate the interactome and transcriptome data by using attributed networks. To circumvent the heterogeneity of attributed networks, a similarity network is constructed for the attributes of genes, casting it into the common module detection problem in multi-layer networks. To explore the relation between attributes and topological structure of networks, NMF-DEC jointly factorizes the similarity and interaction networks with the same basis, where the interaction network is dynamically updated during the optimization procedure. In this case, information of attributes is dynamically incorporated into the interaction networks, providing a better strategy to characterize the structure of modules in attributed networks. The extensive experiments indicates that NMF-DEC is more accurate than state-of-the-art baselines on the social networks, and it also outperforms the baselines on the cancer attributed networks, implying the superiority of the proposed methods for the integrative analysis of omic data.",
        "Query optimization is the process of identifying the best Query Execution Plan (QEP). The query optimizer produces a close to optimal QEP for the given queries based on the minimum resource usage. The problem is that for a given query, there are plenty of different equivalent execution plans, each with a corresponding execution cost. To produce an effective query plan thus requires examining a large number of alternative plans. Access plan recommendation is an alternative technique to database query optimization, which reuses the previously-generated QEPs to execute new queries. In this technique, the query optimizer uses clustering methods to identify groups of similar queries. However, clustering such large datasets is challenging for traditional clustering algorithms due to huge processing time. Numerous cloud-based platforms have been introduced that offer low-cost solutions for the processing of distributed queries such as Hadoop, Hive, Pig, etc. This paper has applied and tested a model for clustering variant sizes of large query datasets parallelly using MapReduce. The results demonstrate the effectiveness of the parallel implementation of query workloads clustering to achieve good scalability.",
        "Attributed graph clustering aims to discover node groups by utilizing both graph structure and node features. Recent studies mostly adopt graph neural networks to learn node embeddings, then apply traditional clustering methods to obtain clusters. However, they usually suffer from the following issues: (1) they adopt original graph structure which is unfavorable for clustering due to its noise and sparsity problems; (2) they mainly utilize non-clustering driven losses that cannot well capture the global cluster structure, thus the learned embeddings are not sufficient for the downstream clustering task. In this paper, we propose a spectral embedding network for attributed graph clustering (SENet), which improves graph structure by leveraging the information of shared neighbors, and learns node embeddings with the help of a spectral clustering loss. By combining the original graph structure and shared neighbor based similarity, both the first-order and second-order proximities are encoded into the improved graph structure, thus alleviating the noise and sparsity issues. To make the spectral loss well adapt to attributed graphs, we integrate both structure and feature information into kernel matrix via a higher-order graph convolution. Experiments on benchmark attributed graphs show that SENet achieves superior performance over state-of-the-art methods.",
        "BACKGROUND: Internet provides different tools for communicating with patients, such as social media (e.g., Twitter) and email platforms. These platforms provided new data sources to shed lights on patient experiences with health care and improve our understanding of patient-provider communication. Several existing topic modeling and document clustering methods have been adapted to analyze these new free-text data automatically. However, both tweets and emails are often composed of short texts; and existing topic modeling and clustering approaches have suboptimal performance on these short texts. Moreover, research over health-related short texts using these methods has become difficult to reproduce and benchmark, partially due to the absence of a detailed comparison of state-of-the-art topic modeling and clustering methods on these short texts. METHODS: We trained eight state-of- the-art topic modeling and clustering algorithms on short texts from two health-related datasets (tweets and emails): Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), LDA with Gibbs Sampling (GibbsLDA), Online LDA, Biterm Model (BTM), Online Twitter LDA, and Gibbs Sampling for Dirichlet Multinomial Mixture (GSDMM), as well as the k-means clustering algorithm with two different feature representations: TF-IDF and Doc2Vec. We used cluster validity indices to evaluate the performance of topic modeling and clustering: two internal indices (i.e. assessing the goodness of a clustering structure without external information) and five external indices (i.e. comparing the results of a cluster analysis to an externally known provided class labels). RESULTS: In overall, for number of clusters (k) from 2 to 50, Online Twitter LDA and GSDMM achieved the best performance in terms of internal indices, while LSI and k-means with TF-IDF had the highest external indices. Also, of all tweets (N=286, 971; HPV represents 94.6% of tweets and lynch syndrome represents 5.4%), for k=2, most of the methods could respect this initial clustering distribution. However, we found model performance varies with the source of data and hyper-parameters such as the number of topics and the number of iterations used to train the models. We also conducted an error analysis using the Hamming loss metric, for which the poorest value was obtained by GSDMM on both datasets. CONCLUSIONS: Researchers hoping to group or classify health related short-text data can expect to select the most suitable topic modeling and clustering methods for their specific research questions. Therefore, we presented a comparison of the most common used topic modeling and clustering algorithms over two health-related, short-text datasets using both internal and external clustering validation indices. Internal indices suggested Online Twitter LDA and GSDMM as the best, while external indices suggested LSI and k-means with TF-IDF as the best. In summary, our work suggested researchers can improve their analysis of model performance by using a variety of metrics, since there is not a single best metric.",
        "BACKGROUND: Health behaviors such as physical inactivity, unhealthy eating, smoking tobacco, and alcohol use are leading risk factors for noncommunicable chronic diseases and play a central role in limiting health and life satisfaction. To date, however, health behaviors tend to be considered separately from one another, resulting in guidelines and interventions for healthy aging siloed by specific behaviors and often focused only on a given health behavior without considering the co-occurrence of family, social, work, and other behaviors of everyday life. OBJECTIVE: The aim of this study is to understand how behaviors cluster and how such clusters are associated with physical and mental health, life satisfaction, and health care utilization may provide opportunities to leverage this co-occurrence to develop and evaluate interventions to promote multiple health behavior changes. METHODS: Using cross-sectional baseline data from the Canadian Longitudinal Study on Aging, we will perform a predefined set of exploratory and hypothesis-generating analyses to examine the co-occurrence of health and everyday life behaviors. We will use agglomerative hierarchical cluster analysis to cluster individuals based on their behavioral tendencies. Multinomial logistic regression will then be used to model the relationships between clusters and demographic indicators, health care utilization, and general health and life satisfaction, and assess whether sex and age moderate these relationships. In addition, we will conduct network community detection analysis using the clique percolation algorithm to detect overlapping communities of behaviors based on the strength of relationships between variables. RESULTS: Baseline data for the Canadian Longitudinal Study on Aging were collected from 51,338 participants aged between 45 and 85 years. Data were collected between 2010 and 2015. Secondary data analysis for this project was approved by the Ottawa Health Science Network Research Ethics Board (protocol ID #20190506-01H). CONCLUSIONS: This study will help to inform the development of interventions tailored to subpopulations of adults (eg, physically inactive smokers) defined by the multiple behaviors that describe their everyday life experiences. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/24887.",
        "BACKGROUND: Current approaches to studying relations between taste perception and diet quality typically consider each taste-sweet, salt, sour, bitter, umami-separately or aggregately, as total taste scores. Consistent with studying dietary patterns rather than single foods or total energy, an additional approach may be to study all 5 tastes collectively as \"taste perception profiles.\" OBJECTIVE: We developed a data-driven clustering approach to derive taste perception profiles from taste perception scores and examined whether profiles outperformed total taste scores for capturing individual variability in taste perception. METHODS: The cohort included 367 community-dwelling adults [55-75 y; 55% female; BMI (kg/m2): 32.2 +/- 3.6] with metabolic syndrome from PREDIMED-Plus, Valencia. Cluster analysis identified subgroups of individuals with similar patterns in taste perception (taste perception profiles); quantitative criteria were used to select the cluster algorithm, determine the optimal number of clusters, and assess the profiles' validity and stability. Goodness-of-fit parameters from adjusted linear regression evaluated the individual variability captured by each approach. RESULTS: A k-means algorithm with 6 clusters best fit the data and identified the following taste perception profiles: Low All, High Bitter, High Umami, Low Bitter & Umami, High All But Bitter and High All But Umami. All profiles were valid and stable. Compared with total taste scores, taste perception profiles explained more variability in bitter and umami perception (adjusted R2: 0.19 vs. 0.63, respectively; 0.40 vs. 0.65, respectively) and were comparable for sweet, salt, and sour. In addition, taste perception profiles captured differential perceptions of each taste within individuals, whereas these patterns were lost with total taste scores. CONCLUSIONS: Among older adults with metabolic syndrome, taste perception profiles derived via data-driven clustering may provide a valuable approach to capture individual variability in perception of all 5 tastes and their collective influence on diet quality. This trial was registered at https://www.isrctn.com/ as ISRCTN89898870.",
        "Image clustering has recently attracted significant attention due to the increased availability of unlabeled datasets. The efficiency of traditional clustering algorithms heavily depends on the distance functions used and the dimensionality of the features. Therefore, performance degradation is often observed when tackling either unprocessed images or high-dimensional features extracted from processed images. To deal with these challenges, we propose a deep clustering framework consisting of a modified generative adversarial network (GAN) and an auxiliary classifier. The modification employs Sobel operations prior to the discriminator of the GAN to enhance the separability of the learned features. The discriminator is then leveraged to generate representations as to the input to an auxiliary classifier. An objective function is utilized to train the auxiliary classifier by maximizing the mutual information between the representations obtained via the discriminator model and the same representations perturbed via adversarial training. We further improve the robustness of the auxiliary classifier by introducing a penalty term into the objective function. This minimizes the divergence across multiple transformed representations generated by the discriminator model with a low dropout rate. The auxiliary classifier is implemented with a group of multiple cluster-heads, where a tolerance hyper-parameter is used to tackle imbalanced data. Our results indicate that the proposed method achieves competitive results compared with state-of-the-art clustering methods on a wide range of benchmark datasets including CIFAR-10, CIFAR-100/20, and STL10.",
        "When using deep neural networks in medical image classification tasks, it is mandatory to prepare a large-scale labeled image set, and this often requires significant effort by medical experts. One strategy to reduce the labeling cost is group-based labeling, where image samples are clustered and then a label is attached to each cluster. The efficiency of this strategy depends on the purity of the clusters. Constrained clustering is an effective way to improve the purity of the clusters if we can give appropriate must-links and cannot-links as constraints. However, for medical image clustering, the conventional constrained clustering methods encounter two issues. The first issue is that constraints are not always appropriate due to the gap between semantic and visual similarities. The second issue is that attaching constraints requires extra effort from medical experts. To deal with the first issue, we propose a novel soft-constrained clustering method, which has the ability to ignore inappropriate constraints. To deal with the second issue, we propose a self-constrained clustering method that utilizes prior knowledge about the target images to set the constraints automatically. Experiments with the endoscopic image datasets demonstrated that the proposed methods give clustering results with higher purity.",
        "Evolutionary multiobjective clustering (MOC) algorithms have shown promising potential to outperform conventional single-objective clustering algorithms, especially when the number of clusters k is not set before clustering. However, the computational burden becomes a tricky problem due to the extensive search space and fitness computational time of the evolving population, especially when the data size is large. This article proposes a new, hierarchical, topology-based cluster representation for scalable MOC, which can simplify the search procedure and decrease computational overhead. A coarse-to-fine-trained topological structure that fits the spatial distribution of the data is utilized to identify a set of seed points/nodes, then a tree-based graph is built to represent clusters. During optimization, a bipartite graph partitioning strategy incorporated with the graph nodes helps in performing a cluster ensemble operation to generate offspring solutions more effectively. For the determination of the final result, which is underexplored in the existing methods, the usage of a cluster ensemble strategy is also presented, whether k is provided or not. Comparison experiments are conducted on a series of different data distributions, revealing the superiority of the proposed algorithm in terms of both clustering performance and computing efficiency.",
        "Critically ill patients constitute a highly heterogeneous population, with seemingly distinct patients having similar outcomes, and patients with the same admission diagnosis having opposite clinical trajectories. We aimed to develop a machine learning methodology that identifies and provides better characterization of patient clusters at high risk of mortality and kidney injury. We analysed prospectively collected data including co-morbidities, clinical examination, and laboratory parameters from a minimally-selected population of 743 patients admitted to the ICU of a Dutch hospital between 2015 and 2017. We compared four clustering methodologies and trained a classifier to predict and validate cluster membership. The contribution of different variables to the predicted cluster membership was assessed using SHapley Additive exPlanations values. We found that deep embedded clustering yielded better results compared to the traditional clustering algorithms. The best cluster configuration was achieved for 6 clusters. All clusters were clinically recognizable, and differed in in-ICU, 30-day, and 90-day mortality, as well as incidence of acute kidney injury. We identified two high mortality risk clusters with at least 60%, 40%, and 30% increased. ICU, 30-day and 90-day mortality, and a low risk cluster with 25-56% lower mortality risk. This machine learning methodology combining deep embedded clustering and variable importance analysis, which we made publicly available, is a possible solution to challenges previously encountered by clustering analyses in heterogeneous patient populations and may help improve the characterization of risk groups in critical care.",
        "Due to the ``curse of dimensionality'' issue, how to discard redundant features and select informative features in high-dimensional data has become a critical problem, hence there are many research studies dedicated to solving this problem. Unsupervised feature selection technique, which does not require any prior category information to conduct with, has gained a prominent place in preprocessing high-dimensional data among all feature selection techniques, and it has been applied to many neural networks and learning systems related applications, e.g., pattern classification. In this article, we propose an efficient method for unsupervised feature selection via orthogonal basis clustering and reliable local structure preserving, which is referred to as OCLSP briefly. Our OCLSP method consists of an orthogonal basis clustering together with an adaptive graph regularization, which realizes the functionality of simultaneously achieving excellent cluster separation and preserving the local information of data. Besides, we exploit an efficient alternative optimization algorithm to solve the challenging optimization problem of our proposed OCLSP method, and we perform a theoretical analysis of its computational complexity and convergence. Eventually, we conduct comprehensive experiments on nine real-world datasets to test the validity of our proposed OCLSP method, and the experimental results demonstrate that our proposed OCLSP method outperforms many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy and normalized mutual information, which indicates that our proposed OCLSP method has a strong ability in identifying more important features.",
        "SUMMARY: Genomic sequences are widely used to infer the evolutionary history of a given group of individuals. Many methods have been developed for sequence clustering and tree building. In the early days of genome sequencing, these were often limited to hundreds of sequences, but due to the surge of high throughput sequencing, it is now common to have millions of sampled sequences at hand. We introduce MNHN-Tree-Tools, a high performance set of algorithms that builds multi-scale, nested clusters of sequences found in a FASTA file. MNHN-Tree-Tools does not rely on sequence alignment and can thus be used on large datasets to infer a sequence tree. Herein we outline two applications: A human alpha-satellite repeats classification and a tree of life derivation from 16S/18S rDNA sequences. CODE AVAILABILITY: Open source with a Zlib License via the Git protocol: https://gitlab.in2p3.fr/mnhn-tools/mnhn-tree-tools. SUPPLEMENTARY INFORMATION: An in depth discussion about the algorithm with numerical simulations: https://gitlab.in2p3.fr/mnhn-tools/tree-tools-algorithms-document/-/raw/master/ar ticle.pdf. MANUAL: A detailed users guide and tutorial: https://gitlab.in2p3.fr/mnhn-tools/mnhn-tree-tools-manual/-/raw/master/manual.pdf . WEBSITE AND FAQ: http://treetools.haschka.net.",
        "In proteomics, the identification of peptides from mass spectral data can be mathematically described as the partitioning of mass spectra into clusters (i.e., groups of spectra derived from the same peptide). The way partitions are validated is just as important, having evolved side by side with the clustering algorithms themselves and given rise to many partition assessment measures. An assessment measure is said to have a selection bias if, and only if, the probability that a randomly chosen partition scoring a high value depends on the number of clusters in the partition. In the context of clustering mass spectra, this might mislead the validation process to favor clustering algorithms that generate too many (or few) spectral clusters, regardless of the underlying peptide sequence. A selection bias toward the number of peptides is desirable for proteomics as it estimates the number of peptides in a complex protein mixture. Here, we introduce an assessment measure that is purposely biased toward the number of peptide ion species. We also introduce a partition assessment framework for proteomics, called the Partition Assessment Tool, and demonstrate its importance by evaluating the performance of eight clustering algorithms on seven proteomics datasets while discussing the trade-offs involved. SIGNIFICANCE: Clustering algorithms are widely adopted in proteomics for undertaking several tasks such as speeding up search engines, generating consensus mass spectra, and to aid in the classification of proteomic profiles. Choosing which algorithm is most fit for the task at hand is not simple as each algorithm has advantages and disadvantages; furthermore, specifying clustering parameters is also a necessary and fundamental step. For example, deciding on whether to generate \"pure clusters\" or fewer clusters but accepting noise. With this as motivation, we verify the performance of several widely adopted algorithms on proteomic datasets and introduce a theoretical framework for drawing conclusions on which approach is suitable for the task at hand.",
        "Deep gray matter nuclei are the synaptic relays, responsible to route signals between specific brain areas. Dentate nuclei (DNs) represent the main output channel of the cerebellum and yet are often unexplored especially in humans. We developed a multimodal MRI approach to identify DNs topography on the basis of their connectivity as well as their microstructural features. Based on results, we defined DN parcellations deputed to motor and to higher-order functions in humans in vivo. Whole-brain probabilistic tractography was performed on 25 healthy subjects from the Human Connectome Project to infer DN parcellations based on their connectivity with either the cerebral or the cerebellar cortex, in turn. A third DN atlas was created inputting microstructural diffusion-derived metrics in an unsupervised fuzzy c-means classification algorithm. All analyses were performed in native space, with probability atlas maps generated in standard space. Cerebellar lobule-specific connectivity identified one motor parcellation, accounting for about 30% of the DN volume, and two non-motor parcellations, one cognitive and one sensory, which occupied the remaining volume. The other two approaches provided overlapping results in terms of geometrical distribution with those identified with cerebellar lobule-specific connectivity, although with some differences in volumes. A gender effect was observed with respect to motor areas and higher-order function representations. This is the first study that indicates that more than half of the DN volumes is involved in non-motor functions and that connectivity-based and microstructure-based atlases provide complementary information. These results represent a step-ahead for the interpretation of pathological conditions involving cerebro-cerebellar circuits.",
        "In the current age of overwhelming information and massive production of textual data on the Web, Event Detection has become an increasingly important task in various application domains. Several research branches have been developed to tackle the problem from different perspectives, including Natural Language Processing and Big Data analysis, with the goal of providing valuable resources to support decision-making in a wide variety of fields. In this paper, we propose a real-time domain-specific clustering-based event-detection approach that integrates textual information coming, on one hand, from traditional newswires and, on the other hand, from microblogging platforms. The goal of the implemented pipeline is twofold: (i) providing insights to the user about the relevant events that are reported in the press on a daily basis; (ii) alerting the user about potentially important and impactful events, referred to as hot events, for some specific tasks or domains of interest. The algorithm identifies clusters of related news stories published by globally renowned press sources, which guarantee authoritative, noise-free information about current affairs; subsequently, the content extracted from microblogs is associated to the clusters in order to gain an assessment of the relevance of the event in the public opinion. To identify the events of a day d we create the lexicon by looking at news articles and stock data of previous days up to d(-1) Although the approach can be extended to a variety of domains (e.g. politics, economy, sports), we hereby present a specific implementation in the financial sector. We validated our solution through a qualitative and quantitative evaluation, performed on the Dow Jones' Data, News and Analytics dataset, on a stream of messages extracted from the microblogging platform Stocktwits, and on the Standard & Poor's 500 index time-series. The experiments demonstrate the effectiveness of our proposal in extracting meaningful information from real-world events and in spotting hot events in the financial sphere. An added value of the evaluation is given by the visual inspection of a selected number of significant real-world events, starting from the Brexit Referendum and reaching until the recent outbreak of the Covid-19 pandemic in early 2020.",
        "Gastric cancer (GC) is highly heterogeneous in the stromal and immune microenvironment, genome instability (GI), and oncogenic signatures. However, a classification of GC by combining these features remains lacking. Using the consensus clustering algorithm, we clustered GCs based on the activities of 15 pathways associated with immune, DNA repair, oncogenic, and stromal signatures in three GC datasets. We identified three GC subtypes: immunity-deprived (ImD), stroma-enriched (StE), and immunity-enriched (ImE). ImD showed low immune infiltration, high DNA damage repair activity, high tumor aneuploidy level, high intratumor heterogeneity (ITH), and frequent TP53 mutations. StE displayed high stromal signatures, low DNA damage repair activity, genomic stability, low ITH, and poor prognosis. ImE had strong immune infiltration, high DNA damage repair activity, high tumor mutation burden, prevalence of microsatellite instability, frequent ARID1A mutations, elevated PD-L1 expression, and favorable prognosis. Based on the expression levels of four genes (TAP2, SERPINB5, LTBP1, and LAMC1) in immune, DNA repair, oncogenic, and stromal pathways, we developed a prognostic model (IDOScore). The IDOScore was an adverse prognostic factor and correlated inversely with immunotherapy response in cancer. Our identification of new GC subtypes provides novel insights into tumor biology and has potential clinical implications for the management of GCs.",
        "BACKGROUND: The rapid development of single-cell RNA sequencing (scRNA-seq) enables the exploration of cell heterogeneity, which is usually done by scRNA-seq data clustering. The essence of scRNA-seq data clustering is to group cells by measuring the similarities among genes/transcripts of cells. And the selection of features for cell similarity evaluation is of great importance, which will significantly impact clustering effectiveness and efficiency. RESULTS: In this paper, we propose a novel method called CaFew to select genes based on cluster-aware feature weighting. By optimizing the clustering objective function, CaFew obtains a feature weight matrix, which is further used for feature selection. The genes have large weights in at least one cluster or the genes whose weights vary greatly in different clusters are selected. Experiments on 8 real scRNA-seq datasets show that CaFew can obviously improve the clustering performance of existing scRNA-seq data clustering methods. Particularly, the combination of CaFew with SC3 achieves the state-of-art performance. Furthermore, CaFew also benefits the visualization of scRNA-seq data. CONCLUSION: CaFew is an effective scRNA-seq data clustering method due to its gene selection mechanism based on cluster-aware feature weighting, and it is a useful tool for scRNA-seq data analysis.",
        "Technological advances have enabled us to profile multiple molecular layers at unprecedented single-cell resolution and the available datasets from multiple samples or domains are growing. These datasets, including scRNA-seq data, scATAC-seq data and sc-methylation data, usually have different powers in identifying the unknown cell types through clustering. So, methods that integrate multiple datasets can potentially lead to a better clustering performance. Here we propose coupleCoC+ for the integrative analysis of single-cell genomic data. coupleCoC+ is a transfer learning method based on the information-theoretic co-clustering framework. In coupleCoC+, we utilize the information in one dataset, the source data, to facilitate the analysis of another dataset, the target data. coupleCoC+ uses the linked features in the two datasets for effective knowledge transfer, and it also uses the information of the features in the target data that are unlinked with the source data. In addition, coupleCoC+ matches similar cell types across the source data and the target data. By applying coupleCoC+ to the integrative clustering of mouse cortex scATAC-seq data and scRNA-seq data, mouse and human scRNA-seq data, mouse cortex sc-methylation and scRNA-seq data, and human blood dendritic cells scRNA-seq data from two batches, we demonstrate that coupleCoC+ improves the overall clustering performance and matches the cell subpopulations across multimodal single-cell genomic datasets. coupleCoC+ has fast convergence and it is computationally efficient. The software is available at https://github.com/cuhklinlab/coupleCoC_plus.",
        "There is a well-established tradition within the statistics literature that explores different techniques for reducing the dimensionality of large feature spaces. The problem is central to machine learning and it has been largely explored under the unsupervised learning paradigm. We introduce a supervised clustering methodology that capitalizes on a Metropolis Hastings algorithm to optimize the partition structure of a large categorical feature space tailored towards minimizing the test error of a learning algorithm. This is a general methodology that can be applied to any supervised learning problem with a large categorical feature space. We show the benefits of the algorithm by applying this methodology to the problem of risk adjustment in competitive health insurance markets. We use a large claims data set that records ICD-10 codes, a large categorical feature space. We aim at improving risk adjustment by clustering diagnostic codes into risk groups suitable for health expenditure prediction. We test the performance of our methodology against common alternatives using panel data from a representative sample of twenty three million citizens in Colombian Healthcare System. Our results outperform common alternatives and suggest that it has potential to improve risk adjustment.",
        "Text document clustering refers to the unsupervised classification of textual documents into clusters based on content similarity and can be applied in applications such as search optimization and extracting hidden information from data generated by IoT sensors. Swarm intelligence (SI) algorithms use stochastic and heuristic principles that include simple and unintelligent individuals that follow some simple rules to accomplish very complex tasks. By mapping features of problems to parameters of SI algorithms, SI algorithms can achieve solutions in a flexible, robust, decentralized, and self-organized manner. Compared to traditional clustering algorithms, these solving mechanisms make swarm algorithms suitable for resolving complex document clustering problems. However, each SI algorithm shows a different performance based on its own strengths and weaknesses. In this paper, to find the best performing SI algorithm in text document clustering, we performed a comparative study for the PSO, bat, grey wolf optimization (GWO), and K-means algorithms using six data sets of various sizes, which were created from BBC Sport news and 20 newsgroups. Based on our experimental results, we discuss the features of a document clustering problem with the nature of SI algorithms and conclude that the PSO and GWO SI algorithms are better than K-means, and among those algorithms, the PSO performs best in terms of finding the optimal solution.",
        "The distinguishable subregions that compose the hippocampus are differently involved in functions associated with Alzheimer's disease (AD). Thus, the identification of hippocampal subregions and genes that classify AD and healthy control (HC) groups with high accuracy is meaningful. In this study, by jointly analyzing the multimodal data, we propose a novel method to construct fusion features and a classification method based on the random forest for identifying the important features. Specifically, we construct the fusion features using the gene sequence and subregions correlation to reduce the diversity in same group. Moreover, samples and features are selected randomly to construct a random forest, and genetic algorithm and clustering evolutionary are used to amplify the difference in initial decision trees and evolve the trees. The features in resulting decision trees that reach the peak classification are the important \"subregion gene pairs\". The findings verify that our method outperforms well in classification performance and generalization. Particularly, we identified some significant subregions and genes, such as hippocampus amygdala transition area (HATA), fimbria, parasubiculum and genes included RYR3 and PRKCE. These discoveries provide some new candidate genes for AD and demonstrate the contribution of hippocampal subregions and genes to AD.",
        "We introduce the problem of clustering the set of vertices in a given 3D mesh. The problem is motivated by the need for value engineering in architectural projects. We first derive a max-norm based metric to estimate the geometric disparity between a given pair of vertices, and characterize the problem in terms of this measure. We show that this distance can be computed by using Sequential Quadratic Programming (SQP). Next we introduce two different algorithms for clustering the set of vertices on a given mesh, respectively based on two disparity measurements: max-norm and L2-norm based metric. An equivalence is established between mesh vertices and physical joints in an architectural mesh. By replacing individual joints by their equivalent cluster representative, the number of unique joints in the facade mesh, and therefore the fabrication cost, is dramatically reduced. Finally, we present an algorithm for remeshing a given surface in order to further reduce the number of joint clusters. The framework is tested for a set of real-world architectural surfaces to illustrate the effectiveness and utility of our approach. Overall, this approach tackles the important problem reducing fabrication cost of joints without modifying the underlying connectivity that was specified by the architect.",
        "Maltose crystallization affects the processibility and stability of sugar-rich foods. This study introduced a color-based clustering algorithm (CCA) to analyze crystallinity from the images of amorphous maltose/protein models. The XRD and DSC were also implemented in maltose crystallization characterization and validated the CCA analysis. The results indicated that CCA could effectively recognize maltose crystals (R = 0.9942), and amorphous maltose mainly crystallized to anhydrate alpha-maltose and beta-maltose monohydrate according to its morphological aspects measured by CCA, XRD, and DSC. However, protein could change the mechanism of maltose crystal formation by disturbing the mutarotation and recrystallization processes of unstable beta-maltose. Besides, maltose crystal formation and crystallinity were governed by molecular mobility as the CCA-derived Avrami indexes changed with the Strength parameter. Compared to XRD and DSC, the proposed CCA can provide a rapid and quantitative measure for maltose crystallinity and has great potential applications in the online detection of sugar crystallization.",
        "BACKGROUND: Single-cell RNA sequencing (scRNA-seq) has emerged has a main strategy to study transcriptional activity at the cellular level. Clustering analysis is routinely performed on scRNA-seq data to explore, recognize or discover underlying cell identities. The high dimensionality of scRNA-seq data and its significant sparsity accentuated by frequent dropout events, introducing false zero count observations, make the clustering analysis computationally challenging. Even though multiple scRNA-seq clustering techniques have been proposed, there is no consensus on the best performing approach. On a parallel research track, self-supervised contrastive learning recently achieved state-of-the-art results on images clustering and, subsequently, image classification. RESULTS: We propose contrastive-sc, a new unsupervised learning method for scRNA-seq data that perform cell clustering. The method consists of two consecutive phases: first, an artificial neural network learns an embedding for each cell through a representation training phase. The embedding is then clustered in the second phase with a general clustering algorithm (i.e. KMeans or Leiden community detection). The proposed representation training phase is a new adaptation of the self-supervised contrastive learning framework, initially proposed for image processing, to scRNA-seq data. contrastive-sc has been compared with ten state-of-the-art techniques. A broad experimental study has been conducted on both simulated and real-world datasets, assessing multiple external and internal clustering performance metrics (i.e. ARI, NMI, Silhouette, Calinski scores). Our experimental analysis shows that constastive-sc compares favorably with state-of-the-art methods on both simulated and real-world datasets. CONCLUSION: On average, our method identifies well-defined clusters in close agreement with ground truth annotations. Our method is computationally efficient, being fast to train and having a limited memory footprint. contrastive-sc maintains good performance when only a fraction of input cells is provided and is robust to changes in hyperparameters or network architecture. The decoupling between the creation of the embedding and the clustering phase allows the flexibility to choose a suitable clustering algorithm (i.e. KMeans when the number of expected clusters is known, Leiden otherwise) or to integrate the embedding with other existing techniques.",
        "Patients' electronic records in community pharmacy are an untapped resource to uncover new ways of providing healthcare services. In this paper, we present a preliminary work, where we explore this resource, aiming to identify patients' clusters that will help to define a future algorithm. This algorithm will then enable community pharmacists to provide tailored pharmaceutical interventions according to patient's risk assessment and needs. In this way, this work will provide a way to overcome known barriers for community pharmacists' provision of services and integration in the health system, while also contributing to support a better care for chronic patients.",
        "The present study is devoted to interpretable artificial intelligence in medicine. In our previous work we proposed an approach to clustering results interpretation based on Bayesian Inference. As an application case we used clinical pathways clustering explanation. However, the approach was limited by working for only binary features. In this work, we expand the functionality of the method and adapt it for modelling posterior distributions of continuous features. To solve the task, we apply BEST algorithm to provide Bayesian t-testing and use NUTS algorithm for posterior sampling. The general results of both binary and continuous interpretation provided by the algorithm have been compared with the interpretation of two medical experts.",
        "BACKGROUND: In recent years, various sequencing techniques have been used to collect biomedical omics datasets. It is usually possible to obtain multiple types of omics data from a single patient sample. Clustering of omics data plays an indispensable role in biological and medical research, and it is helpful to reveal data structures from multiple collections. Nevertheless, clustering of omics data consists of many challenges. The primary challenges in omics data analysis come from high dimension of data and small size of sample. Therefore, it is difficult to find a suitable integration method for structural analysis of multiple datasets. RESULTS: In this paper, a multi-view clustering based on Stiefel manifold method (MCSM) is proposed. The MCSM method comprises three core steps. Firstly, we established a binary optimization model for the simultaneous clustering problem. Secondly, we solved the optimization problem by linear search algorithm based on Stiefel manifold. Finally, we integrated the clustering results obtained from three omics by using k-nearest neighbor method. We applied this approach to four cancer datasets on TCGA. The result shows that our method is superior to several state-of-art methods, which depends on the hypothesis that the underlying omics cluster class is the same. CONCLUSION: Particularly, our approach has better performance than compared approaches when the underlying clusters are inconsistent. For patients with different subtypes, both consistent and differential clusters can be identified at the same time.",
        "Heavy metal adsorption onto biochar is an effective method for the treatment of the heavy metal contamination of water and wastewater. This study aims to evaluate the heavy metals sorption efficiency of different biochar characteristics and propose a novel intelligence method for predicting the sorption efficiency of heavy metal onto biochar with high accuracy based on the back-propagation neural network (BPNN) and fuzzy C-means clustering algorithm (FCM), named as FCM-BPNN. Accordingly, the FCM algorithm was used to simulate the properties of metal adsorption data and divide them into clusters with similar features. The clustering results showed that the FCM algorithm simulated metal adsorption data's properties very well and classified them based on biochar characteristics and adsorption conditions. Afterward, BPNN models were well-developed based on these clusters, and their outcomes were then combined (i.e., FCM-BPNN). The results indicated that the FCM-BPNN model could predict heavy metal's sorption efficiency onto biochar with a promising result (i.e., RMSE of 0.036, R(2) of 0.987, RSE of 0.006, MAPE of 0.706, and VAF of 98.724). Whereas the BPNN model, without optimizing the FCM algorithm, was proved with lower performance (RMSE = 0.050, R(2) = 0.977, RSE = 0.011, MAPE = 0.802, and VAF = 97.662). These findings revealed that the FCM algorithm's presence impressively improved the BPNN model's accomplishment in predicting heavy metal's sorption efficiency onto biochar, and the proposed FCM-BPNN model can improve water/wastewater treatment plants' quality and provide a more efficient process for heavy metals with performance superiority.",
        "Auto-Encoder (AE)-based deep subspace clustering (DSC) methods have achieved impressive performance due to the powerful representation extracted using deep neural networks while prioritizing categorical separability. However, self-reconstruction loss of an AE ignores rich useful relation information and might lead to indiscriminative representation, which inevitably degrades the clustering performance. It is also challenging to learn high-level similarity without feeding semantic labels. Another unsolved problem facing DSC is the huge memory cost due to nxn similarity matrix, which is incurred by the self-expression layer between an encoder and decoder. To tackle these problems, we use pairwise similarity to weigh the reconstruction loss to capture local structure information, while a similarity is learned by the self-expression layer. Pseudo-graphs and pseudo-labels, which allow benefiting from uncertain knowledge acquired during network training, are further employed to supervise similarity learning. Joint learning and iterative training facilitate to obtain an overall optimal solution. Extensive experiments on benchmark datasets demonstrate the superiority of our approach. By combining with the k -nearest neighbors algorithm, we further show that our method can address the large-scale and out-of-sample problems. The source code of our method is available: https://github.com/sckangz/SelfsupervisedSC.",
        "Towards exploring the topological structure of data, numerous graph embedding clustering methods have been developed in recent years, none of them takes into account the cluster-specificity distribution of the nodes representations, resulting in suboptimal clustering performance. Moreover, most existing graph embedding clustering methods execute the nodes representations learning and clustering in two separated steps, which increases the instability of its original performance. Additionally, rare of them simultaneously takes node attributes reconstruction and graph structure reconstruction into account, resulting in degrading the capability of graph learning. In this work, we integrate the nodes representations learning and clustering into a unified framework, and propose a new deep graph attention auto-encoder for nodes clustering that attempts to learn more favorable nodes representations by leveraging self-attention mechanism and node attributes reconstruction. Meanwhile, a cluster-specificity distribution constraint, which is measured by l1,2-norm, is employed to make the nodes representations within the same cluster end up with a common distribution in the dimension space while representations with different clusters have different distributions in the intrinsic dimensions. Extensive experiment results reveal that our proposed method is superior to several state-of-the-art methods in terms of performance.",
        "Geochemical fingerprinting is a rapidly expanding discipline in the earth and environmental sciences, anchored in the recognition that geological processes leave behind physical, chemical and sometimes also isotopic patterns in the samples. Furthermore, the geochemical fingerprinting of natural cycles (water, carbon, soil and biota fingerprinting) are influenced by the anthropogenic impact and by the climate change. So, their monitoring is a tool of resilience and adaptation. In recent years, computational statistics and artificial intelligence methods have started to be used to help the process of geochemical fingerprinting. In this paper we consider data from 57 wells located in the province of Ferrara (Italy), all belonging to the same geological group and separated into 4 different aquifers. The aquifer from which each well extracts its water is known only in 18 of the 57 cases, while in other 39 cases it can be only hypothesized based on geological considerations. We devise a novel technique for geochemical fingerprinting of groundwater by means of which we are able to identify the exact aquifer from which a sample is extracted with a sufficiently high accuracy. Then, we experimentally prove that out method is sensibly more accurate than typical statistical approaches, such as principal component analysis, for this particular problem.",
        "The aim of the work is to identify a clustering structure for the 20 Italian regions according to the main variables related to COVID-19 pandemic. Data are observed over time, spanning from the last week of February 2020 to the first week of February 2021. Dealing with geographical units observed at several time occasions, the proposed fuzzy clustering model embedded both space and time information. Properly, an Exponential distance-based Fuzzy Partitioning Around Medoids algorithm with spatial penalty term has been proposed to classify the spline representation of the time trajectories. The results show that the heterogeneity among regions along with the spatial contiguity is essential to understand the spread of the pandemic and to design effective policies to mitigate the effects.",
        "Social distancing to reduce the spread of coronavirus disease 2019 (COVID-19) made a huge increase in the global OTT market, and OTT service providers get millions of new subscribers. Recently OTT service providers are extending their service to video broadcasting. As a one type of video broadcasting, this paper covers multimedia streaming with multiple sources. Multimedia streaming with multiple sources has multiple sources, and receivers can select one specific source to watch the video from the source. Sources include cameras capturing different angles of same event or location, cameras in geographical locations, etc. For delivering video to rapidly increasing number of users, multimedia streaming with multiple sources system needs efficient and scalable delivery method. Tree-based Peer-to-peer (P2P) networking has been investigated as the delivery solution of multimedia streaming with multiple sources, and set-top boxes or mobile apps of OTT service can be used as peers connecting the subscriber of OTT service. However, the scalability of the tree-based P2P networking is limited by the out-degree of a tree that branches linearly with the number of users. Hence, this study proposes clustering peers based on the location proximity of the peers to enhance the scalability of the P2P multimedia streaming with multiple sources. By clustering peers, one or more peers can be grouped into a virtual peer with an aggregated uplink/downlink capacity. This paper describes P2P multimedia streaming with multiple sources and algorithms for the proposed clustering method. Two applications which are one-view multiparty video conferencing and multi-view video streaming are introduced, and considerations for applying the proposed method to the applications are also discussed. The experimental results show that location-proximity-based clustering is effective in achieving a scalable P2P multimedia streaming with multiple sources by reducing the out-degree of a tree for the introduced applications. The proposed clustering leads improvement in the maximum achievable video bit rate, the average viewing video bit rate, and perceived delay.",
        "A novel combination of machine learning algorithms is proposed for the differentiation of distinct spectra in a large electron energy loss spectroscopy spectrum image (EELS-SI) dataset. For clustering of the EEL spectra including similar fine structures in an efficient space, linear and nonlinear dimensionality reduction methods are used to project the EEL spectra onto a low-dimensional space. Then, a density-based clustering algorithm is applied to distinguish the meaningful data clusters. By applying this strategy to various experimental EELS-SI datasets, differentiation of several groups of EEL spectra representing specific fine structures was achieved. It is possible to investigate particular fine structures by averaging all of the spectra in each cluster. Also, the spatial distributions of each cluster in the scanning regions can be observed, which enables investigation of the locations of different fine structures in materials. This method does not require any prior knowledge, i.e., it is a data-driven analysis; therefore, it can be applied to any hyperspectral image.",
        "MOTIVATION: Hi-C technology provides insights into the 3D organization of the chromatin, and the single-cell Hi-C method enables researchers to gain knowledge about the chromatin state in individual cell levels. Single-cell Hi-C interaction matrices are high dimensional and very sparse. To cluster thousands of single-cell Hi-C interaction matrices, they are flattened and compiled into one matrix. Depending on the resolution, this matrix can have a few million or even billions of features; therefore, computations can be memory intensive. We present a single-cell Hi-C clustering approach using an approximate nearest neighbors method based on locality-sensitive hashing to reduce the dimensions and the computational resources. RESULTS: The presented method can process a 10 kb single-cell Hi-C data set with 2600 cells and needs 40 GB of memory, while competitive approaches are not computable even with 1 TB of memory. It can be shown that the differentiation of the cells by their chromatin folding properties and, therefore, the quality of the clustering of single-cell Hi-C data is advantageous compared to competitive algorithms. AVAILABILITY: The presented clustering algorithm is part of the scHiCExplorer, is available on Github https://github.com/joachimwolff/scHiCExplorer, and as a conda package via the bioconda channel. The approximate nearest neighbors implementation is available via https://github.com/joachimwolff/sparse-neighbors-search and as a conda package via the bioconda channel. SUPPLEMENTARY INFORMATION: Supplementary data online available.",
        "Sparsity constrained optimization problems are common in machine learning, such as sparse coding, low-rank minimization and compressive sensing. However, most of previous studies focused on constructing various hand-crafted sparse regularizers, while little work was devoted to learning adaptive sparse regularizers from given input data for specific tasks. In this paper, we propose a deep sparse regularizer learning model that learns data-driven sparse regularizers adaptively. Via the proximal gradient algorithm, we find that the sparse regularizer learning is equivalent to learning a parameterized activation function. This encourages us to learn sparse regularizers in the deep learning framework. Therefore, we build a neural network composed of multiple blocks, each being differentiable and reusable. All blocks contain learnable piecewise linear activation functions which correspond to the sparse regularizer to be learned. Further, the proposed model is trained with back propagation, and all parameters in this model are learned end-to-end. We apply our framework to the multi-view clustering and semi-supervised classification tasks for learning a latent compact representation. Experimental results demonstrate the superiority of the proposed framework over state-of-the-art multi-view learning models.",
        "In the form of multidimensional arrays, tensor data have become increasingly prevalent in modern scientific studies and biomedical applications such as computational biology, brain imaging analysis, and process monitoring system. These data are intrinsically heterogeneous with complex dependencies and structure. Therefore, ad-hoc dimension reduction methods on tensor data may lack statistical efficiency and can obscure essential findings. Model-based clustering is a cornerstone of multivariate statistics and unsupervised learning; however, existing methods and algorithms are not designed for tensor-variate samples. In this article, we propose a tensor envelope mixture model (TEMM) for simultaneous clustering and multiway dimension reduction of tensor data. TEMM incorporates tensor-structure-preserving dimension reduction into mixture modeling and drastically reduces the number of free parameters and estimative variability. An expectation-maximization-type algorithm is developed to obtain likelihood-based estimators of the cluster means and covariances, which are jointly parameterized and constrained onto a series of lower dimensional subspaces known as the tensor envelopes. We demonstrate the encouraging empirical performance of the proposed method in extensive simulation studies and a real data application in comparison with existing vector and tensor clustering methods.",
        "In this article, we analyse the usefulness of multidimensional scaling in relation to performing K-means clustering on a dissimilarity matrix, when the dimensionality of the objects is unknown. In this situation, traditional algorithms cannot be used, and so K-means clustering procedures are being performed directly on the basis of the observed dissimilarity matrix. Furthermore, the application of criteria originally formulated for two-mode data sets to determine the number of clusters depends on their possible reformulation in a one-mode situation. The linear invariance property in K-means clustering for squared dissimilarities, together with the use of multidimensional scaling, is investigated to determine the cluster membership of the observations and to address the problem of selecting the number of clusters in K-means for a dissimilarity matrix. In particular, we analyse the performance of K-means clustering on the full dimensional scaling configuration and on the equivalently partitioned configuration related to a suitable translation of the squared dissimilarities. A Monte Carlo experiment is conducted in which the methodology examined is compared with the results obtained by procedures directly applicable to a dissimilarity matrix.",
        "Single cell sequencing (SCS) technologies provide a level of resolution that makes it indispensable for inferring from a sequenced tumor, evolutionary trees or phylogenies representing an accumulation of cancerous mutations. A drawback of SCS is elevated false negative and missing value rates, resulting in a large space of possible solutions, which in turn makes it difficult, sometimes infeasible using current approaches and tools. One possible solution is to reduce the size of an SCS instance - usually represented as a matrix of presence, absence, and uncertainty of the mutations found in the different sequenced cells - and to infer the tree from this reduced-size instance. In this work, we present a new clustering procedure aimed at clustering such categorical vector, or matrix data - here representing SCS instances, called celluloid. We show that celluloid clusters mutations with high precision: never pairing too many mutations that are unrelated in the ground truth, but also obtains accurate results in terms of the phylogeny inferred downstream from the reduced instance produced by this method. We demonstrate the usefulness of a clustering step by applying the entire pipeline (clustering + inference method) to a real dataset, showing a significant reduction in the runtime, raising considerably the upper bound on the size of SCS instances which can be solved in practice. Our approach, celluloid: clustering single cell sequencing data around centroids is available at https://github.com/AlgoLab/celluloid/ under an MIT license, as well as on the Python Package Index (PyPI) at https://pypi.org/project/celluloid-clust/.",
        "An unsupervised machine learning strategy is developed to automatically cluster the vortex wakes of bio-inspired propulsors into groups of similar propulsive thrust and efficiency metrics. A pitching and heaving foil is simulated via computational fluid dynamics with 121 unique kinematics by varying the frequency, heaving amplitude, and pitching amplitude. A Reynolds averaged Navier-Stokes model is employed to simulate the flow over the oscillating foils at Re = 10(6), computing the propulsive efficiency, thrust coefficient and the unsteady vorticity wake signature. Using a pairwise Pearson correlation it is found that the Strouhal number most strongly influences the thrust coefficient, whereas the relative angle of attack, defined by both the mid-stroke and maximum have the most significant impact on propulsive efficiency. Next, the various kinematics are automatically clustered into distinct groups exclusively using the vorticity footprint in the wake. A convolutional autoencoder is developed to reduce vortex wake images to their most significant features, and ak-means++ algorithm performs the clustering. The results are assessed by comparing clusters to a thrust versus propulsive efficiency map, which confirms that wakes of similar performance metrics are successfully clustered together. This automated clustering has the potential to identify complex vorticity patterns in the wake and modes of propulsion not easily discerned from traditional classification methods.",
        "In batch spawning fish, secondary growth oocytes (SGO) are recruited and spawned in successive cohorts, and multiple cohorts co-occur in spawning-capable females. So far, histological features such as the prevalence of cortical alveoli or yolk granules are conservatively used to distinguish oocytes in different developmental stages which do not necessarily correspond to different cohorts. In this way, valuable information about spawning dynamics remains unseen and consequently misleading conclusions might be drawn, especially for species with high spawning rates and increased overlapping among oocyte cohorts. We introduce a new method for grouping oocytes into different cohorts based on the application of the K-means clustering algorithm on the characteristics of cytoplasmic structures, such as the varying size and intensity of cortical alveoli and yolk granules in oocytes of different development. The method allowed the grouping of oocytes without the need of using oocyte diameter, and thus, a crucial histological bias dealing with the cutting angle and the orientation of reference points (e.g. nucleus) has been overcome. Using sardine, Sardina pilchardus, as a case study, the separation of cohorts provided new insight into the ovarian dynamics, indentifying successive recruitment of up to five oocyte cohorts between SGO recruitment and spawning. These results verified previous histological indications of the number of cohorts in sardine. Altogether, this method represents an improved tool to study species with complex ovarian dynamics.",
        "Determining the best partition for a dataset can be a challenging task because of the lack of a priori information within an unsupervised learning framework and the absence of a unique clustering validation approach to evaluate clustering solutions. Here we present reval: a Python package that leverages stability-based relative clustering validation methods to select best clustering solutions as the ones that replicate, via supervised learning, on unseen subsets of data. The implementation of relative validation methods can contribute to the theory of clustering by fostering new approaches for the investigation of clustering results in different situations and for different data distributions. This work aims at contributing to this effort by implementing a package that works with multiple clustering and classification algorithms, hence allowing both the automation of the labeling process and the assessment of the stability of different clustering mechanisms.",
        "This article presents the data used to evaluate the performance of evolutionary clustering algorithm star (ECA*) compared to five traditional and modern clustering algorithms. Two experimental methods are employed to examine the performance of ECA* against genetic algorithm for clustering++ (GENCLUST++), learning vector quantisation (LVQ), expectation maximisation (EM), K-means++ (KM++) and K-means (KM). These algorithms are applied to 32 heterogenous and multi-featured datasets to determine which one performs well on the three tests. For one, ther paper examines the efficiency of ECA* in contradiction of its corresponding algorithms using clustering evaluation measures. These validation criteria are objective function and cluster quality measures. For another, it suggests a performance rating framework to measurethe the performance sensitivity of these algorithms on varos dataset features (cluster dimensionality, number of clusters, cluster overlap, cluster shape and cluster structure). The contributions of these experiments are two-folds: (i) ECA* exceeds its counterpart aloriths in ability to find out the right cluster number; (ii) ECA* is less sensitive towards dataset features compared to its competitive techniques. Nonetheless, the results of the experiments performed demonstrate some limitations in the ECA*: (i) ECA* is not fully applied based on the premise that no prior knowledge exists; (ii) Adapting and utilising ECA* on several real applications has not been achieved yet.",
        "Genome-wide association study (GWAS) has identified thousands of genetic variants associated with complex traits and diseases. Compared with analyzing a single phenotype at a time, the joint analysis of multiple phenotypes can improve statistical power by taking into account the information from phenotypes. However, most established joint algorithms ignore the different level of correlations between multiple phenotypes; instead of that, they simultaneously analyze all phenotypes in a genetic model. Thus, they may fail to capture the genetic structure of phenotypes and consequently reduce the statistical power. In this study, we develop a novel method agglomerative nesting clustering algorithm for phenotypic dimension reduction analysis (AGNEP) to jointly analyze multiple phenotypes for GWAS. First, AGNEP uses an agglomerative nesting clustering algorithm to group correlated phenotypes and then applies principal component analysis (PCA) to generate representative phenotypes for each group. Finally, multivariate analysis is employed to test associations between genetic variants and the representative phenotypes rather than all phenotypes. We perform three simulation experiments with various genetic structures and a real dataset analysis for 19 Arabidopsis phenotypes. Compared to established methods, AGNEP is more powerful in terms of statistical power, computing time, and the number of quantitative trait nucleotides (QTNs). The analysis of the Arabidopsis real dataset further illustrates the efficiency of AGNEP for detecting QTNs, which are confirmed by The Arabidopsis Information Resource gene bank.",
        "Patient stratification has been studied widely to tackle subtype diagnosis problems for effective treatment. Due to the dimensionality curse and poor interpretability of data, there is always a long-lasting challenge in constructing a stratification model with high diagnostic ability and good generalization. To address these problems, this article proposes two novel evolutionary multiobjective clustering algorithms with ensemble (NSGA-II-ECFE and MOEA/D-ECFE) with four cluster validity indices used as the objective functions. First, an effective ensemble construction method is developed to enrich the ensemble diversity. After that, an ensemble clustering fitness evaluation (ECFE) method is proposed to evaluate the ensembles by measuring the consensus clustering under those four objective functions. To generate the consensus clustering, ECFE exploits the hybrid co-association matrix from the ensembles and then dynamically selects the suitable clustering algorithm on that matrix. Multiple experiments have been conducted to demonstrate the effectiveness of the proposed algorithm in comparison with seven clustering algorithms, twelve ensemble clustering approaches, and two multiobjective clustering algorithms on 55 synthetic datasets and 35 real patient stratification datasets. The experimental results demonstrate the competitive edges of the proposed algorithms over those compared methods. Furthermore, the proposed algorithm is applied to extend its advantages by identifying cancer subtypes from five cancer-related single-cell RNA-seq datasets.",
        "The rapid emergence of high-dimensional data in various areas has brought new challenges to current ensemble clustering research. To deal with the curse of dimensionality, recently considerable efforts in ensemble clustering have been made by means of different subspace-based techniques. However, besides the emphasis on subspaces, rather limited attention has been paid to the potential diversity in similarity/dissimilarity metrics. It remains a surprisingly open problem in ensemble clustering how to create and aggregate a large population of diversified metrics, and furthermore, how to jointly investigate the multilevel diversity in the large populations of metrics, subspaces, and clusters in a unified framework. To tackle this problem, this article proposes a novel multidiversified ensemble clustering approach. In particular, we create a large number of diversified metrics by randomizing a scaled exponential similarity kernel, which are then coupled with random subspaces to form a large set of metric-subspace pairs. Based on the similarity matrices derived from these metric-subspace pairs, an ensemble of diversified base clusterings can be thereby constructed. Furthermore, an entropy-based criterion is utilized to explore the cluster wise diversity in ensembles, based on which three specific ensemble clustering algorithms are presented by incorporating three types of consensus functions. Extensive experiments are conducted on 30 high-dimensional datasets, including 18 cancer gene expression datasets and 12 image/speech datasets, which demonstrate the superiority of our algorithms over the state of the art. The source code is available at https://github.com/huangdonghere/MDEC.",
        "MOTIVATION: Recent breakthroughs of single-cell RNA sequencing (scRNA-seq) technologies offer an exciting opportunity to identify heterogeneous cell types in complex tissues. However, the unavoidable biological noise and technical artifacts in scRNA-seq data as well as the high dimensionality of expression vectors make the problem highly challenging. Consequently, although numerous tools have been developed, their accuracy remains to be improved. RESULTS: Here, we introduce a novel clustering algorithm and tool RCSL (Rank Constrained Similarity Learning) to accurately identify various cell types using scRNA-seq data from a complex tissue. RCSL considers both local similarity and global similarity among the cells to discern the subtle differences among cells of the same type as well as larger differences among cells of different types. RCSL uses Spearman's rank correlations of a cell's expression vector with those of other cells to measure its global similarity, and adaptively learns neighbour representation of a cell as its local similarity. The overall similarity of a cell to other cells is a linear combination of its global similarity and local similarity. RCSL automatically estimates the number of cell types defined in the similarity matrix, and identifies them by constructing a block-diagonal matrix, such that its distance to the similarity matrix is minimized. Each block-diagonal submatrix is a cell cluster/type, corresponding to a connected component in the cognate similarity graph. When tested on 16 benchmark scRNA-seq datasets in which the cell types are well-annotated, RCSL substantially outperformed six state-of-the-art methods in accuracy and robustness as measured by three metrics. AVAILABILITY: The RCSL algorithm is implemented in R and can be freely downloaded at https://cran.r-project.org/web/packages/RCSL/index.html. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Data acquisition problem in large-scale distributed Wireless Sensor Networks (WSNs) is one of the main issues that hinder the evolution of Internet of Things (IoT) technology. Recently, combination of Compressive Sensing (CS) and routing protocols has attracted much attention. An open question in this approach is how to integrate these techniques effectively for specific tasks. In this paper, we introduce an effective deterministic clustering based CS scheme (DCCS) for fog-supported heterogeneous WSNs to handle the data acquisition problem. DCCS employs the concept of fog computing, reduces total overhead and computational cost needed to self-organize sensor network by using a simple approach, and then uses CS at each sensor node to minimize the overall energy expenditure and prolong the IoT network lifetime. Additionally, the proposed scheme includes an effective algorithm for CS reconstruction called Random Selection Matching Pursuit (RSMP) to enhance the recovery process at the base station (BS) side with a complete scenario using CS. RSMP adds random selection process during the forward step to give opportunity for more columns to be selected as an estimated solution in each iteration. The results of simulation prove that the proposed technique succeeds to minimize the overall network power expenditure, prolong the network lifetime and provide better performance in CS data reconstruction.",
        "Purpose: The goal of this study is to develop innovative methods for identifying radiomic features that are reproducible over varying image acquisition settings. Approach: We propose a regularized partial correlation network to identify reliable and reproducible radiomic features. This approach was tested on two radiomic feature sets generated using two different reconstruction methods on computed tomography (CT) scans from a cohort of 47 lung cancer patients. The largest common network component between the two networks was tested on phantom data consisting of five cancer samples. To further investigate whether radiomic features found can identify phenotypes, we propose a k -means clustering algorithm coupled with the optimal mass transport theory. This approach following the regularized partial correlation network analysis was tested on CT scans from 77 head and neck squamous cell carcinoma (HNSCC) patients in the Cancer Imaging Archive (TCIA) and validated using an independent dataset. Results: A set of common radiomic features was found in relatively large network components between the resultant two partial correlation networks resulting from a cohort of lung cancer patients. The reliability and reproducibility of those radiomic features were further validated on phantom data using the Wasserstein distance. Further analysis using the network-based Wasserstein k -means algorithm on the TCIA HNSCC data showed that the resulting clusters separate tumor subsites as well as HPV status, and this was validated on an independent dataset. Conclusion: We showed that a network-based analysis enables identifying reproducible radiomic features and use of the selected set of features can enhance clustering results.",
        "Many properties of water, such as turbulent flow, are closely related to water clusters, whereas how water clusters form and transform in bulk water remains unclear. A hierarchical clustering method is introduced to search out water clusters in hydrogen bonded network based on modified Louvain algorithm of graph community. Hydrogen bonds, rings and fragments are considered as 1st-, 2nd-, and 3rd-level structures, respectively. The distribution, dynamics and structural characteristics of 4th- and 5th-level clusters undergoing non-shear- and shear-driven flow are also analyzed at various temperatures. At low temperatures, nearly 50% of water molecules are included in clusters. Over 60% of clusters remain unchanged between neighboring configurations. Obvious collective translational motion of clusters is observed. The topological difference for clusters is elucidated between the inner layer, which favors 6-membered rings, and the external surface layer, which contains more 5-membered rings. Temperature and shearing can not only accelerate the transformation or destruction of clusters at all levels but also change cluster structures. The assembly of large clusters can be used to discretize continuous liquid water to elucidate the properties of liquid water.",
        "Data clustering facilitates the identification of biologically relevant molecular features in quantitative proteomics experiments with thousands of measurements over multiple conditions. It finds groups of proteins or peptides with similar quantitative behavior across multiple experimental conditions. This co-regulatory behavior suggests that the proteins of such a group share their functional behavior and thus often can be mapped to the same biological processes and molecular subnetworks.While usual clustering approaches dismiss the variance of the measured proteins, VSClust combines statistical testing with pattern recognition into a common algorithm. Here, we show how to use the VSClust web service on a large proteomics data set and present further tools to assess the quantitative behavior of protein complexes.",
        "Clustering algorithms for multi-database mining (MDM) rely on computing (n2-n)/2 pairwise similarities between n multiple databases to generate and evaluate min[1,(n2-n)/2] candidate clusterings in order to select the ideal partitioning that optimizes a predefined goodness measure. However, when these pairwise similarities are distributed around the mean value, the clustering algorithm becomes indecisive when choosing what database pairs are considered eligible to be grouped together. Consequently, a trivial result is produced by putting all the n databases in one cluster or by returning n singleton clusters. To tackle the latter problem, we propose a learning algorithm to reduce the fuzziness of the similarity matrix by minimizing a weighted binary entropy loss function via gradient descent and back-propagation. As a result, the learned model will improve the certainty of the clustering algorithm by correctly identifying the optimal database clusters. Additionally, in contrast to gradient-based clustering algorithms, which are sensitive to the choice of the learning rate and require more iterations to converge, we propose a learning-rate-free algorithm to assess the candidate clusterings generated on the fly in fewer upper-bounded iterations. To achieve our goal, we use coordinate descent (CD) and back-propagation to search for the optimal clustering of the n multiple database in a way that minimizes a convex clustering quality measure L(theta) in less than (n2-n)/2 iterations. By using a max-heap data structure within our CD algorithm, we optimally choose the largest weight variable thetap,q(i) at each iteration i such that taking the partial derivative of L(theta) with respect to thetap,q(i) allows us to attain the next steepest descent minimizing L(theta) without using a learning rate. Through a series of experiments on multiple database samples, we show that our algorithm outperforms the existing clustering algorithms for MDM.",
        "Round genomes are found in bacteria, plant chloroplasts, and mitochondria. Genetic or epigenetic marks can present biologically interesting clusters along a circular genome. The circular data clustering problem groups N points on a circle into K clusters to minimize the within-cluster sum of squared distances. Repeatedly applying the K-means algorithm takes quadratic time, impractical for large circular datasets. To overcome this issue, we developed a reproducible fast optimal circular clustering (FOCC) algorithm of worst-case O(KN log(2) N) time. The core is a fast optimal framed clustering algorithm, which we designed by integrating two divide-and-conquer and one bracket dynamic programming strategies. The algorithm is optimal based on a property of monotonic increasing cluster borders over frames on linearized data. On clustering 50,000 circular data points, FOCC outruns brute-force or heuristic circular clustering by three orders of magnitude in time. We produced clusters of CpG sites and genes along three round genomes, exhibiting higher quality than heuristic clustering. More broadly, the presented subquadratic-time algorithms offer the fastest known solution to not only framed and circular clustering, but also angular, periodical, and looped clustering. We implemented these algorithms in the R package 'OptCirClust' (https://CRAN.R-project.org/package=OptCirClust).",
        "BACKGROUND: Protein-protein interactions (PPIs) are the core of protein function, which provide an effective means to understand the function at cell level. Identification of PPIs is the crucial foundation of predicting drug-target interactions. Although traditional biological experiments of identifying PPIs are becoming available, these experiments remain to be extremely time-consuming and expensive. Therefore, various computational models have been introduced to identify PPIs. In protein-protein interaction network (PPIN), Hub protein, as a highly connected node, can coordinate PPIs and play biological functions. Detecting hot regions on Hub protein interaction interfaces is an issue worthy of discussing. METHODS: Two clustering methods, LCSD and RCNOIK are used to detect the hot regions on Hub protein interaction interfaces in this paper. In order to improve the efficiency of K-means clustering algorithm, the best k value is selected by calculating the distance square sum and the average silhouette coefficients. Then, the optimization of residue coordination number strategy is used to calculate the average coordination number. In addition, the pair potentials and relative ASA (PPRA) strategy is also used to optimize the predicted results. RESULTS: DataHub dataset and PartyHub dataset were used to train two clustering models respectively. Experiments show that LCSD and RCNOIK have the same coverage with Hub protein datasets, and RCNOIK is slightly higher than LCSD in Precision. The predicted hot regions are closer to the standard hot regions. CONCLUSIONS: This paper optimizes two clustering methods based on PPRA strategy. Compared our methods for hot regions prediction against the well-known approaches, our improved methods have the higher reliability and are effective for predicting hot regions on Hub protein interaction interfaces.",
        "One of the challenges of high granularity calorimeters, such as that to be built to cover the endcap region in the CMS Phase-2 Upgrade for HL-LHC, is that the large number of channels causes a surge in the computing load when clustering numerous digitized energy deposits (hits) in the reconstruction stage. In this article, we propose a fast and fully parallelizable density-based clustering algorithm, optimized for high-occupancy scenarios, where the number of clusters is much larger than the average number of hits in a cluster. The algorithm uses a grid spatial index for fast querying of neighbors and its timing scales linearly with the number of hits within the range considered. We also show a comparison of the performance on CPU and GPU implementations, demonstrating the power of algorithmic parallelization in the coming era of heterogeneous computing in high-energy physics.",
        "Antioxidant proteins perform significant functions in disease control and delaying aging which can prevent free radicals from damaging organisms. Accurate identification of antioxidant proteins has important implications for the development of new drugs and the treatment of related diseases, as they play a critical role in the control or prevention of cancer and aging-related conditions. Since experimental identification techniques are time-consuming and expensive, many computational methods have been proposed to identify antioxidant proteins. Although the accuracy of these methods is acceptable, there are still some challenges. In this study, we developed a computational model called ANPrAod to identify antioxidant proteins based on a support vector machine. In order to eliminate potential redundant features and improve prediction accuracy, 673 amino acid reduction alphabets were calculated by us to find the optimal feature representation scheme. The final model could produce an overall accuracy of 87.53% with the ROC of 0.7266 in five-fold cross-validation, which was better than the existing methods. The results of the independent dataset also demonstrated the excellent robustness and reliability of ANPrAod, which could be a promising tool for antioxidant protein identification and contribute to hypothesis-driven experimental design.",
        "OBJECTIVE: The heterogeneity of asthma has inspired widespread application of statistical clustering algorithms to a variety of datasets for identification of potentially clinically meaningful phenotypes. There has not been a standardized data analysis approach for asthma clustering, which can affect reproducibility and clinical translation of results. Our objective was to identify common and effective data analysis practices in the asthma clustering literature and apply them to data from a Southern California population-based cohort of schoolchildren with asthma. METHODS: As of January 1, 2020, we reviewed key statistical elements of 77 asthma clustering studies. Guided by the literature, we used 12 input variables and three clustering methods (hierarchical clustering, k-medoids, and latent class analysis) to identify clusters in 598 schoolchildren with asthma from the Southern California Children's Health Study (CHS). RESULTS: Clusters of children identified by latent class analysis were characterized by exhaled nitric oxide, FEV1/FVC, FEV1 percent predicted, asthma control and allergy score; and were predictive of control at two year follow up. Clusters from the other two methods were less clinically remarkable, primarily differentiated by sex and race/ethnicity and less predictive of asthma control over time. CONCLUSION: Upon review of the asthma phenotyping literature, common approaches of data clustering emerged. When applying these elements to the Children's Health Study data, latent class analysis clusters-represented by exhaled nitric oxide and spirometry measures-had clinical relevance over time.",
        "Traditional information retrieval systems return a ranked list of results to a user's query. This list is often long, and the user cannot explore all the results retrieved. It is also ineffective for a highly ambiguous language such as Arabic. The modern writing style of Arabic excludes the diacritical marking, without which Arabic words become ambiguous. For a search query, the user has to skim over the document to infer if the word has the same meaning they are after, which is a time-consuming task. It is hoped that clustering the retrieved documents will collate documents into clear and meaningful groups. In this paper, we use an enhanced k-means clustering algorithm, which yields a faster clustering time than the regular k-means. The algorithm uses the distance calculated from previous iterations to minimize the number of distance calculations. We propose a system to cluster Arabic search results using the enhanced k-means algorithm, labeling each cluster with the most frequent word in the cluster. This system will help Arabic web users identify each cluster's topic and go directly to the required cluster. Experimentally, the enhanced k-means algorithm reduced the execution time by 60% for the stemmed dataset and 47% for the non-stemmed dataset when compared to the regular k-means, while slightly improving the purity.",
        "Whole cardiac segmentation in chest CT images is important to identify functional abnormalities that occur in cardiovascular diseases, such as coronary artery disease (CAD) detection. However, manual efforts are time-consuming and labor intensive. Additionally, labeling the ground truth for cardiac segmentation requires the extensive manual annotation of images by the radiologist. Due to the difficulty in obtaining the annotated data and the required expertise as an annotator, an unsupervised approach is proposed. In this paper, we introduce a semantic whole-heart segmentation combining K-Means clustering as a threshold criterion of the mean-thresholding method and mathematical morphology method as a threshold shifting enhancer. The experiment was conducted on 500 subjects in two cases: (1) 56 slices per volume containing full heart scans, and (2) 30 slices per volume containing about half of the top of heart scans before the liver appears. In both cases, the results showed an average silhouette score of the K-Means method of 0.4130. Additionally, the experiment on 56 slices per volume achieved an overall accuracy (OA) and mean intersection over union (mIoU) of 34.90% and 41.26%, respectively, while the performance for the first 30 slices per volume achieved an OA and mIoU of 55.10% and 71.46%, respectively.",
        "Integrating multigenomic data to recognize cancer subtype is an important task in bioinformatics. In recent years, some multiview clustering algorithms have been proposed and applied to identify cancer subtype. However, these clustering algorithms ignore that each data contributes differently to the clustering results during the fusion process, and they require additional clustering steps to generate the final labels. In this paper, a new one-step method for cancer subtype recognition based on graph learning framework is designed, called Laplacian Rank Constrained Multiview Clustering (LRCMC). LRCMC first forms a graph for a single biological data to reveal the relationship between data points and uses affinity matrix to encode the graph structure. Then, it adds weights to measure the contribution of each graph and finally merges these individual graphs into a consensus graph. In addition, LRCMC constructs the adaptive neighbors to adjust the similarity of sample points, and it uses the rank constraint on the Laplacian matrix to ensure that each graph structure has the same connected components. Experiments on several benchmark datasets and The Cancer Genome Atlas (TCGA) datasets have demonstrated the effectiveness of the proposed algorithm comparing to the state-of-the-art methods.",
        "The question of molecular similarity is core in cheminformatics and is usually assessed via a pairwise comparison based on vectors of properties or molecular fingerprints. We recently exploited variational autoencoders to embed 6M molecules in a chemical space, such that their (Euclidean) distance within the latent space so formed could be assessed within the framework of the entire molecular set. However, the standard objective function used did not seek to manipulate the latent space so as to cluster the molecules based on any perceived similarity. Using a set of some 160,000 molecules of biological relevance, we here bring together three modern elements of deep learning to create a novel and disentangled latent space, viz transformers, contrastive learning, and an embedded autoencoder. The effective dimensionality of the latent space was varied such that clear separation of individual types of molecules could be observed within individual dimensions of the latent space. The capacity of the network was such that many dimensions were not populated at all. As before, we assessed the utility of the representation by comparing clozapine with its near neighbors, and we also did the same for various antibiotics related to flucloxacillin. Transformers, especially when as here coupled with contrastive learning, effectively provide one-shot learning and lead to a successful and disentangled representation of molecular latent spaces that at once uses the entire training set in their construction while allowing \"similar\" molecules to cluster together in an effective and interpretable way.",
        "We present a systematic approach for the identification of statistically relevant conformational macrostates of organic molecules from molecular dynamics trajectories. The approach applies to molecules characterized by an arbitrary number of torsional degrees of freedom and enables the transferability of the macrostates definition across different environments. We formulate a dissimilarity measure between molecular configurations that incorporates information on the characteristic energetic cost associated with transitions along all relevant torsional degrees of freedom. Such metric is employed to perform unsupervised clustering of molecular configurations based on the Fast Search and Find of Density Peaks algorithm. We apply this method to investigate the equilibrium conformational ensemble of Sildenafil, a conformationally complex pharmaceutical compound, in different environments including the crystal bulk, the gas phase, and three different solvents (acetonitrile, 1-butanol, and toluene). We demonstrate that while Sildenafil can adopt more than 100 metastable conformational configurations, only 12 are significantly populated across all of the environments investigated. Despite the complexity of the conformational space, we find that the most abundant conformers in solution are the closest to the conformers found in the most common Sildenafil crystal phase.",
        "ABSTRACT: Visual analogue scales are widely used to measure subjective responses. Norris' 16 visual analogue scales (N_VAS) measure subjective feelings of alertness and mood. Up to now, different scientists have clustered items of N_VAS into different ways and Bond and Lader's way has been the most frequently used in clinical research. However, there are concerns about the stability of this clustering over different subject samples and different drug classes. The aim of this study was to test whether Bond and Lader's clustering was stable in terms of subject samples and drug effects. Alternative clustering of N_VAS was tested.Data from studies with 3 types of drugs: cannabinoid receptor agonist (delta-9-tetrahydrocannabinol [THC]), muscarinic antagonist (scopolamine), and benzodiazepines (midazolam and lorazepam), collected between 2005 and 2012, were used for this analysis. Exploratory factor analysis (EFA) was used to test the clustering algorithm of Bond and Lader. Consensus clustering was performed to test the stability of clustering results over samples and over different drug types. Stability analysis was performed using a three-cluster assumption, and then on other alternative assumptions.Heat maps of the consensus matrix (CM) and density plots showed instability of the three-cluster hypothesis and suggested instability over the 3 drug classes. Two- and four-cluster hypothesis were also tested. Heat maps of the CM and density plots suggested that the two-cluster assumption was superior.In summary, the two-cluster assumption leads to a provably stable outcome over samples and the 3 drug types based on the data used.",
        "PURPOSE: Quantitative bone single-photon emission computed tomography (QBSPECT) has the potential to provide a better quantitative assessment of bone metastasis than planar bone scintigraphy due to its ability to better quantify activity in overlapping structures. An important element of assessing the response of bone metastasis is accurate image segmentation. However, limited by the properties of QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs) still relies heavily on the manual delineation by experts. This work proposes a fast and robust automated segmentation method for partitioning a QBSPECT image into lesion, bone, and background. METHODS: We present a new unsupervised segmentation loss function and its semi- and supervised variants for training a convolutional neural network (ConvNet). The loss functions were developed based on the objective function of the classical Fuzzy C-means (FCM) algorithm. The first proposed loss function can be computed within the input image itself without any ground truth labels, and is thus unsupervised; the proposed supervised loss function follows the traditional paradigm of the deep learning-based segmentation methods and leverages ground truth labels during training. The last loss function is a combination of the first and the second and includes a weighting parameter, which enables semi-supervised segmentation using deep learning neural network. EXPERIMENTS AND RESULTS: We conducted a comprehensive study to compare our proposed methods with ConvNets trained using supervised, cross-entropy and Dice loss functions, and conventional clustering methods. The Dice similarity coefficient (DSC) and several other metrics were used as figures of merit as applied to the task of delineating lesion and bone in both simulated and clinical SPECT/CT images. We experimentally demonstrated that the proposed methods yielded good segmentation results on a clinical dataset even though the training was done using realistic simulated images. On simulated SPECT/CT, the proposed unsupervised model's accuracy was greater than the conventional clustering methods while reducing computation time by 200-fold. For the clinical QBSPECT/CT, the proposed semi-supervised ConvNet model, trained using simulated images, produced DSCs of 0.75 and 0.74 for lesion and bone segmentation in SPECT, and a DSC of 0.79 bone segmentation of CT images. These DSCs were larger than that for standard segmentation loss functions by > 0.4 for SPECT segmentation, and > 0.07 for CT segmentation with P-values < 0.001 from a paired t-test. CONCLUSIONS: A ConvNet-based image segmentation method that uses novel loss functions was developed and evaluated. The method can operate in unsupervised, semi-supervised, or fully-supervised modes depending on the availability of annotated training data. The results demonstrated that the proposed method provides fast and robust lesion and bone segmentation for QBSPECT/CT. The method can potentially be applied to other medical image segmentation applications.",
        "RATIONALE: We evaluated K-means clustering to classify amyloid brain PETs as positive or negative. PATIENTS AND METHODS: Sixty-six participants (31 men, 35 women; age range, 52-81 years) were recruited through a multicenter observational study: 19 cognitively normal, 25 mild cognitive impairment, and 22 dementia (11 Alzheimer disease, 3 subcortical vascular cognitive impairment, and 8 Parkinson-Lewy Body spectrum disorder). As part of the neurocognitive and imaging evaluation, each participant had an 18F-flutemetamol (Vizamyl, GE Healthcare) brain PET. All studies were processed using Cortex ID software (General Electric Company, Boston, MA) to calculate SUV ratios in 19 regions of interest and clinically interpreted by 2 dual-certified radiologists/nuclear medicine physicians, using MIM software (MIM Software Inc, Cleveland, OH), blinded to the quantitative analysis, with final interpretation based on consensus. K-means clustering was retrospectively used to classify the studies from the quantitative data. RESULTS: Based on clinical interpretation, 46 brain PETs were negative and 20 were positive for amyloid deposition. Of 19 cognitively normal participants, 1 (5%) had a positive 18F-flutemetamol brain PET. Of 25 participants with mild cognitive impairment, 9 (36%) had a positive 18F-flutemetamol brain PET. Of 22 participants with dementia, 10 (45%) had a positive 18F-flutemetamol brain PET; 7 of 11 participants with Alzheimer disease (64%), 1 of 3 participants with vascular cognitive impairment (33%), and 2 of 8 participants with Parkinson-Lewy Body spectrum disorder (25%) had a positive 18F-flutemetamol brain PET. Using clinical interpretation as the criterion standard, K-means clustering (K = 2) gave sensitivity of 95%, specificity of 98%, and accuracy of 97%. CONCLUSIONS: K-means clustering may be a powerful algorithm for classifying amyloid brain PET.",
        "Multiview subspace clustering has been demonstrated to achieve excellent performance in practice by exploiting multiview complementary information. One of the strategies used in most existing methods is to learn a shared self-expressiveness coefficient matrix for all the view data. Different from such a strategy, this article proposes a rank consistency induced multiview subspace clustering model to pursue a consistent low-rank structure among view-specific self-expressiveness coefficient matrices. To facilitate a practical model, we parameterize the low-rank structure on all self-expressiveness coefficient matrices through the tri-factorization along with orthogonal constraints. This specification ensures that self-expressiveness coefficient matrices of different views have the same rank to effectively promote structural consistency across multiviews. Such a model can learn a consistent subspace structure and fully exploit the complementary information from the view-specific self-expressiveness coefficient matrices, simultaneously. The proposed model is formulated as a nonconvex optimization problem. An efficient optimization algorithm with guaranteed convergence under mild conditions is proposed. Extensive experiments on several benchmark databases demonstrate the advantage of the proposed model over the state-of-the-art multiview clustering approaches.",
        "Fuzzy rule-based models (FRBMs) are sound constructs to describe complex systems. However, in reality, we may encounter situations, where the user or owner of a system only owns either the input or output data of that system (the other part could be owned by another user); and due to the consideration of data privacy, he/she could not obtain all the needed data to build the FRBMs. Since this type of situation has not been fully realized (noticed) and studied before, our objective is to come up with some strategy to address this challenge to meet the specific privacy consideration during the modeling process. In this study, the concept and algorithm of the collaborative fuzzy clustering (CFC) are applied to the identification of FRBMs, describing either multiple-input-single-output (MISO) or multiple-input-multiple-output (MIMO) systems. The collaboration between input and output spaces based on their structural information (conveyed in terms of the corresponding partition matrices) makes it possible to build FRBMs when input and output data could not be collected and used in unison. Surprisingly, on top of this primary pursuit, with the collaboration mechanism the input and output spaces of a system are endowed with an innovative way to comprehensively share, exchange, and utilize the structural information between each other, which results in their more relevant structures that guarantee better model performance compared with performance produced by some state-of-the-art modeling strategies. The effectiveness of the proposed approach is demonstrated by experiments on a series of synthetic and publicly available datasets.",
        "Spike sorting technologies support neuroscientists to access the neural activity with single-neuron or single-action-potential resolutions. However, conventional spike sorting technologies perform the feature extraction and the clustering separately after the spikes are well detected. It not only induces many redundant processes, but it also yields a lower accuracy and an unstable result especially when noises and/or overlapping spikes exist in the dataset. To address these issues, this paper proposes a unified optimization model integrating the feature extraction and the clustering for spike sorting. Unlike the widely used combination strategies, i.e., performing the principal component analysis (PCA) for spike feature extraction and the K-means (KM) for clustering in sequence, interestingly, this paper finds the solution of the proposed unified model by iteratively performing PCA and KM-like procedures. Subsequently, by embedding the K-means++ strategy in KM-like initializing and a comparison updating rule in the solving process, the proposed model can well handle the noises and overlapping interference as well as enjoy a high accuracy and a low computational complexity. Finally, an automatic spike sorting method is derived after taking the best of the clustering validity indices into the proposed model. The extensive numerical simulation results on both synthetic and real-world datasets confirm that our proposed method outperforms the related state-of-the-art approaches.",
        "With the abundance of data, machine learning applications engaged increased attention in the last decade. An attractive approach to robustify the statistical analysis is to preprocess the data through clustering. This paper develops a low-complexity Riemannian optimization framework for solving optimization problems on the set of positive semidefinite stochastic matrices. The low-complexity feature of the proposed algorithms stems from the factorization of the optimization variable X=YY(T) and deriving conditions on the number of columns of Y under which the factorization yields a satisfactory solution. The paper further investigates the embedded and quotient geometries of the resulting Riemannian manifolds. In particular, the paper explicitly derives the tangent space, Riemannian gradients and Hessians, and a retraction operator allowing the design of efficient first and second-order optimization methods for the graph-based clustering applications of interest. The numerical results reveal that the resulting algorithms present a clear complexity advantage as compared with state-of-the-art Euclidean and Riemannian approaches for graph clustering applications.",
        "Recently, single-stage embedding based deep learning algorithms gain increasing attention in cell segmentation and tracking. Compared with the traditional \"segment-then-associate\" two-stage approach, a single-stage algorithm not only simultaneously achieves consistent instance cell segmentation and tracking but also gains superior performance when distinguishing ambiguous pixels on boundaries and overlaps. However, the deployment of an embedding based algorithm is restricted by slow inference speed (e.g., approximately 1-2 min per frame). In this study, we propose a novel Faster Mean-shift algorithm, which tackles the computational bottleneck of embedding based cell segmentation and tracking. Different from previous GPU-accelerated fast mean-shift algorithms, a new online seed optimization policy (OSOP) is introduced to adaptively determine the minimal number of seeds, accelerate computation, and save GPU memory. With both embedding simulation and empirical validation via the four cohorts from the ISBI cell tracking challenge, the proposed Faster Mean-shift algorithm achieved 7-10 times speedup compared to the state-of-the-art embedding based cell instance segmentation and tracking algorithm. Our Faster Mean-shift algorithm also achieved the highest computational speed compared to other GPU benchmarks with optimized memory consumption. The Faster Mean-shift is a plug-and-play model, which can be employed on other pixel embedding based clustering inference for medical image analysis. (Plug-and-play model is publicly available: https://github.com/masqm/Faster-Mean-Shift).",
        "Remote sensing image data clustering is a tough task, which involves classifying the image without any prior information. Remote sensing image clustering, in essence, belongs to a complex optimization problem, due to the high dimensionality and complexity of remote sensing imagery. Therefore, it can be easily affected by the initial values and trapped in locally optimal solutions. Meanwhile, remote sensing images contain complex and diverse spatial-spectral information, which makes them difficult to model with only a single objective function. Although evolutionary multiobjective optimization methods have been presented for the clustering task, the tradeoff between the global and local search abilities is not well adjusted in the evolutionary process. In this article, in order to address these problems, a multiobjective sine cosine algorithm for remote sensing image data spatial-spectral clustering (MOSCA_SSC) is proposed. In the proposed method, the clustering task is converted into a multiobjective optimization problem, and the Xie-Beni (XB) index and Jeffries-Matusita (Jm) distance combined with the spatial information term (SI_Jm measure) are utilized as the objective functions. In addition, for the first time, the sine cosine algorithm (SCA), which can effectively adjust the local and global search capabilities, is introduced into the framework of multiobjective clustering for continuous optimization. Furthermore, the destination solution in the SCA is automatically selected and updated from the current Pareto front through employing the knee-point-based selection approach. The benefits of the proposed method were demonstrated by clustering experiments with ten UCI datasets and four real remote sensing image datasets.",
        "INTRODUCTION: Clustering analyses in clinical contexts hold promise to improve the understanding of patient phenotype and disease course in chronic and acute clinical medicine. However, work remains to ensure that solutions are rigorous, valid, and reproducible. In this paper, we evaluate best practices for dissimilarity matrix calculation and clustering on mixed-type, clinical data. METHODS: We simulate clinical data to represent problems in clinical trials, cohort studies, and EHR data, including single-type datasets (binary, continuous, categorical) and 4 data mixtures. We test 5 single distance metrics (Jaccard, Hamming, Gower, Manhattan, Euclidean) and 3 mixed distance metrics (DAISY, Supersom, and Mercator) with 3 clustering algorithms (hierarchical (HC), k-medoids, self-organizing maps (SOM)). We quantitatively and visually validate by Adjusted Rand Index (ARI) and silhouette width (SW). We applied our best methods to two real-world data sets: (1) 21 features collected on 247 patients with chronic lymphocytic leukemia, and (2) 40 features collected on 6000 patients admitted to an intensive care unit. RESULTS: HC outperformed k-medoids and SOM by ARI across data types. DAISY produced the highest mean ARI for mixed data types for all mixtures except unbalanced mixtures dominated by continuous data. Compared to other methods, DAISY with HC uncovered superior, separable clusters in both real-world data sets. DISCUSSION: Selecting an appropriate mixed-type metric allows the investigator to obtain optimal separation of patient clusters and get maximum use of their data. Superior metrics for mixed-type data handle multiple data types using multiple, type-focused distances. Better subclassification of disease opens avenues for targeted treatments, precision medicine, clinical decision support, and improved patient outcomes.",
        "PURPOSE: AI algorithms have shown promise in medical image analysis. Previous studies of ASD clusters have analyzed alignment metrics-this study sought to complement these efforts by analyzing images of sagittal anatomical spinopelvic landmarks. We hypothesized that an AI algorithm would cluster preoperative lateral radiographs into groups with distinct morphology. METHODS: This was a retrospective review of a multicenter, prospectively collected database of adult spinal deformity. A total of 915 patients with adult spinal deformity and preoperative lateral radiographs were included. A 2 x 3, self-organizing map-a form of artificial neural network frequently employed in unsupervised classification tasks-was developed. The mean spine shape was plotted for each of the six clusters. Alignment, surgical characteristics, and outcomes were compared. RESULTS: Qualitatively, clusters C and D exhibited only mild sagittal plane deformity. Clusters B, E, and F, however, exhibited marked positive sagittal balance and loss of lumbar lordosis. Cluster A had mixed characteristics, likely representing compensated deformity. Patients in clusters B, E, and F disproportionately underwent 3-CO. PJK and PJF were particularly prevalent among clusters A and E. Among clusters B and F, patients who experienced PJK had significantly greater positive sagittal balance than those who did not. CONCLUSIONS: This study clustered preoperative lateral radiographs of ASD patients into groups with highly distinct overall spinal morphology and association with sagittal alignment parameters, baseline HRQOL, and surgical characteristics. The relationship between SVA and PJK differed by cluster. This study represents significant progress toward incorporation of computer vision into clinically relevant classification systems in adult spinal deformity. LEVEL OF EVIDENCE IV: Diagnostic: individual cross-sectional studies with the consistently applied reference standard and blinding.",
        "Although clinical and laboratory data have long been used to guide medical practice, this information is rarely integrated with multi-omic data to identify endotypes. We present Merged Affinity Network Association Clustering (MANAclust), a coding-free, automated pipeline enabling integration of categorical and numeric data spanning clinical and multi-omic profiles for unsupervised clustering to identify disease subsets. Using simulations and real-world data from The Cancer Genome Atlas, we demonstrate that MANAclust's feature selection algorithms are accurate and outperform competitors. We also apply MANAclust to a clinically and multi-omically phenotyped asthma cohort. MANAclust identifies clinically and molecularly distinct clusters, including heterogeneous groups of \"healthy controls\" and viral and allergy-driven subsets of asthmatic subjects. We also find that subjects with similar clinical presentations have disparate molecular profiles, highlighting the need for additional testing to uncover asthma endotypes. This work facilitates data-driven personalized medicine through integration of clinical parameters with multi-omics. MANAclust is freely available at https://bitbucket.org/scottyler892/manaclust/src/master/.",
        "Background: The brain magnetic resonance imaging (MRI) image segmentation method mainly refers to the division of brain tissue, which can be divided into tissue parts such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). The segmentation results can provide a basis for medical image registration, 3D reconstruction, and visualization. Generally, MRI images have defects such as partial volume effects, uneven grayscale, and noise. Therefore, in practical applications, the segmentation of brain MRI images has difficulty obtaining high accuracy. Materials and Methods: The fuzzy clustering algorithm establishes the expression of the uncertainty of the sample category and can describe the ambiguity brought by the partial volume effect to the brain MRI image, so it is very suitable for brain MRI image segmentation (B-MRI-IS). The classic fuzzy c-means (FCM) algorithm is extremely sensitive to noise and offset fields. If the algorithm is used directly to segment the brain MRI image, the ideal segmentation result cannot be obtained. Accordingly, considering the defects of MRI medical images, this study uses an improved multiview FCM clustering algorithm (IMV-FCM) to improve the algorithm's segmentation accuracy of brain images. IMV-FCM uses a view weight adaptive learning mechanism so that each view obtains the optimal weight according to its cluster contribution. The final division result is obtained through the view ensemble method. Under the view weight adaptive learning mechanism, the coordination between various views is more flexible, and each view can be adaptively learned to achieve better clustering effects. Results: The segmentation results of a large number of brain MRI images show that IMV-FCM has better segmentation performance and can accurately segment brain tissue. Compared with several related clustering algorithms, the IMV-FCM algorithm has better adaptability and better clustering performance.",
        "Taking the assumption that data samples are able to be reconstructed with the dictionary formed by themselves, recent multiview subspace clustering (MSC) algorithms aim to find a consensus reconstruction matrix via exploring complementary information across multiple views. Most of them directly operate on the original data observations without preprocessing, while others operate on the corresponding kernel matrices. However, they both ignore that the collected features may be designed arbitrarily and hard guaranteed to be independent and nonoverlapping. As a result, original data observations and kernel matrices would contain a large number of redundant details. To address this issue, we propose an MSC algorithm that groups samples and removes data redundancy concurrently. In specific, eigendecomposition is employed to obtain the robust data representation of low redundancy for later clustering. By utilizing the two processes into a unified model, clustering results will guide eigendecomposition to generate more discriminative data representation, which, as feedback, helps obtain better clustering results. In addition, an alternate and convergent algorithm is designed to solve the optimization problem. Extensive experiments are conducted on eight benchmarks, and the proposed algorithm outperforms comparative ones in recent literature by a large margin, verifying its superiority. At the same time, its effectiveness, computational efficiency, and robustness to noise are validated experimentally.",
        "Environmental DNA metabarcoding is a powerful tool for studying biodiversity. However, bioinformatic approaches need to adjust to the diversity of taxonomic compartments targeted as well as to each barcode gene specificities. We built and tested a pipeline based on read correction with DADA2 allowing analysing metabarcoding data from prokaryotic (16S) and eukaryotic (18S, COI) life compartments. We implemented the option to cluster amplicon sequence variants (ASVs) into operational taxonomic units (OTUs) with swarm, a network-based clustering algorithm, and the option to curate ASVs/OTUs using LULU. Finally, taxonomic assignment was implemented via the Ribosomal Database Project Bayesian classifier (RDP) and BLAST. We validated this pipeline with ribosomal and mitochondrial markers using metazoan mock communities and 42 deep-sea sediment samples. The results show that ASVs and OTUs describe different levels of biotic diversity, the choice of which depends on the research questions. They underline the advantages and complementarity of clustering and LULU-curation for producing metazoan biodiversity inventories at a level approaching the one obtained using morphological criteria. While clustering removes intraspecific variation, LULU effectively removes spurious clusters, originating from errors or intragenomic variability. Swarm clustering affected alpha and beta diversity differently depending on genetic marker. Specifically, d-values > 1 appeared to be less appropriate with 18S for metazoans. Similarly, increasing LULU's minimum ratio level proved essential to avoid losing species in sample-poor data sets. Comparing BLAST and RDP underlined that accurate assignments of deep-sea species can be obtained with RDP, but highlighted the need for a concerted effort to build comprehensive, ecosystem-specific databases.",
        "We introduce a novel adaptive version of the Neighborhood Retrieval Visualizer (NeRV). We maintain the advantages of the conventional NeRV method, while proposing an improvement of the data samples' neighborhood width calculation, in the input and output data space. In the standard NeRV, the data samples' neighborhood widths are determined in an arbitrary manner, in this way, inhibiting the possible quality of the resulting data visualization. We propose to compute the widths adaptively, on the basis of the input data scattering. Therefore, we first perform the preliminary input data clustering, next, we calculate the values of the inner-cluster variances, which convey the information on the input data scattering, then, we assign them to each data sample, and finally, we use them as the basis for the data samples' neighborhood widths determination. The results of the experiments conducted on the three different real datasets confirm the effectiveness and usefulness of the proposed approach.",
        "Trajectory classification has become frequent in clinical research to understand the heterogeneity of individual trajectories. The standard classification model for trajectories assumes no between-individual variance within groups. However, this assumption is often not appropriate, which may overestimate the error variance of the model, leading to a biased classification. Hence, two extensions of the standard classification model were developed through a mixed model. The first one considers an equal between-individual variance across groups, and the second one considers unequal between-individual variance. Simulations were performed to evaluate the impact of these considerations on the classification. The simulation results showed that the first extended model gives a lower misclassification percentage (with differences up to 50%) than the standard one in case of presence of a true variance between individuals inside groups. The second model decreases the misclassification percentage compared with the first one (up to 11%) when the between-individual variance is unequal between groups. However, these two extensions require high number of repeated measurements to be adjusted correctly. Using human chorionic gonadotropin trajectories after curettage for hydatidiform mole, the standard classification model classified trajectories mainly according to their levels whereas the two extended models classified them according to their patterns, which provided more clinically relevant groups. In conclusion, for studies with a nonnegligible number of repeated measurements, the use, in first instance, of a classification model that considers equal between-individual variance across groups rather than a standard classification model, appears more appropriate. A model that considers unequal between-individual variance may find its place thereafter.",
        "In recent years, single-cell RNA sequencing (scRNA-seq) technologies have been widely adopted to interrogate gene expression of individual cells; it brings opportunities to understand the underlying processes in a high-throughput manner. Deep embedded clustering (DEC) was demonstrated successful in high-dimensional sparse scRNA-seq data by joint feature learning and cluster assignment for identifying cell types simultaneously. However, the deep network architecture for embedding clustering is not trivial to optimize. Therefore, we propose an evolutionary multiobjective DEC by synergizing the multiobjective evolutionary optimization to simultaneously evolve the hyperparameters and architectures of DEC in an automatic manner. Firstly, a denoising autoencoder is integrated into the DEC to project the high-dimensional sparse scRNA-seq data into a low-dimensional space. After that, to guide the evolution, three objective functions are formulated to balance the model's generality and clustering performance for robustness. Meanwhile, migration and mutation operators are proposed to optimize the objective functions to select the suitable hyperparameters and architectures of DEC in the multiobjective framework. Multiple comparison analyses are conducted on twenty synthetic data and eight real data from different representative single-cell sequencing platforms to validate the effectiveness. The experimental results reveal that the proposed algorithm outperforms other state-of-the-art clustering methods under different metrics. Meanwhile, marker genes identification, gene ontology enrichment and pathology analysis are conducted to reveal novel insights into the cell type identification and characterization mechanisms.",
        "Effective ultrasound (US) analysis for preliminary breast tumor diagnosis is constrained due to the presence of complex echogenic patterns. Implementing pretrained models of convolutional neural networks (CNNs) which mostly focuses on natural images and using transfer learning seldom gives good results in medical domain. In this work, a CNN architecture, StepNet, with step-wise incremental convolution layers for each downsampled block was developed for classification of breast tumors as benign/malignant. To increase noise robustness and as an improvement over existing methodologies, neutrosophic preprocessing was performed, and the enhanced images were appended to the original image during training and data augmentation. The final layers' activation maps are clustered using fuzzy c-means clustering which qualify as a validation method for the prediction of StepNet. Using neutrosophic preprocessing alone had increased the validation accuracy from 0.84 to 0.93, while using neutrosophic preprocessing and augmentation had increased the accuracy to 0.98. StepNet has comparably less training and validation time than other state of the art architectures and methods and shows an increase in prediction accuracy even for challenging isoechoic and hypoechoic tumors.",
        "Clustering and cell type classification are important steps in single-cell RNA-seq (scRNA-seq) analysis. As more and more scRNA-seq data are becoming available, supervised cell type classification methods that utilize external well-annotated source data start to gain popularity over unsupervised clustering algorithms. However, the performance of existing supervised methods is highly dependent on source data quality, and they often have limited accuracy to classify cell types that are missing in the source data. To overcome these limitations, we developed ItClust, a transfer learning algorithm that borrows idea from supervised cell type classification algorithms, but also leverages information in target data to ensure sensitivity in classifying cells that are only present in the target data. Through extensive evaluations using data from different species and tissues generated with diverse scRNA-seq protocols, we show that ItClust significantly improves clustering and cell type classification accuracy over popular unsupervised clustering and supervised cell type classification algorithms.",
        "The Capacitated Centered Clustering Problem (CCCP)-a multi-facility location model-is very important within the logistics and supply chain management fields due to its impact on industrial transportation and distribution. However, solving the CCCP is a challenging task due to its computational complexity. In this work, a strategy based on Gaussian mixture models (GMMs) and dispersion reduction is presented to obtain the most likely locations of facilities for sets of client points considering their distribution patterns. Experiments performed on large CCCP instances, and considering updated best-known solutions, led to estimate the performance of the GMMs approach, termed as Dispersion Reduction GMMs, with a mean error gap smaller than 2.6%. This result is more competitive when compared to Variable Neighborhood Search, Simulated Annealing, Genetic Algorithm and CKMeans and faster to achieve when compared to the best-known solutions obtained by Tabu-Search and Clustering Search.",
        "The categorical clustering problem has attracted much attention especially in the last decades since many real world applications produce categorical data. The k-mode algorithm, proposed since 1998, and its multiple variants were widely used in this context. However, they suffer from a great limitation related to the update of the modes in each iteration. The mode in the last step of these algorithms is randomly selected although it is possible to identify many candidate ones. In this paper, a rough density mode selection method is proposed to identify the adequate modes among a list of candidate ones in each iteration of the k-modes. The proposed method, called Density Rough k-Modes (DRk-M) was experimented using real world datasets extracted from the UCI Machine Learning Repository, the Global Terrorism Database (GTD) and a set of collected Tweets. The DRk-M was also compared to many states of the art clustering methods and has shown great efficiency.",
        "Online diagnosis for sucker rod pumping well has great significances for rapidly grasping operations of the oil well. Feature extraction of the working condition and determination of the online diagnostic algorithm are two indispensable parts. In this paper, five feature vectors are extracted using Freeman chain codes. Then, an optimized density peak clustering (DPC) method is proposed to realize online diagnosis solved by an improved brain storm optimization (BSO) algorithm, in which the cloud model is adopted to generate new solutions in the searching space. During the online diagnosis process, a new cluster updating strategy is used to update the cluster centers online. According to the proposed online diagnostic method, various samples are automatically classified into different classifications by the unsupervised learning. The simulation results verify that the proposed online diagnosis method is satisfactory, which can give a higher and more stable diagnostic accuracy.",
        "BACKGROUND: Health-related behaviors during adolescence could influence adolescents' health outcomes, leading to either advantageous or deteriorative conditions. Clustering of adolescents' health-related behaviors by gender identifies the target groups for intervention and informs the strategies to be implemented for behavioral changes. METHODS: Data from 1807 adolescents in grades 7 and 10 in a city in South Korea were used. Health-related behaviors including eating habits, physical activity, hand washing, brushing teeth, drinking alcohol, smoking, and Internet use were examined. Latent class analysis (LCA) was used to identify subgroups of adolescents with regard to their health-related behaviors. RESULTS: A four-class model was the most adequate grouping classification across genders: adolescents with (1) healthy behaviors, (2) neither health-promoting nor health-risk behaviors, (3) good hygiene behaviors, and (4) unhealthy behaviors. The majority of both male and female adolescents were classified into the healthy group. Male adolescents belonging to the healthy group were more likely to engage in vigorous physical activities, while vigorous physical activity was not important for female adolescents. The smallest group was the unhealthy group, regardless of gender; however, the proportion of boys in the unhealthy group was almost twice that of girls. Only female adolescents engaged in excessive Internet use, especially the group with neither health-promoting nor health-risk behaviors. CONCLUSION: To improve adolescents' health-related behaviors, it would be more effective to develop tailored interventions considering the behavioral profiles of the target groups.",
        "In the context of smart cities, there is a general benefit from monitoring close encounters among pedestrians. For instance, for the access control to office buildings, subway, commercial malls, etc., where a high amount of users may be present simultaneously, and keeping a strict record on each individual may be challenging. GPS tracking may not be available in many indoor cases; video surveillance may require expensive deployment (mainly due to the high-quality cameras and face recognition algorithms) and can be restrictive in case of low budget applications; RFID systems can be cumbersome and limited in the detection range. This information can later be used in many different scenarios. For instance, in case of earthquakes, fires, and accidents in general, the administration of the buildings can have a clear record of the people inside for victim searching activities. However, in the pandemic derived from the COVID-19 outbreak, a tracking that allows detecting of pedestrians in close range (a few meters) can be particularly useful to control the virus propagation. Hence, we propose a mobile clustering scheme where only a selected number of pedestrians (Cluster Heads) collect the information of the people around them (Cluster Members) in their trajectory inside the area of interest. Hence, a small number of transmissions are made to a control post, effectively limiting the collision probability and increasing the successful registration of people in close contact. Our proposal shows an increased success packet transmission probability and a reduced collision and idle slot probability, effectively improving the performance of the system compared to the case of direct transmissions from each node.",
        "Direction of arrival (DOA) estimation has always been a hot topic for researchers. The complex and changeable environment makes it very challenging to estimate the DOA in a small snapshot and strong noise environment. The direction-of-arrival estimation method based on compressed sensing (CS) is a new method proposed in recent years. It has received widespread attention because it can realize the direction-of-arrival estimation under small snapshots. However, this method will cause serious distortion in a strong noise environment. To solve this problem, this paper proposes a DOA estimation algorithm based on the principle of CS and density-based spatial clustering (DBSCAN). First of all, in order to make the estimation accuracy higher, this paper selects a signal reconstruction strategy based on the basis pursuit de-noising (BPDN). In response to the challenge of the selection of regularization parameters in this strategy, the power spectrum entropy is proposed to characterize the noise intensity of the signal, so as to provide reasonable suggestions for the selection of regularization parameters; Then, this paper finds out that the DOA estimation based on the principle of CS will get a denser estimation near the real angle under the condition of small snapshots through analysis, so it is proposed to use a DBSCAN method to process the above data to obtain the final DOA estimate; Finally, calculate the cluster center value of each cluster, the number of clusters is the number of signal sources, and the cluster center value is the final DOA estimate. The proposed method is applied to the simulation experiment and the micro electro mechanical system (MEMS) vector hydrophone lake test experiment, and they are proved that the proposed method can obtain good results of DOA estimation under the conditions of small snapshots and low signal-to-noise ratio (SNR).",
        "Kernel fuzzy c-means (KFCM) is a significantly improved version of fuzzy c-means (FCM) for processing linearly inseparable datasets. However, for fuzzification parameter m=1, the problem of KFCM (kernel fuzzy c-means) cannot be solved by Lagrangian optimization. To solve this problem, an equivalent model, called kernel probabilistic k-means (KPKM), is proposed here. The novel model relates KFCM to kernel k-means (KKM) in a unified mathematic framework. Moreover, the proposed KPKM can be addressed by the active gradient projection (AGP) method, which is a nonlinear programming technique with constraints of linear equalities and linear inequalities. To accelerate the AGP method, a fast AGP (FAGP) algorithm was designed. The proposed FAGP uses a maximum-step strategy to estimate the step length, and uses an iterative method to update the projection matrix. Experiments demonstrated the effectiveness of the proposed method through a performance comparison of KPKM with KFCM, KKM, FCM and k-means. Experiments showed that the proposed KPKM is able to find nonlinearly separable structures in synthetic datasets. Ten real UCI datasets were used in this study, and KPKM had better clustering performance on at least six datsets. The proposed fast AGP requires less running time than the original AGP, and it reduced running time by 76-95% on real datasets.",
        "The purposes are to monitor the nitrogen utilization efficiency of crops and intelligently evaluate the absorption of nutrients by crops during the production process. The research object is Chinese cabbage. The Chinese cabbage population with different agricultural parameters is constructed through different densities and nitrogen fertilizer application rates based on digital image processing technology, and an estimation NC (Nitrogen Content) model is established. The population is classified through the K-Means Clustering algorithm using the feature extraction method, and the Chinese cabbage population quality BPNN (Backpropagation Neural Network) model is constructed. The nonlinear mapping relationship between different agricultural parameters and population quality, and the contribution rate of each indicator, are studied. The nitrogen utilization of Chinese cabbage is monitored effectively. Results demonstrate that the proposed NC estimation model has correlation coefficients above 0.70 in different growth stages. This model can accurately estimate the NC of the Chinese cabbage population. The results of the Chinese cabbage population quality BPNN model show that the population planting density based on the seedling number is reasonable. The constructed population quality evaluation model has a high R2 value and a comparatively low RMSE (Root Mean Square Error) value for the quality evaluation of Chinese cabbage in different periods, showing that it applies to evaluate the population quality of Chinese cabbage in different growth stages. The constructed nitrogen utilization model and quality evaluation model can monitor the nutrient utilization of crops in different growth stages, ascertain the agricultural characteristics of other yield groups in different growth stages, and clarify the performance of agricultural parameters in different growth stages. The above results can provide some ideas for crop growth intelligent detection.",
        "Unsupervised clustering is a fundamental step of single-cell RNA sequencing data analysis. This issue has inspired several clustering methods to classify cells in single-cell RNA sequencing data. However, accurate prediction of the cell clusters remains a substantial challenge. In this study, we propose a new algorithm for single-cell RNA sequencing data clustering based on Sparse Optimization and low-rank matrix factorization (scSO). We applied our scSO algorithm to analyze multiple benchmark datasets and showed that the cluster number predicted by scSO was close to the number of reference cell types and that most cells were correctly classified. Our scSO algorithm is available at https://github.com/QuKunLab/scSO. Overall, this study demonstrates a potent cell clustering approach that can help researchers distinguish cell types in single-cell RNA sequencing data.",
        "Italy has experienced the epidemic of Severe Acute Respiratory Syndrome Coronavirus 2, which spread at different times and with different intensities throughout its territory. We aimed to identify clusters with similar epidemic patterns across Italian regions. To do that, we defined a set of regional indicators reflecting different domains and employed a hierarchical clustering on principal component approach to obtain an optimal cluster solution. As of 24 April 2020, Lombardy was the worst hit Italian region and entirely separated from all the others. Sensitivity analysis-by excluding data from Lombardy-partitioned the remaining regions into four clusters. Although cluster 1 (i.e. Veneto) and 2 (i.e. Piedmont and Emilia-Romagna) included the most hit regions beyond Lombardy, this partition reflected differences in the efficacy of restrictions and testing strategies. Cluster 3 was heterogeneous and comprised regions where the epidemic started later and/or where it spread with the lowest intensity. Regions within cluster 4 were those where the epidemic started slightly after Veneto, Emilia-Romagna and Piedmont, favoring timely adoption of control measures. Our findings provide policymakers with a snapshot of the epidemic in Italy, which might help guiding the adoption of countermeasures in accordance with the situation at regional level.",
        "While accurately modeling the conformational ensemble is required for predicting properties of flexible molecules, the optimal method of obtaining the conformational ensemble appears as varied as their applications. Ensemble structures have been modeled by generation, refinement, and clustering of conformations with a sufficient number of samples. We present a conformational clustering algorithm intended to automate the conformational clustering step through the Louvain algorithm, which requires minimal hyperparameters and importantly no predefined number of clusters or threshold values. The conformational graphs produced by this method for O-succinyl-l-homoserine, oxidized nicotinamide adenine dinucleotide, and 200 representative metabolites each preserved the geometric/energetic correlation expected for points on the potential energy surface. Clustering based on these graphs provides partitions informed by the potential energy surface. Automating conformational clustering in a workflow with AutoGraph may mitigate human biases introduced by guess and check over hyperparameter selection while allowing flexibility to the result by not imposing predefined criteria other than optimizing the model's loss function. Associated codes are available at https://github.com/TanemuraKiyoto/AutoGraph.",
        "PURPOSE: This technical note seeks to act as a practical guide for implementing a supervised clustering algorithm (SVCA) reference region approach and to explain the main strengths and limitations of the technique in the context of 18-kilodalton translocator protein (TSPO) positron emission tomography (PET) studies in experimental medicine. BACKGROUND: TSPO PET is the most widely used imaging technique for studying neuroinflammation in vivo in humans. Quantifying neuroinflammation with PET can be a challenging and invasive procedure, especially in frail patients, because it often requires blood sampling from an arterial catheter. A widely used alternative to arterial sampling is SVCA, which identifies the voxels with minimal specific binding in the PET images, thus extracting a pseudo-reference region for non-invasive quantification. Unlike other reference region approaches, SVCA does not require specification of an anatomical reference region a priori, which alleviates the limitation of TSPO contamination in anatomically-defined reference regions in individuals with underlying inflammatory processes. Furthermore, SVCA can be applied to any TSPO PET tracer across different neurological and neuropsychiatric conditions, providing noninvasivequantification of TSPO expression. METHODS: We provide an overview of the development of SVCA as well as step-by-step instructions for implementing SVCA with suggestions for specific settings. We review the literature on SVCAapplications using first- and second- generation TSPO PET tracers and discuss potential clinically relevant limitations and applications. CONCLUSIONS: The correct implementation of SVCA can provide robust and reproducible estimates of brain TSPO expression. This review encourages the standardisation of SVCA methodology in TSPO PET analysis, ultimately aiming to improve replicability and comparability across study sites.",
        "By utilizing the complementary information from multiple views, multi-view clustering (MVC) algorithms typically achieve much better clustering performance than conventional single-view methods. Although in this field, great progresses have been made in past few years, most existing multi-view clustering methods still suffer the following shortcomings: (1) most MVC methods are non-convex and thus are easily stuck into suboptimal local minima; (2) the effectiveness of these methods is sensitive to the existence of noises or outliers; and (3) the qualities of different features and views are usually ignored, which can also influence the clustering result. To address these issues, we propose dual self-paced multi-view clustering (DSMVC) in this paper. Specifically, DSMVC takes advantage of self-paced learning to tackle the non-convex issue. By applying a soft-weighting scheme of self-paced learning for instances, the negative impact caused by noises and outliers can be significantly reduced. Moreover, to alleviate the feature and view quality issues, we develop a novel feature selection approach in a self-paced manner and a weighting term for views. Experimental results on real-world data sets demonstrate the effectiveness of the proposed method.",
        "The mechanism of message passing in graph neural networks (GNNs) is still mysterious. Apart from convolutional neural networks, no theoretical origin for GNNs has been proposed. To our surprise, message passing can be best understood in terms of power iteration. By fully or partly removing activation functions and layer weights of GNNs, we propose subspace power iteration clustering (SPIC) models that iteratively learn with only one aggregator. Experiments show that our models extend GNNs and enhance their capability to process random featured networks. Moreover, we demonstrate the redundancy of some state-of-the-art GNNs in design and define a lower limit for model evaluation by a random aggregator of message passing. Our findings push the boundaries of the theoretical understanding of neural networks.",
        "Identification of groups of co-expressed or co-regulated genes is critical for exploring the underlying mechanism behind a particular disease like cancer. Condition-specific (disease-specific) gene-expression profiles acquired from different platforms are widely utilized by researchers to get insight into the regulatory mechanism of the disease. Several clustering algorithms are developed using gene expression profiles to identify the group of similar genes. These algorithms are computationally efficient but are not able to capture the functional similarity present between the genes, which is very important from a biological perspective. In this study, an algorithm named CorGO is introduced, that specifically deals with the identification of functionally similar gene-clusters. Two types of relationships are calculated for this purpose. Firstly, the Correlation (Cor) between the genes are captured from the gene-expression data, which helps in deciphering the relationship between genes based on its expression across several diseased samples. Secondly, Gene Ontology (GO)-based semantic similarity information available for the genes is utilized, that helps in adding up biological relevance to the identified gene-clusters. A similarity measure is defined by integrating these two components that help in the identification of homogeneous and functionally similar groups of genes. CorGO is applied to four different types of gene expression profiles of different types of cancer. Gene-clusters identified by CorGO, are further validated by pathway enrichment, disease enrichment, and network analysis. These biological analyses demonstrated significant connectivity and functional relatedness within the genes of the same cluster. A comparative study with commonly used clustering algorithms is also performed to show the efficacy of the proposed method.",
        "Breast cancer is one of the leading causes of mortality in the world and it occurs in high frequency among women that carries away many lives. To detect cancer, extraction or segmentation of lesions/tumors is required. Segmentation process is very crucial if the mammogram images are blurred or low contrast. This paper suggests a novel clustering approach for segmenting lesions/tumors in the mammogram images using Atanassov's intuitionistic fuzzy set theory. The algorithm initially converts an image to an intuitionistic fuzzy image using a novel intuitionistic fuzzy generator. From the intuitionistic fuzzy image, two membership intervals are computed. Then, using Zadeh's min t-conorm, a new membership function is computed. Using the new membership function, an interval type 2 fuzzy image is constructed. Two types of distance functions are used in clustering-intuitionistic fuzzy divergence and a fuzzy exponential type distance function. Further, in each iteration, membership matrix is updated using a hesitation degree and a clustered image is obtained. Tumors/lesions are then segmented from the clustered image. The proposed method is compared with existing methods both quantitatively and qualitatively and it is observed that the proposed method performs better than the existing methods.",
        "Fuzzy C-means clustering algorithm is one of the typical clustering algorithms in data mining applications. However, due to the sensitive information in the dataset, there is a risk of user privacy being leaked during the clustering process. The fuzzy C-means clustering of differential privacy protection can protect the user's individual privacy while mining data rules, however, the decline in availability caused by data disturbances is a common problem of these algorithms. Aiming at the problem that the algorithm accuracy is reduced by randomly initializing the membership matrix of fuzzy C-means, in this paper, the maximum distance method is firstly used to determine the initial center point. Then, the gaussian value of the cluster center point is used to calculate the privacy budget allocation ratio. Additionally, Laplace noise is added to complete differential privacy protection. The experimental results demonstrate that the clustering accuracy and effectiveness of the proposed algorithm are higher than baselines under the same privacy protection intensity.",
        "Since the outbreak of the novel coronavirus, COVID-19 has continuously spread across the globe briskly. However, since its existence, the symptoms of the disease have been varying widely; thus, developing an urgent need to stratify high-risk categories of people who show more propensity to be affected by this deadly virus will be beneficial for health care. Using the open-access data and machine learning algorithms, this paper aims to cluster countries in groups with similar profiles with respect to the country level pre COVID-19 pandemic parameters. The purpose of performing the data analysis is to measure the extent to which these major risk factors determine the mortality rate due to the coronavirus disease 2019. An unsupervised machine learning model (k-means) was employed for two hundred and eight countries to define data-driven clusters based on thirteen country-level parameters. After performing the one-way ANOVA for comparing the clusters in terms of total cases, total deaths, total cases per population, total deaths per population, and death rate, the paradigm with four and seven clusters showed the best ability to stratify the countries according to total cases per population and death rate with p-values of less than 0.05 and 0.001, respectively. However, the model could not stratify countries in total deaths/cases and total deaths per population.",
        "BACKGROUND: Pericoronary adipose tissue attenuation expressed by fat attenuation index (FAI) on coronary CT angiography (CCTA) reflects pericoronary inflammation and is associated with cardiac mortality. OBJECTIVE: The aim of this study was to define the sub-phenotypes of coronary CCTA-defined plaque and whole vessel quantification by unsupervised machine learning (ML) and its prognostic impact when combined with pericoronary inflammation. METHODS: A total of 220 left anterior descending arteries (LAD) with intermediate stenosis who underwent fractional flow reserve (FFR) measurement and CCTA were studied. After removal of outcome and FAI data, the phenotype heterogeneity of CCTA-defined plaque and whole vessel quantification was investigated by unsupervised hierarchical clustering analysis based on Ward's method. Detailed features of CCTA findings were assessed according to the clusters (CS1 and CS2). Major adverse cardiac events (MACE)-free survivals were assessed according to the stratifications by FAI and the clusters. RESULTS: Compared with CS2 (n = 119), CS1 (n = 101) were characterized by greater vessel size, increased plaque volume, and high-risk plaque features. FAI was significantly higher in CS1. ROC analyses revealed that best cut-off value of FAI to predict MACE was -73.1. Kaplan-Meier analysis revealed that lesions with FAI >/= -73.1 had a significantly higher risk of MACE. Multivariate Cox proportional hazards regression analysis revealed that age, FAI >/= -73.1, and the clusters were independent predictors of MACE. CONCLUSION: Unsupervised hierarchical clustering analysis revealed two distinct CCTA-defined subgroups and discriminated by high-risk plaque features and increased FAI. The risk of MACE differs significantly according to the increased FAI and ML-defined clusters.",
        "PURPOSE: Little is known about the clinical course of chronic urticaria (CU) and predictors of its prognosis. We evaluated CU patient clusters based on medication scores during the initial 3 months of treatment in an attempt to investigate time to remission and relapse rates for CU and to identify predictors for CU remission. METHODS: In total, 4,552 patients (57.9% female; mean age of 38.6 years) with CU were included in this retrospective cohort study. The K-medoids algorithm was used for clustering CU patients. Kaplan-Meier survival analysis with Cox regression was applied to identify predictors of CU remission. RESULTS: Four distinct clusters were identified: patients with consistently low disease activity (cluster 1, n = 1,786), with medium-to-low disease activity (cluster 2, n = 1,031), with consistently medium disease activity (cluster 3, n = 1,332), or with consistently high disease activity (cluster 4, n = 403). Mean age, treatment duration, peripheral neutrophil counts, total immunoglobulin E, and complements levels were significantly higher for cluster 4 than the other 3 clusters. Median times to remission were also different among the 4 clusters (2.1 vs. 3.3 vs. 6.4 vs. 9.4 years, respectively, P < 0.001). Sensitization to house dust mites (HDMs; at least class 3) and female sex were identified as significant predictors of CU remission. Around 20% of patients who achieved CU remission experienced relapse. CONCLUSIONS: In this study, we identified 4 CU patient clusters by analyzing medication scores during the first 3 months of treatment and found that sensitization to HDMs and female sex can affect CU prognosis. The use of immunomodulators was implicated in the risk for CU relapse.",
        "Shannon's information theoretic perspective of communication helps one to understand the storage and processing of information in one-dimensional sequences. An information theoretic analysis of 937 available completely sequenced prokaryotic genomes and 238 eukaryotic chromosomes is presented. Information content (Id) values were used to cluster these chromosomes. Chargaff's second parity rule i.e compositional self-complementarity, an empirical fact is observed in all the genomes, except for the proteobacteria Candidatus Hodgkinia cicadicola. High information content, arising out of biased base composition in all the 14 chromosomes of Plasmodium falciparum is found among two other genomes of prokaryotes viz. Buchnera aphidicola str. Cc (Cinara cedri) and Candidatus Carsonella ruddii PV. Despite size and compositional variations, both prokaryotic and eukaryotic genomes do not deviate significantly from an equiprobable and random situation. Eukaryotic chromosomes of an organism tend to have similar informational restraints as seen when a simple distance based method is used to cluster them. In eukaryotes, in certain cases, Id values are also similar for the two arms (p and q arm) of the chromosomes. The results of this current study confirm that the information content can provide insights into the clustering of genomes and the evolution of messaging strategies of the genomes. An efficient and robust Perl CGI standalone tool is created based on this information theory algorithm for the analysis of the whole genomes and is made available at https://github.com/AlagurajVeluchamy/InformationTheory.",
        "The ``curse of dimensionality'' and the high computational cost have still limited the application of the evolutionary algorithm in high-dimensional feature selection (FS) problems. This article proposes a new three-phase hybrid FS algorithm based on correlation-guided clustering and particle swarm optimization (PSO) (HFS-C-P) to tackle the above two problems at the same time. To this end, three kinds of FS methods are effectively integrated into the proposed algorithm based on their respective advantages. In the first and second phases, a filter FS method and a feature clustering-based method with low computational cost are designed to reduce the search space used by the third phase. After that, the third phase applies oneself to finding an optimal feature subset by using an evolutionary algorithm with the global searchability. Moreover, a symmetric uncertainty-based feature deletion method, a fast correlation-guided feature clustering strategy, and an improved integer PSO are developed to improve the performance of the three phases, respectively. Finally, the proposed algorithm is validated on 18 publicly available real-world datasets in comparison with nine FS algorithms. Experimental results show that the proposed algorithm can obtain a good feature subset with the lowest computational cost.",
        "The modes of occurrence of elements in coal are important not only because they can provide insights into the sources of mineral matter in coal but also because they are vital in determining the behavior of their environmental and human health impacts. Besides a number of physical and chemical analyses for determining the modes of occurrence in coal, some statistical methods have been commonly adopted to investigate elements in coal. Among many statistical methods, the hierarchy clustering algorithm is the most common method for deducing modes of occurrence of elements in coal. However, different hierarchical clustering algorithms with a number of similarity measures sometimes result in different modes of occurrence of elements in coal, and subsequently in some cases, such results could be confusing. Therefore, which algorithm is more effective in determining the modes of occurrence in coal deserves to be investigated. In this paper, the data sets of coals from the Adaohai coal mine in Inner Mongolia, China, are used for this performance evaluation. From the analytical results with the average linkage hierarchical clustering algorithm on Adaohai coal samples, many instructive and surprising insights can be concluded. For example, selenium, Be, and Tl do not appear to be in agreement with geochemical principles, that is, substituting for P, associated with rare earth elements, and occurring in Fe-sulfides, respectively. In conclusion, the average linkage hierarchical clustering algorithm with correlation similarity is much better in the analysis of the geological processes than the previous statistical method used in Adaohai coal samples, that is, centroid linkage hierarchical clustering algorithm with Pearson correlation similarity.",
        "SUMMARY: In this article, we introduce a hierarchical clustering and Gaussian mixture model with expectation-maximization (EM) algorithm for detecting copy number variants (CNVs) using whole exome sequencing (WES) data. The R shiny package \"HCMMCNVs\" is also developed for processing user-provided bam files, running CNVs detection algorithm, and conducting visualization. Through applying our approach to 325 cancer cell lines in 22 tumor types from Cancer Cell Line Encyclopedia (CCLE), we show that our algorithm is competitive with other existing methods and feasible in using multiple cancer cell lines for CNVs estimation. In addition, by applying our approach to WES data of 120 oral squamous cell carcinoma (OSCC) samples, our algorithm, using the tumor sample only, exhibits more power in detecting CNVs as compared with the methods using both tumors and matched normal counterparts. AVAILABILITY AND IMPLEMENTATION: HCMMCNVs R shiny software is freely available at github repository https://github.com/lunching/HCMM_CNVs. and Zenodo https://doi.org/10.5281/zenodo.4593371. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Energy sources are power force of society and economy to develop. In recent years, extensive energy consumption patterns have threatened China's economic development step by step and made China's economic growth encounter unprecedented \"bottlenecks.\" Therefore, this paper introduce a novel energy allocation scheme including cluster analysis and weighted voting allocation model to limit energy consumption in each region of China. The entire quota allocation process is divided into two parts by the proposed allocation scheme. In the first part, 30 regions in China are grouped into four classes through energy conservation pressure, capacity, responsibility, potential, and effectiveness. And, the total energy consumption is quoted to various classes. In the second part, the total energy consumption of each class is allocated to the corresponding regions. The weighted voting model runs through two-tier allocation schemes, and the allocation schemes based on historical energy consumption, GDP, and population are selected by each class and each region based on the voting rights. The voting rights are quantified by multi-index comprehensive evaluation model, which adopts entropy weight method in the first part owing to inexperience and cuckoo search algorithm (CS) in the second part to choose better weights. The combination of entropy weight method and CS can increase the flexibility of reducing energy consumption policy while maintaining impartiality in decision process. According to the proposed allocation scheme, case study of the allocation for energy consumption in China by 2020 is performed. The allocation results indicate that the proposed allocation scheme improves the fairness and effectiveness to a certain extent, which is superior to the allocation scheme based on historical energy consumption, GDP, and population. We also compare with the state-of-the-art algorithm and prove that our algorithm is more fair and effective. In addition, the proposed distribution scheme can stimulate all regions to cut down energy intensity in the case of meeting the energy consumption needs of each region.",
        "BACKGROUND: The identification of protein families is of outstanding practical importance for in silico protein annotation and is at the basis of several bioinformatic resources. Pfam is possibly the most well known protein family database, built in many years of work by domain experts with extensive use of manual curation. This approach is generally very accurate, but it is quite time consuming and it may suffer from a bias generated from the hand-curation itself, which is often guided by the available experimental evidence. RESULTS: We introduce a procedure that aims to identify automatically putative protein families. The procedure is based on Density Peak Clustering and uses as input only local pairwise alignments between protein sequences. In the experiment we present here, we ran the algorithm on about 4000 full-length proteins with at least one domain classified by Pfam as belonging to the Pseudouridine synthase and Archaeosine transglycosylase (PUA) clan. We obtained 71 automatically-generated sequence clusters with at least 100 members. While our clusters were largely consistent with the Pfam classification, showing good overlap with either single or multi-domain Pfam family architectures, we also observed some inconsistencies. The latter were inspected using structural and sequence based evidence, which suggested that the automatic classification captured evolutionary signals reflecting non-trivial features of protein family architectures. Based on this analysis we identified a putative novel pre-PUA domain as well as alternative boundaries for a few PUA or PUA-associated families. As a first indication that our approach was unlikely to be clan-specific, we performed the same analysis on the P53 clan, obtaining comparable results. CONCLUSIONS: The clustering procedure described in this work takes advantage of the information contained in a large set of pairwise alignments and successfully identifies a set of putative families and family architectures in an unsupervised manner. Comparison with the Pfam classification highlights significant overlap and points to interesting differences, suggesting that our new algorithm could have potential in applications related to automatic protein classification. Testing this hypothesis, however, will require further experiments on large and diverse sequence datasets.",
        "BACKGROUND: chronic diseases and multimorbidity are on the rise and have a great impact on health and services. OBJECTIVES: to assess the prevalence and patterns of chronic diseases. DESIGN: cross-sectional population-based study on administrative data. SETTING AND PARTICIPANTS: the study includes 3,234,276 Tuscany (Central Italy) inhabitants aged over 15, observed as at 01.01.2019. MAIN OUTCOME MEASURES: subjects were classified as affected or not affected by one of the 17 chronic diseases considered, according to administrative data algorithms. Population prevalence was estimated overall and stratified by gender, age range, and socioeconomic level. A factor analysis was performed in order to evaluate multimorbidity. RESULTS: in Tuscany, 444.8 per 1,000 inhabitants aged over 15 have a chronic disease. The prevalence is 463.5 per 1,000 among females and 424.5 per 1,000 among males, but the two age-adjusted prevalences are equal. The prevalence of chronic patients increases with the level of socioeconomic disadvantage. The most frequent disease is hypertension (308.7 per 1,000), followed by dyslipidaemia (251 per 1,000) and diabetes (75.7 per 1,000). Inflammatory rheumatic diseases and neurological diseases are more prevalent among females than males. The prevalence identified among males almost doubles in comparison to females for all other diseases, in particular for circulatory system diseases. Chronic patients suffer from at least two pathologies in 53.2% of cases. On average, males have more diseases than females. The cardiovascular factor (circulatory system diseases and related) and the neurological factor (neurological diseases and mental disorders) emerged from the factor analysis. CONCLUSIONS: this study quantifies the burden of chronic diseases in the population, which is useful information in epidemiology, in clinical practice, and in services management.",
        "The purposes are to evaluate the Distributed Clustering Algorithm (DCA) applicability in the power system's big data processing and find the information economic dispatch strategy suitable for new energy consumption in power systems. A two-layer DCA algorithm is proposed based on K-Means Clustering (KMC) and Affinity Propagation (AP) clustering algorithms. Then the incentive Demand Response (DR) is introduced, and the DR flexibility of the user side is analyzed. Finally, the day-ahead dispatch and real-time dispatch schemes are combined, and a multi-period information economic dispatch model is constructed. The algorithm performance is analyzed according to case analyses of new energy consumption. Results demonstrate that the two-layer DCA's calculation time is 5.23s only, the number of iterations is small, and the classification accuracy rate reaches 0.991. Case 2 corresponding to the proposed model can consume the new energy, and the income of the aggregator can be maximized. In short, the multi-period information economic dispatch model can consume the new energy and meet the DR of the user side.",
        "BACKGROUND: Most studies on children evaluate longitudinal growth as an important health indicator. Different methods have been used to detect growth patterns across childhood, but with no comparison between them to evaluate result consistency. We explored the variation in growth patterns as detected by different clustering and latent class modelling techniques. Moreover, we investigated how the characteristics/features (e.g. slope, tempo, velocity) of longitudinal growth influence pattern detection. METHODS: We studied 1134 children from The Applied Research Group for Kids cohort with longitudinal-growth measurements [height, weight, body mass index (BMI)] available from birth until 12 years of age. Growth patterns were identified by latent class mixed models (LCMM) and time-series clustering (TSC) using various algorithms and distance measures. Time-invariant features were extracted from all growth measures. A random forest classifier was used to predict the identified growth patterns for each growth measure using the extracted features. RESULTS: Overall, 72 TSC configurations were tested. For BMI, we identified three growth patterns by both TSC and LCMM. The clustering agreement was 58% between LCMM and TS clusters, whereas it varied between 30.8% and 93.3% within the TSC configurations. The extracted features (n = 67) predicted the identified patterns for each growth measure with accuracy of 82%-89%. Specific feature categories were identified as the most important predictors for patterns of all tested growth measures. CONCLUSION: Growth-pattern detection is affected by the method employed. This can impact on comparisons across different populations or associations between growth patterns and health outcomes. Growth features can be reliably used as predictors of growth patterns.",
        "Extracting genes involved in cancer lesions from gene expression data is critical for cancer research and drug development. the method of feature selection has attracted much attention in the field of bioinformatics. Principal Component Analysis (PCA) is a widely used method for learning low-dimensional representation. Some variants of PCA have been proposed to improve the robustness and sparsity of the algorithm. However, the existing methods ignore the high-order relationships between data. In this paper, a new model named Robust Principal Component Analysis via Hypergraph Regularization (HRPCA) is proposed. In detail, HRPCA utilizes L2,1-norm to reduce the effect of outliers and make data sufficiently row-sparse. And the Hypergraph Regularization is introduced to consider the complex relationship between data. Important information hidden in the data are mined, and this method ensures the accuracy of the resulting data relationship information. Extensive experiments on multi-view biological data demonstrate that the feasible and effective of the proposed approach.",
        "Water resources management requires a proper understanding of the status of available and exploitable water. One of the useful management tools is the use of simulation models that are highly efficient in spite of the complex problems in the groundwater sector. In the present study, three data-based models, namely, group method of data handling (GMDH), Bayesian network (BN), and artificial neural network (ANN), have been investigated to simulate the groundwater levels and assess the quantitative status of aquifers. Five observation wells were selected in Birjand aquifer using spatial clustering to analyze and evaluate the aquifer. To determine the effective variables in predicting groundwater level, 10 scenarios were developed by combining several variables, including groundwater level in the previous month, aquifer exploitation, surface recharge, precipitation, temperature, and evaporation. Results showed that the GMDH model with three input variables, i.e., the groundwater level in the previous month, aquifer exploitation, and precipitation, had the highest prediction performance, RMSE, NASH, MAPE, and R(2) of which were obtained equal to 0.074, 0.97, 0.0037, and 0.97, respectively. Furthermore, Taylor's diagram showed that the predicted values using the GMDH model had the highest correlation with the observational data. Hydrograph simulation was performed for 6 years to analyze the condition of the aquifer. The results showed that the groundwater level is in critical condition in this aquifer, and a 1.2-m groundwater loss was predicted for this aquifer. The findings of this study show that the management of the studied aquifer is necessary to improve its current situation.",
        "This paper introduces a kernel based fuzzy clustering approach to deal with the non-linear separable problems by applying kernel Radial Basis Functions (RBF) which maps the input data space non-linearly into a high-dimensional feature space. Discovering clusters in the high-dimensional genomics data is extremely challenging for the bioinformatics researchers for genome analysis. To support the investigations in bioinformatics, explicitly on genomic clustering, we proposed high-dimensional kernelized fuzzy clustering algorithms based on Apache Spark framework for clustering of Single Nucleotide Polymorphism (SNP) sequences. The paper proposes the Kernelized Scalable Random Sampling with Iterative Optimization Fuzzy c-Means (KSRSIO-FCM) which inherently uses another proposed Kernelized Scalable Literal Fuzzy c-Means (KSLFCM) clustering algorithm. Both the approaches completely adapt the Apache Spark cluster framework by localized sub-clustering Resilient Distributed Dataset (RDD) method. Additionally, we are also proposing a preprocessing approach for generating numeric feature vectors for huge SNP sequences and making it a scalable preprocessing approach by executing it on an Apache Spark cluster, which is applied to real-world SNP datasets taken from open-internet repositories of two different plant species, i.e., soybean and rice. The comparison of the proposed scalable kernelized fuzzy clustering results with similar works shows the significant improvement of the proposed algorithm in terms of time and space complexity, Silhouette index, and Davies-Bouldin index. Exhaustive experiments are performed on various SNP datasets to show the effectiveness of proposed KSRSIO-FCM in comparison with proposed KSLFCM and other scalable clustering algorithms, i.e., SRSIO-FCM, and SLFCM.",
        "PURPOSE: Probe-based dynamic (4-D) imaging modalities capture breast intratumor heterogeneity both spatially and kinetically. Characterizing heterogeneity through tumor sub-populations with distinct functional behavior may elucidate tumor biology to improve targeted therapy specificity and enable precision clinical decision making. METHODS: We propose an unsupervised clustering algorithm for 4-D imaging that integrates Markov-Random Field (MRF) image segmentation with time-series analysis to characterize kinetic intratumor heterogeneity. We applied this to dynamic FDG PET scans by identifying distinct time-activity curve (TAC) profiles with spatial proximity constraints. We first evaluated algorithm performance using simulated dynamic data. We then applied our algorithm to a dataset of 50 women with locally advanced breast cancer imaged by dynamic FDG PET prior to treatment and followed to monitor for disease recurrence. A functional tumor heterogeneity (FTH) signature was then extracted from functionally distinct sub-regions within each tumor. Cross-validated time-to-event analysis was performed to assess the prognostic value of FTH signatures compared to established histopathological and kinetic prognostic markers. RESULTS: Adding FTH signatures to a baseline model of known predictors of disease recurrence and established FDG PET uptake and kinetic markers improved the concordance statistic (C-statistic) from 0.59 to 0.74 (p = 0.005). Unsupervised hierarchical clustering of the FTH signatures identified two significant (p < 0.001) phenotypes of tumor heterogeneity corresponding to high and low FTH. Distributions of FDG flux, or Ki, were significantly different (p = 0.04) across the two phenotypes. CONCLUSIONS: Our findings suggest that imaging markers of FTH add independent value beyond standard PET imaging metrics in predicting recurrence-free survival in breast cancer and thus merit further study.",
        "Most everyday actions engender interactions with meaningful emotionally-laden stimuli. This study aimed to select pictures of objects as emotional stimulus of affordance to be grasped. The participant's depression trait was also assessed to examine its effect on the judgment of these pictures, and time spent in the classification was computed. Sixty-three participants joined this study. Self-Assessment-Manikin scale was used to classify pictures of the objects, and Beck Depression Inventory was applied to distribute the sample according depression trait. Cluster analysis was used in the classification of 123 objects based on valence and arousal values. Cluster results returned 102 classified pictures in three categories: pleasant (21), neutral (48) and unpleasant (33). Where cluster analysis did not agree, the picture was excluded and not used any further (21). Pleasant pictures presented the highest valence values and unpleasant pictures the lowest, and both categories returned the highest arousal level. In the middle of the valence range, the neutral category evoked the lowest arousal levels. Participants were slower to classify unpleasant pictures in valence sub-scale and faster to classify neutral pictures in arousal one. There was no effect of depression in the response time needed to score the pictures. Thus, agreement of high-performance soft clustering algorithms emerged as a good tool to classify pictures representing objects based on valence and arousal dimensions. Depression trait does not significantly affect the accuracy or time-order of emotional classification. Finally, we presented a set of emotional stimuli that can be employed to examine distinct aspects of emotion over physiology and behavior.",
        "The COVID-19 virus is spreading across the world very rapidly. The World Health Organization (WHO) declared it a global pandemic on 11 March 2020. Early detection of this virus is necessary because of the unavailability of any specific drug. The researchers have developed different techniques for COVID-19 detection, but only a few of them have achieved satisfactory results. There are three ways for COVID-19 detection to date, those are real-time reverse transcription-polymerize chain reaction (RT-PCR), Computed Tomography (CT), and X-ray plays. In this work, we have proposed a less expensive computational model for automatic COVID-19 detection from Chest X-ray and CT-scan images. Our paper has a two-fold contribution. Initially, we have extracted deep features from the image dataset and then introduced a completely novel meta-heuristic feature selection approach, named Clustering-based Golden Ratio Optimizer (CGRO). The model has been implemented on three publicly available datasets, namely the COVID CT-dataset, SARS-Cov-2 dataset, and Chest X-Ray dataset, and attained state-of-the-art accuracies of 99.31%, 98.65%, and 99.44%, respectively.",
        "Single-cell RNA-seq (scRNA-seq) is a powerful tool to measure the expression patterns of individual cells and discover heterogeneity and functional diversity among cell populations. Due to variability, it is challenging to analyze such data efficiently. Many clustering methods have been developed using at least one free parameter. Different choices for free parameters may lead to substantially different visualizations and clusters. Tuning free parameters is also time consuming. Thus there is need for a simple, robust, and efficient clustering method. In this paper, we propose a new regularized Gaussian graphical clustering (RGGC) method for scRNA-seq data. RGGC is based on high-order (partial) correlations and subspace learning, and is robust over a wide-range of a regularized parameter lambda. Therefore, we can simply set lambda=2 or lambda=log(p) for AIC (Akaike information criterion) or BIC (Bayesian information criterion) without cross-validation. Cell subpopulations are discovered by the Louvain community detection algorithm that determines the number of clusters automatically. There is no free parameter to be tuned with RGGC. When evaluated with simulated and benchmark scRNA-seq data sets against widely used methods, RGGC is computationally efficient and one of the top performers. It can detect inter-sample cell heterogeneity, when applied to glioblastoma scRNA-seq data.",
        "A new original procedure based on k-means clustering is designed to find the most appropriate clinical variables able to efficiently separate into groups similar patients diagnosed with diabetes mellitus type 2 (DMT2) and underlying diseases (arterial hypertonia (AH), ischemic heart disease (CHD), diabetic polyneuropathy (DPNP), and diabetic microangiopathy (DMA)). Clustering is a machine learning tool for discovering structures in datasets. Clustering has been proven to be efficient for pattern recognition based on clinical records. The considered combinatorial k-means procedure explores all possible k-means clustering with a determined number of descriptors and groups. The predetermined conditions for the partitioning were as follows: every single group of patients included patients with DMT2 and one of the underlying diseases; each subgroup formed in such a way was subject to partitioning into three patterns (good health status, medium health status, and degenerated health status); optimal descriptors for each disease and groups. The selection of the best clustering is obtained through the parameter called global variance, defined as the sum of all variance values of all clinical variables of all the clusters. The best clinical parameters are found by minimizing this global variance. This methodology has to identify a set of variables that are assumed to separate each underlying disease efficiently in three different subgroups of patients. The hierarchical clustering obtained for these four underlying diseases could be used to build groups of patients with correlated clinical data. The proposed methodology gives surmised results from complex data based on a relationship with the health status of the group and draws a picture of the prediction rate of the ongoing health status.",
        "On-body device position awareness plays an important role in providing smartphone-based services with high levels of usability and quality. Traditionally, the problem assumed that the positions that were supported by the system were fixed at the time of design. Thus, if a user stores his/her terminal into an unsupported position, the system forcibly classifies it into one of the supported positions. In contrast, we propose a framework to discover new positions that are not initially supported by the system, which adds them as recognition targets via labeling by a user and re-training on-the-fly. In this article, we focus on a component of identifying a set of samples that are derived from a single storing position, which we call new position candidate identification. Clustering is applied as a key component to prepare a reliable dataset for re-training and to reduce the user's burden of labeling. Specifically, density-based spatial clustering of applications with noise (DBSCAN) is employed because it does not require the number of clusters in advance. We propose a method of finding an optimal value of a main parameter, Eps-neighborhood (eps), which affects the accuracy of the resultant clusters. Simulation-based experiments show that the proposed method performs as if the number of new positions were known in advance. Furthermore, we clarify the timing of performing the new position candidate identification process, in which we propose criteria for qualifying a cluster as the one comprising a new position.",
        "Due to the corruptions or noises that existed in real-world data sets, the affinity graphs constructed by the classical spectral clustering-based subspace clustering algorithms may not be able to reveal the intrinsic subspace structures of data sets faithfully. In this article, we reconsidered the data reconstruction problem in spectral clustering-based algorithms and proposed the idea of ``relation reconstruction.'' We pointed out that a data sample could be represented by the neighborhood relation computed between its neighbors and itself. The neighborhood relation could indicate the true membership of its corresponding original data sample to the subspaces of a data set. We also claimed that a data sample's neighborhood relation could be reconstructed by the neighborhood relations of other data samples; then, we suggested a much different way to define affinity graphs consequently. Based on these propositions, a sparse relation representation (SRR) method was proposed for solving subspace clustering problems. Moreover, by introducing the local structure information of original data sets into SRR, an extension of SRR, namely structured sparse relation representation (SSRR) was presented. We gave an optimization algorithm for solving SRR and SSRR problems and analyzed its computation burden and convergence. Finally, plentiful experiments conducted on different types of databases showed the superiorities of SRR and SSRR.",
        "Handwritten documents can be characterized by their content or by the shape of the written characters. We focus on the problem of comparing a person's handwriting to a document of unknown provenance using the shape of the writing, as is done in forensic applications. To do so, we first propose a method for processing scanned handwritten documents to decompose the writing into small graphical structures, often corresponding to letters. We then introduce a measure of distance between two such structures that is inspired by the graph edit distance, and a measure of center for a collection of the graphs. These measurements are the basis for an outlier tolerant K-means algorithm to cluster the graphs based on structural attributes, thus creating a template for sorting new documents. Finally, we present a Bayesian hierarchical model to capture the propensity of a writer for producing graphs that are assigned to certain clusters. We illustrate the methods using documents from the Computer Vision Lab dataset. We show results of the identification task under the cluster assignments and compare to the same modeling, but with a less flexible grouping method that is not tolerant of incidental strokes or outliers.",
        "Multiview subspace clustering (MSC) has attracted growing attention due to the extensive value in various applications, such as natural language processing, face recognition, and time-series analysis. In this article, we are devoted to address two crucial issues in MSC: 1) high computational cost and 2) cumbersome multistage clustering. Existing MSC approaches, including tensor singular value decomposition (t-SVD)-MSC that has achieved promising performance, generally utilize the dataset itself as the dictionary and regard representation learning and clustering process as two separate parts, thus leading to the high computational overhead and unsatisfactory clustering performance. To remedy these two issues, we propose a novel MSC model called joint skinny tensor learning and latent clustering (JSTC), which can learn high-order skinny tensor representations and corresponding latent clustering assignments simultaneously. Through such a joint optimization strategy, the multiview complementary information and latent clustering structure can be exploited thoroughly to improve the clustering performance. An alternating direction minimization algorithm, which owns low computational complexity and can be run in parallel when solving several key subproblems, is carefully designed to optimize the JSTC model. Such a nice property makes our JSTC an appealing solution for large-scale MSC problems. We conduct extensive experiments on ten popular datasets and compare our JSTC with 12 competitors. Five commonly used metrics, including four external measures (NMI, ACC, F-score, and RI) and one internal metric (SI), are adopted to evaluate the clustering quality. The experimental results with the Wilcoxon statistical test demonstrate the superiority of the proposed method in both clustering performance and operational efficiency.",
        "BACKGROUND: In recent years, there has been a massive increase in the number of people suffering from psoriasis. For proper psoriasis diagnosis, psoriasis lesion segmentation is a pre-requisite for quantifying the severity of this disease. However, segmentation of psoriatic lesion cannot be evaluated just by visual inspection as they exhibit inter and intra variability among the severity classes. Most of the approaches currently pursued by dermatologists are subjective in nature. The existing conventional clustering algorithm for objective segmentation of psoriasis lesion suffers from limitations of premature local convergence. OBJECTIVE: An alternative method for psoriatic lesion segmentation with the objective analysis is sought in the present work. The present work aims at obtaining optimal lesion segmentation by adopting an evolutionary optimization technique which possesses a higher probability of global convergence for psoriasis lesion segmentation. METHOD: A hybrid evolutionary optimization technique based on the combination of two swarm intelligence algorithms; namely Artificial Bee Colony and Seeker Optimization algorithm has been proposed. The initial population for the hybrid technique is obtained from the two conventional local-based approaches i.e. Fuzzy C-means and K-means clustering algorithms. RESULTS: The initial population selection from the convergence of classical techniques reduces the effect of population dynamics on the final solution and hence yields precise lesion segmentation with Jaccard Index of 0.91 from 720 psoriasis images. CONCLUSION: The performance comparison reflects the superior performance of the proposed algorithm over other swarm intelligence and conventional clustering algorithms.",
        "INTRODUCTION: Bicyclists are more vulnerable compared to other road users. Therefore, it is critical to investigate the contributing factors to bicyclist injury severity to help provide better biking environment and improve biking safety. According to the data provided by National Highway Traffic Safety Administration (NHTSA), a total of 8,028 bicyclists were killed in bicycle-vehicle crashes from 2007 to 2017. The number of fatal bicyclists had increased rapidly by approximately 11.70% during the past 10 years (NHTSA, 2019). METHODS: This paper conducts a latent class clustering analysis based on the police reported bicycle-vehicle crash data collected from 2007 to 2014 in North Carolina to identify the heterogeneity inherent in the crash data. First, the most appropriate number of clusters is determined in which each cluster has been characterized by the distribution of the featured variables. Then, partial proportional odds models are developed for each cluster to further analyze the impacts on bicyclist injury severity for specific crash patterns. RESULTS: Marginal effects are calculated and used to evaluate and interpret the effect of each significant explanatory variable. The model results reveal that variables could have different influence on the bicyclist injury severity between clusters, and that some variables only have significant impacts on particular clusters. CONCLUSIONS: The results clearly indicate that it is essential to conduct latent class clustering analysis to investigate the impact of explanatory variables on bicyclist injury severity considering unobserved or latent features. In addition, the latent class clustering is found to be able to provide more accurate and insightful information on the bicyclist injury severity analysis. Practical Applications: In order to improve biking safety, regulations need to be established to prevent drinking and lights need to be provided since alcohol and lighting condition are significant factors in severe injuries according to the modeling results.",
        "Multiview clustering as an important unsupervised method has been gathering a great deal of attention. However, most multiview clustering methods exploit the self-representation property to capture the relationship among data, resulting in high computation cost in calculating the self-representation coefficients. In addition, they usually employ different regularizers to learn the representation tensor or matrix from which a transition probability matrix is constructed in a separate step, such as the one proposed by Wu et al.. Thus, an optimal transition probability matrix cannot be guaranteed. To solve these issues, we propose a unified model for multiview spectral clustering by directly learning an adaptive transition probability matrix (MCA^2M), rather than an individual representation matrix of each view. Different from the one proposed by Wu et al., MCA^2M utilizes the one-step strategy to directly learn the transition probability matrix under the robust principal component analysis framework. Unlike existing methods using the absolute symmetrization operation to guarantee the nonnegativity and symmetry of the affinity matrix, the transition probability matrix learned from MCA^2M is nonnegative and symmetric without any postprocessing. An alternating optimization algorithm is designed based on the efficient alternating direction method of multipliers. Extensive experiments on several real-world databases demonstrate that the proposed method outperforms the state-of-the-art methods.",
        "Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has a worldwide devastating effect. Understanding the evolution and transmission of SARS-CoV-2 is of paramount importance for controlling, combating and preventing COVID-19. Due to the rapid growth in both the number of SARS-CoV-2 genome sequences and the number of unique mutations, the phylogenetic analysis of SARS-CoV-2 genome isolates faces an emergent large-data challenge. We introduce a dimension-reduced K-means clustering strategy to tackle this challenge. We examine the performance and effectiveness of three dimension-reduction algorithms: principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP). By using four benchmark datasets, we found that UMAP is the best-suited technique due to its stable, reliable, and efficient performance, its ability to improve clustering accuracy, especially for large Jaccard distanced-based datasets, and its superior clustering visualization. The UMAP-assisted K-means clustering enables us to shed light on increasingly large datasets from SARS-CoV-2 genome isolates.",
        "The aim of this work is to develop a common automatic computer method to distinguish human individuals with abnormal gait patterns from those with normal gait patterns. As long as the silhouette gait images of the subjects are obtainable, the proposed method is capable of providing online anomaly gait detection result without additional work on analyzing the gait features of the target subjects before ahead. Moreover, the proposed method does not need any parameter settings by users and can start producing detection results under the work by only collecting a very small number of gait samples, even though none of those gait samples are abnormal. Therefore, the proposed method can provide fast and simple deployment for various anomaly gait detection application scenarios. The proposed method is composed of two main modules: (1) feature extraction from gait images and (2) anomaly detection via binary classification. In the first module, a new representation of the most frequently involved area of the silhouette gait images called full gait energy image (F-GEI) is proposed. Furthermore, based on the F-GEI, a novel and simple method characterizing individual walking properties is developed to extract gait features from individual subjects. In the second module, based on the very limited prior knowledge on the target dataset, a semisupervised clustering algorithm is proposed to perform the binary classification for detecting the gait anomaly of each subject. The performance of the proposed gait anomaly detection method was evaluated on the human gaits dataset in comparison with three state-of-the-art methods. The experiment results show that the proposed method is an effective and efficient gait anomaly detection method in terms of accuracy, robustness, and computational efficiency.",
        "The combination of Markov state modeling (MSM) and molecular dynamics (MD) simulations has been shown in recent years to be a valuable approach to unravel the slow processes of molecular systems with increasing complexity. While the algorithms for intermediate steps in the MSM workflow such as featurization and dimensionality reduction have been specifically adapted to MD datasets, conventional clustering methods are generally applied to the discretization step. This work adds to recent efforts to develop specialized density-based clustering algorithms for the Boltzmann-weighted data from MD simulations. We introduce the volume-scaled common nearest neighbor (vs-CNN) clustering that is an adapted version of the common nearest neighbor (CNN) algorithm. A major advantage of the proposed algorithm is that the introduced density-based criterion directly links to a free-energy notion via Boltzmann inversion. Such a free-energy perspective allows a straightforward hierarchical scheme to identify conformational clusters at different levels of a generally rugged free-energy landscape of complex molecular systems.",
        "Despite the promising preliminary results, tensor-singular value decomposition (t-SVD)-based multiview subspace is incapable of dealing with real problems, such as noise and illumination changes. The major reason is that tensor-nuclear norm minimization (TNNM) used in t-SVD regularizes each singular value equally, which does not make sense in matrix completion and coefficient matrix learning. In this case, the singular values represent different perspectives and should be treated differently. To well exploit the significant difference between singular values, we study the weighted tensor Schatten p-norm based on t-SVD and develop an efficient algorithm to solve the weighted tensor Schatten p-norm minimization (WTSNM) problem. After that, applying WTSNM to learn the coefficient matrix in multiview subspace clustering, we present a novel multiview clustering method by integrating coefficient matrix learning and spectral clustering into a unified framework. The learned coefficient matrix well exploits both the cluster structure and high-order information embedded in multiview views. The extensive experiments indicate the efficiency of our method in six metrics.",
        "Cellular programs often exhibit strong heterogeneity and asynchrony in the timing of program execution. Single-cell RNA-seq technology has provided an unprecedented opportunity for characterizing these cellular processes by simultaneously quantifying many parameters at single-cell resolution. Robust trajectory inference is a critical step in the analysis of dynamic temporal gene expression, which can shed light on the mechanisms of normal development and diseases. Here, we present TiC2D, a novel algorithm for cell trajectory inference from single-cell RNA-seq data, which adopts a consensus clustering strategy to precisely cluster cells. To evaluate the power of TiC2D, we compare it with three state-of-the-art methods on four independent single-cell RNA-seq datasets. The results show that TiC2D can accurately infer developmental trajectories from single-cell transcriptome. Furthermore, the reconstructed trajectories enable us to identify key genes involved in cell fate determination and to obtain new insights about their roles at different developmental stages.",
        "PURPOSE: Elite athletes experience chronic sleep insufficiency due to training and competition schedules. However, there is little research on sleep and caffeine use of elite youth athletes and a need for a more nuanced understanding of their sleep difficulties. This study aimed to (1) examine the differences in sleep characteristics of elite youth athletes by individual and team sports, (2) study the associations between behavioral risk factors associated with obstructive sleep apnea and caffeine use with sleep quality, and (3) characterize the latent sleep profiles of elite youth athletes to optimize the sleep support strategy. METHODS: A group (N = 135) of elite national youth athletes completed a self-administered questionnaire consisting of the Pittsburgh Sleep Quality Index (PSQI) and questions pertaining to obstructive sleep apnea, napping behavior, and caffeine use. K-means clustering was used to characterize unique sleep characteristic subgroups based on PSQI components. RESULTS: Athletes reported 7.0 (SD = 1.2) hours of sleep. Out of the total group, 45.2% of the athletes had poor quality sleep (PSQI global >5), with team-sport athletes reporting significantly poorer sleep quality than individual-sport athletes. Multiple logistic regression analysis indicated that sport type significantly correlated with poor sleep quality. The K-means clustering algorithm classified athletes' underlying sleep characteristics into 4 clusters to efficiently identify athletes with similar underlying sleep issues to enhance interventional strategies. CONCLUSION: These findings suggest that elite youth team-sport athletes are more susceptible to poorer sleep quality than individual-sport athletes. Clustering methods can help practitioners characterize sleep-related problems and develop efficient athlete support strategies.",
        "Cell clustering is one of the most important and commonly performed tasks in single-cell RNA sequencing (scRNA-seq) data analysis. An important step in cell clustering is to select a subset of genes (referred to as 'features'), whose expression patterns will then be used for downstream clustering. A good set of features should include the ones that distinguish different cell types, and the quality of such set could have a significant impact on the clustering accuracy. All existing scRNA-seq clustering tools include a feature selection step relying on some simple unsupervised feature selection methods, mostly based on the statistical moments of gene-wise expression distributions. In this work, we carefully evaluate the impact of feature selection on cell clustering accuracy. In addition, we develop a feature selection algorithm named FEAture SelecTion (FEAST), which provides more representative features. We apply the method on 12 public scRNA-seq datasets and demonstrate that using features selected by FEAST with existing clustering tools significantly improve the clustering accuracy.",
        "The concentration of nitrogen oxide (NOx) emissions is an important environmental index in the cement production process. The purpose of predicting NOx emission concentration during cement production is to optimize the denitration process to reduce NOx emission. However, due to the problems of time delay, nonlinearity, uncertainty, and data continuity in the cement production process, it is difficult to establish an accurate NOx concentration prediction model. In order to solve the above problems, a NOx emission concentration prediction model using a deep belief network with clustering and time series features (CT-DBN) is proposed in this paper. Particularly, to improve data sparsity and enhance data characteristics, a clustering algorithm is introduced into the model to process the original data of each variable; the time series containing delay information are introduced into the input layer, which combines previous and current variable data into time series data to eliminate the influence of the time delay on the prediction of NOx emission concentration. In addition, restricted Boltzmann machine (RBM) is used to extract data features, and a gradient descent algorithm is used to reversely adjust network parameters to establish a deep belief network model (DBN). Experiments prove that the method in this paper has higher accuracy, stronger stability, and better generalization ability in predicting NOx emission concentration in cement production. The CT-DBN model realizes the accurate prediction of NOx emission concentration, provides guidance for denitration control, and reduces NOx emissions.",
        "The meaningful patterns embedded in high-dimensional multi-view data sets typically tend to have a much more compact representation that often lies close to a low-dimensional manifold. Identification of hidden structures in such data mainly depends on the proper modeling of the geometry of low-dimensional manifolds. In this regard, this article presents a manifold optimization-based integrative clustering algorithm for multi-view data. To identify consensus clusters, the algorithm constructs a joint graph Laplacian that contains denoised cluster information of the individual views. It optimizes a joint clustering objective while reducing the disagreement between the cluster structures conveyed by the joint and individual views. The optimization is performed alternatively over k-means and Stiefel manifolds. The Stiefel manifold helps to model the nonlinearities and differential clusters within the individual views, whereas k-means manifold tries to elucidate the best-fit joint cluster structure of the data. A gradient-based movement is performed separately on the manifold of each view so that individual nonlinearity is preserved while looking for shared cluster information. The convergence of the proposed algorithm is established over the manifold and asymptotic convergence bound is obtained to quantify theoretically how fast the sequence of iterates generated by the algorithm converges to an optimal solution. The integrative clustering on benchmark and multi-omics cancer data sets demonstrates that the proposed algorithm outperforms state-of-the-art multi-view clustering approaches.",
        "The choice of the most appropriate unsupervised machine-learning method for \"heterogeneous\" or \"mixed\" data, i.e. with both continuous and categorical variables, can be challenging. Our aim was to examine the performance of various clustering strategies for mixed data using both simulated and real-life data. We conducted a benchmark analysis of \"ready-to-use\" tools in R comparing 4 model-based (Kamila algorithm, Latent Class Analysis, Latent Class Model [LCM] and Clustering by Mixture Modeling) and 5 distance/dissimilarity-based (Gower distance or Unsupervised Extra Trees dissimilarity followed by hierarchical clustering or Partitioning Around Medoids, K-prototypes) clustering methods. Clustering performances were assessed by Adjusted Rand Index (ARI) on 1000 generated virtual populations consisting of mixed variables using 7 scenarios with varying population sizes, number of clusters, number of continuous and categorical variables, proportions of relevant (non-noisy) variables and degree of variable relevance (low, mild, high). Clustering methods were then applied on the EPHESUS randomized clinical trial data (a heart failure trial evaluating the effect of eplerenone) allowing to illustrate the differences between different clustering techniques. The simulations revealed the dominance of K-prototypes, Kamila and LCM models over all other methods. Overall, methods using dissimilarity matrices in classical algorithms such as Partitioning Around Medoids and Hierarchical Clustering had a lower ARI compared to model-based methods in all scenarios. When applying clustering methods to a real-life clinical dataset, LCM showed promising results with regard to differences in (1) clinical profiles across clusters, (2) prognostic performance (highest C-index) and (3) identification of patient subgroups with substantial treatment benefit. The present findings suggest key differences in clustering performance between the tested algorithms (limited to tools readily available in R). In most of the tested scenarios, model-based methods (in particular the Kamila and LCM packages) and K-prototypes typically performed best in the setting of heterogeneous data.",
        "We present a framework exploiting the cascade of phase transitions occurring during a simulated annealing of the expectation-maximization algorithm to cluster datasets with multiscale structures. Using the weighted local covariance, we can extract, a posteriori and without any prior knowledge, information on the number of clusters at different scales together with their size. We also study the linear stability of the iterative scheme to derive the threshold at which the first transition occurs and show how to approximate the next ones. Finally, we combine simulated annealing together with recent developments of regularized Gaussian mixture models to learn a principal graph from spatially structured datasets that can also exhibit many scales.",
        "The heterogeneity of disease is a major concern in medical research and is commonly characterized as subtypes with different pathogeneses exhibiting distinct prognoses and treatment effects. The classification of a population into homogeneous subgroups is challenging, especially for complex diseases. Recent studies show that gut microbiome compositions play a vital role in disease development, and it is of great interest to cluster patients according to their microbial profiles. There are a variety of beta diversity measures to quantify the dissimilarity between the compositions of different samples for clustering. However, using different beta diversity measures results in different clusters, and it is difficult to make a choice among them. Considering microbial compositions from 16S rRNA sequencing, which are presented as a high-dimensional vector with a large proportion of extremely small or even zero-valued elements, we set up three simulation experiments to mimic the microbial compositional data and evaluate the performance of different beta diversity measures in clustering. It is shown that the Kullback-Leibler divergence-based beta diversity, including the Jensen-Shannon divergence and its square root, and the hypersphere-based beta diversity, including the Bhattacharyya and Hellinger, can capture compositional changes in low-abundance elements more efficiently and can work stably. Their performance on two real datasets demonstrates the validity of the simulation experiments.",
        "Clustering is a widely used machine learning technique for unlabelled data. One of the recently proposed techniques is the twin support vector clustering (TWSVC) algorithm. The idea of TWSVC is to generate hyperplanes for each cluster. TWSVC utilizes the hinge loss function to penalize the misclassification. However, the hinge loss relies on shortest distance between different clusters, and is unstable for noise-corrupted datasets, and for re-sampling. In this paper, we propose a novel Sparse Pinball loss Twin Support Vector Clustering (SPTSVC). The proposed SPTSVC involves the -insensitive pinball loss function to formulate a sparse solution. Pinball loss function provides noise-insensitivity and re-sampling stability. The -insensitive zone provides sparsity to the model and improves testing time. Numerical experiments on synthetic as well as real world benchmark datasets are performed to show the efficacy of the proposed model. An analysis on the sparsity of various clustering algorithms is presented in this work. In order to show the feasibility and applicability of the proposed SPTSVC on biomedical data, experiments have been performed on epilepsy and breast cancer datasets.",
        "Biological functions emerge from complex and dynamic networks of protein-protein interactions. Because these protein-protein interaction networks, or interactomes, represent pairwise connections within a hierarchically organized system, it is often useful to identify higher-order associations embedded within them, such as multimember protein complexes. Graph-based clustering techniques are widely used to accomplish this goal, and dozens of field-specific and general clustering algorithms exist. However, interactomes can be prone to errors, especially when inferred from high-throughput biochemical assays. Therefore, robustness to network-level noise is an important criterion. Here, we tested the robustness of a range of graph-based clustering algorithms in the presence of noise, including algorithms common across domains and those specific to protein networks. Strikingly, we found that all of the clustering algorithms tested here markedly amplified network-level noise. Randomly rewiring only 1% of network edges yielded more than a 50% change in clustering results. Moreover, we found the impact of network noise on individual clusters was not uniform: some clusters were consistently robust to injected noise, whereas others were not. Therefore we developed the clust.perturb R package and Shiny web application to measure the reproducibility of clusters by randomly perturbing the network. We show that clust.perturb results are predictive of real-world cluster stability: poorly reproducible clusters as identified by clust.perturb are significantly less likely to be reclustered across experiments. We conclude that graph-based clustering amplifies noise in protein interaction networks, but quantifying the robustness of a cluster to network noise can separate stable protein complexes from spurious associations.",
        "Benefit from avoiding the utilization of labeled samples, which are usually insufficient in the real world, unsupervised learning has been regarded as a speedy and powerful strategy on clustering tasks. However, clustering directly from primal data sets leads to high computational cost, which limits its application on large-scale and high-dimensional problems. Recently, anchor-based theories are proposed to partly mitigate this problem and field naturally sparse affinity matrix, while it is still a challenge to get excellent performance along with high efficiency. To dispose of this issue, we first presented a fast semisupervised framework (FSSF) combined with a balanced K-means-based hierarchical K-means (BKHK) method and the bipartite graph theory. Thereafter, we proposed a fast self-supervised clustering method involved in this crucial semisupervised framework, in which all labels are inferred from a constructed bipartite graph with exactly k connected components. The proposed method remarkably accelerates the general semisupervised learning through the anchor and consists of four significant parts: 1) obtaining the anchor set as interim through BKHK algorithm; 2) constructing the bipartite graph; 3) solving the self-supervised problem to construct a typical probability model with FSSF; and 4) selecting the most representative points regarding anchors from BKHK as an interim and conducting label propagation. The experimental results on toy examples and benchmark data sets have demonstrated that the proposed method outperforms other approaches.",
        "Breast cancer continues to be a widespread health concern all over the world. Mammography is an important method in the early detection of breast abnormalities. In recent years, using an automatic Computer-Aided Detection (CAD) system based on image processing techniques has been a more reliable interpretation in the illustration of breast distortion. In this study, a fully process-integrated approach with developing a CAD system is presented for the detection of breast masses based on texture description, spectral clustering, and Support Vector Machine (SVM). To this end, breast Regions of Interest (ROIs) are automatically detected from digital mammograms via gray-scale enhancement and data cleansing. The ROIs are segmented as labeled multi-sectional patterns using spectral clustering by the means of intensity descriptors relying on the region's histogram and texture descriptors based on the Gray Level Co-occurrence Matrix (GLCM). In the next step, shape and probabilistic features are derived from the segmented sections and given to the Genetic Algorithm (GA) to do the feature selection. The optimal feature vector comprising a fusion of selected shape and probabilistic features is submitted to linear kernel SVM for robust and reliable classification of mass tissues from the non-mass. Linear discrimination analysis (LDA) is also performed to ascertain the significance of the nominated feature space. The classification results of the proposed approach are presented by sensitivity, specificity, and accuracy measures, which are 89.5%, 91.2%, and 90%, respectively.",
        "Recent advancements in both single-cell RNA-sequencing technology and computational resources facilitate the study of cell types on global populations. Up to millions of cells can now be sequenced in one experiment; thus, accurate and efficient computational methods are needed to provide clustering and post-analysis of assigning putative and rare cell types. Here, we present a novel unsupervised deep learning clustering framework that is robust and highly scalable. To overcome the high level of noise, scAIDE first incorporates an autoencoder-imputation network with a distance-preserved embedding network (AIDE) to learn a good representation of data, and then applies a random projection hashing based k-means algorithm to accommodate the detection of rare cell types. We analyzed a 1.3 million neural cell dataset within 30 min, obtaining 64 clusters which were mapped to 19 putative cell types. In particular, we further identified three different neural stem cell developmental trajectories in these clusters. We also classified two subpopulations of malignant cells in a small glioblastoma dataset using scAIDE. We anticipate that scAIDE would provide a more in-depth understanding of cell development and diseases.",
        "Single-cell RNA sequencing (scRNA-seq) allows researchers to study cell heterogeneity at the cellular level. A crucial step in analyzing scRNA-seq data is to cluster cells into subpopulations to facilitate subsequent downstream analysis. However, frequent dropout events and increasing size of scRNA-seq data make clustering such high-dimensional, sparse and massive transcriptional expression profiles challenging. Although some existing deep learning-based clustering algorithms for single cells combine dimensionality reduction with clustering, they either ignore the distance and affinity constraints between similar cells or make some additional latent space assumptions like mixture Gaussian distribution, failing to learn cluster-friendly low-dimensional space. Therefore, in this paper, we combine the deep learning technique with the use of a denoising autoencoder to characterize scRNA-seq data while propose a soft self-training K-means algorithm to cluster the cell population in the learned latent space. The self-training procedure can effectively aggregate the similar cells and pursue more cluster-friendly latent space. Our method, called 'scziDesk', alternately performs data compression, data reconstruction and soft clustering iteratively, and the results exhibit excellent compatibility and robustness in both simulated and real data. Moreover, our proposed method has perfect scalability in line with cell size on large-scale datasets.",
        "Spectral clustering has become one of the most effective clustering algorithms. We in this work explore the problem of spectral clustering in a lifelong learning framework termed as Generalized Lifelong Spectral Clustering (GL (2)SC). Different from most current studies, which concentrate on a fixed spectral clustering task set and cannot efficiently incorporate a new clustering task, the goal of our work is to establish a generalized model for new spectral clustering task by What and How to lifelong learn from past tasks. For what to lifelong learn, our GL (2)SC framework contains a dual memory mechanism with a deep orthogonal factorization manner: an orthogonal basis memory stores hidden and hierarchical clustering centers among learned tasks, and a feature embedding memory captures deep manifold representation common across multiple related tasks. When a new clustering task arrives, the intuition here for how to lifelong learn is that GL (2)SC can transfer intrinsic knowledge from dual memory mechanism to obtain task-specific encoding matrix. Then the encoding matrix can redefine the dual memory over time to provide maximal benefits when learning future tasks. To the end, empirical comparisons on several benchmark datasets show the effectiveness of our GL (2)SC, in comparison with several state-of-the-art spectral clustering models.",
        "Recent steps towards automation have improved the quality and efficiency of the entire cryo-electron microscopy workflow, from sample preparation to image processing. Most of the image processing steps are now quite automated, but there are still a few steps which need the specific intervention of researchers. One such step is the identification and separation of helical protein polymorphs at early stages of image processing. Here, we tested and evaluated our recent clustering approach on three datasets containing amyloid fibrils, demonstrating that the proposed unsupervised clustering method automatically and effectively identifies the polymorphs from cryo-EM images. As an automated polymorph separation method, it has the potential to complement automated helical picking, which typically cannot easily distinguish between polymorphs with subtle differences in morphology, and is therefore a useful tool for the image processing and structure determination of helical proteins.",
        "Accurate electroencephalogram (EEG) pattern decoding for specific mental tasks is one of the key steps for the development of brain-computer interface (BCI), which is quite challenging due to the considerably low signal-to-noise ratio of EEG collected at the brain scalp. Machine learning provides a promising technique to optimize EEG patterns toward better decoding accuracy. However, existing algorithms do not effectively explore the underlying data structure capturing the true EEG sample distribution and, hence, can only yield a suboptimal decoding accuracy. To uncover the intrinsic distribution structure of EEG data, we propose a clustering-based multitask feature learning algorithm for improved EEG pattern decoding. Specifically, we perform affinity propagation-based clustering to explore the subclasses (i.e., clusters) in each of the original classes and then assign each subclass a unique label based on a one-versus-all encoding strategy. With the encoded label matrix, we devise a novel multitask learning algorithm by exploiting the subclass relationship to jointly optimize the EEG pattern features from the uncovered subclasses. We then train a linear support vector machine with the optimized features for EEG pattern decoding. Extensive experimental studies are conducted on three EEG data sets to validate the effectiveness of our algorithm in comparison with other state-of-the-art approaches. The improved experimental results demonstrate the outstanding superiority of our algorithm, suggesting its prominent performance for EEG pattern decoding in BCI applications.",
        "Wireless sensor networks (WSN) are networks of thousands of nodes installed in a defined physical environment to sense and monitor its state condition. The viability of such a network is directly dependent and limited by the power of batteries supplying the nodes of these networks, which represents a disadvantage of such a network. To improve and extend the life of WSNs, scientists around the world regularly develop various routing protocols that minimize and optimize the energy consumption of sensor network nodes. This article, introduces a new heterogeneous-aware routing protocol well known as Extended Z-SEP Routing Protocol with Hierarchical Clustering Approach for Wireless Heterogeneous Sensor Network or EZ-SEP, where the connection of nodes to a base station (BS) is done via a hybrid method, i.e., a certain amount of nodes communicate with the base station directly, while the remaining ones form a cluster to transfer data. Parameters of the field are unknown, and the field is partitioned into zones depending on the node energy. We reviewed the Z-SEP protocol concerning the election of the cluster head (CH) and its communication with BS and presented a novel extended mechanism for the selection of the CH based on remaining residual energy. In addition, EZ-SEP is weighted up using various estimation schemes such as base station repositioning, altering the field density, and variable nodes energy for comparison with the previous parent algorithm. EZ-SEP was executed and compared to routing protocols such as Z-SEP, SEP, and LEACH. The proposed algorithm performed using the MATLAB R2016b simulator. Simulation results show that our proposed extended version performs better than Z-SEP in the stability period due to an increase in the number of active nodes by 48%, in efficiency of network by the high packet delivery coefficient by 16% and optimizes the average power consumption compared to by 34.",
        "The visual cortex of the mouse brain can be divided into ten or more areas that each contain complete or partial retinotopic maps of the contralateral visual field. It is generally assumed that these areas represent discrete processing regions. In contrast to the conventional input-output characterizations of neuronal responses to standard visual stimuli, here we asked whether six of the core visual areas have responses that are functionally distinct from each other for a given visual stimulus set, by applying machine learning techniques to distinguish the areas based on their activity patterns. Visual areas defined by retinotopic mapping were examined using supervised classifiers applied to responses elicited by a range of stimuli. Using two distinct datasets obtained using wide-field and two-photon imaging, we show that the area labels predicted by the classifiers were highly consistent with the labels obtained using retinotopy. Furthermore, the classifiers were able to model the boundaries of visual areas using resting state cortical responses obtained without any overt stimulus, in both datasets. With the wide-field dataset, clustering neuronal responses using a constrained semi-supervised classifier showed graceful degradation of accuracy. The results suggest that responses from visual cortical areas can be classified effectively using data-driven models. These responses likely reflect unique circuits within each area that give rise to activity with stronger intra-areal than inter-areal correlations, and their responses to controlled visual stimuli across trials drive higher areal classification accuracy than resting state responses.",
        "Single-cell RNA-sequencing (scRNA-seq) explores the transcriptome of genes at cell level, which sheds light on revealing the heterogeneity and dynamics of cell populations. Advances in biotechnologies make it possible to generate scRNA-seq profiles for large-scale cells, requiring effective and efficient clustering algorithms to identify cell types and informative genes. Although great efforts have been devoted to clustering of scRNA-seq, the accuracy, scalability and interpretability of available algorithms are not desirable. In this study, we solve these problems by developing a joint learning algorithm [a.k.a. joints sparse representation and clustering (jSRC)], where the dimension reduction (DR) and clustering are integrated. Specifically, DR is employed for the scalability and joint learning improves accuracy. To increase the interpretability of patterns, we assume that cells within the same type have similar expression patterns, where the sparse representation is imposed on features. We transform clustering of scRNA-seq into an optimization problem and then derive the update rules to optimize the objective of jSRC. Fifteen scRNA-seq datasets from various tissues and organisms are adopted to validate the performance of jSRC, where the number of single cells varies from 49 to 110 824. The experimental results demonstrate that jSRC significantly outperforms 12 state-of-the-art methods in terms of various measurements (on average 20.29% by improvement) with fewer running time. Furthermore, jSRC is efficient and robust across different scRNA-seq datasets from various tissues. Finally, jSRC also accurately identifies dynamic cell types associated with progression of COVID-19. The proposed model and methods provide an effective strategy to analyze scRNA-seq data (the software is coded using MATLAB and is free for academic purposes; https://github.com/xkmaxidian/jSRC).",
        "The success of categorical data clustering generally much relies on the distance metric that measures the dissimilarity degree between two objects. However, most of the existing clustering methods treat the two categorical subtypes, i.e. nominal and ordinal attributes, in the same way when calculating the dissimilarity without considering the relative order information of the ordinal values. Moreover, there would exist interdependence among the nominal and ordinal attributes, which is worth exploring for indicating the dissimilarity. This paper will therefore study the intrinsic difference and connection of nominal and ordinal attribute values from a perspective akin to the graph. Accordingly, we propose a novel distance metric to measure the intra-attribute distances of nominal and ordinal attributes in a unified way, meanwhile preserving the order relationship among ordinal values. Subsequently, we propose a new clustering algorithm to make the learning of intra-attribute distance weights and partitions of data objects into a single learning paradigm rather than two separate steps, whereby circumventing a suboptimal solution. Experiments show the efficacy of the proposed algorithm in comparison with the existing counterparts.",
        "The capacitated arc routing problem (CARP) has attracted much attention for its many practical applications. The large-scale multidepot CARP (LSMDCARP) is an important CARP variant, which is very challenging due to its vast search space. To solve LSMDCARP, we propose an iterative improvement heuristic, called route clustering and search heuristic (RoCaSH). In each iteration, it first (re)decomposes the original LSMDCARP into a set of smaller single-depot CARP subproblems using route cutting off and clustering techniques. Then, it solves each subproblem using the effective Ulusoy's split operator and local search. On one hand, the route clustering helps the search for each subproblem by focusing more on the promising areas. On the other hand, the subproblem solving provides better routes for the subsequent route cutting off and clustering, leading to better problem decomposition. The proposed RoCaSH was compared with the state-of-the-art MDCARP algorithms on a range of MDCARP instances, including different problem sizes. The experimental results showed that RoCaSH significantly outperformed the state-of-the-art algorithms, especially for the large-scale instances. It managed to achieve much better solutions within a much shorter computational time.",
        "Under a dense and large IoT network, a star topology where each device is directly connected to the Internet gateway may cause serious waste of energy and congestion issues. Grouping network devices into clusters provides a suitable architecture to reduce the energy consumption and allows an effective management of communication channels. Although several clustering approaches were proposed in the literature, most of them use the single-hop intra-clustering model. In a large network, the number of clusters increases and the energy draining remains almost the same as in un-clustered architecture. To solve the problem, several approaches use the k-hop intra-clustering to generate a reduced number of large clusters. However, k-hop proposed schemes are, generally, centralized and only assume the node direct neighbors information which lack of robustness. In this regard, the present work proposes a distributed approach for the k-hop intra-clustering called Distributed Clustering based 2-Hop Connectivity (DC2HC). The algorithm uses the two-hop neighbors connectivity to elect the appropriate set of cluster heads and strengthen the clusters connectivity. The objective is to optimize the set of representative cluster heads to minimize the number of long range communication channels and expand the network lifetime. The paper provides the convergence proof of the proposed solution. Simulation results show that our proposed protocol outperforms similar approaches available in the literature by reducing the number of generated cluster heads and achieving longer network lifetime.",
        "In data clustering, the measured data are usually regarded as uncertain data. As a probability-based clustering technique, possible world can easily cluster the uncertain data. However, the method of possible world needs to satisfy two conditions: determine the data of different possible worlds and determine the corresponding probability of occurrence. The existing methods mostly make multiple measurements and treat each measurement as deterministic data of a possible world. In this paper, a possible world-based fusion estimation model is proposed, which changes the deterministic data into probability distribution according to the estimation algorithm, and the corresponding probability can be confirmed naturally. Further, in the clustering stage, the Kullback-Leibler divergence is introduced to describe the relationships of probability distributions among different possible worlds. Then, an application in wearable body networks (WBNs) is given, and some interesting conclusions are shown. Finally, simulations show better performance when the relationships between features in measured data are more complex.",
        "BACKGROUND: The mechanisms of improvement of left ventricular (LV) function with cardiac resynchronization therapy (CRT) are not yet elucidated. The aim of this study was to characterize CRT responder profiles through clustering analysis, on the basis of clinical and echocardiographic preimplantation data, integrating automatic quantification of longitudinal strain signals. METHODS: This was a multicenter observational study of 250 patients with chronic heart failure evaluated before CRT device implantation and followed up to 4 years. Clinical, electrocardiographic, and echocardiographic data were collected. Regional longitudinal strain signals were also analyzed with custom-made algorithms in addition to existing approaches, including myocardial work indices. Response was defined as a decrease of >/=15% in LV end-systolic volume. Death and hospitalization for heart failure at 4 years were considered adverse events. Seventy features were analyzed using a clustering approach (k-means clustering). RESULTS: Five clusters were identified, with response rates between 50% in cluster 1 and 92.7% in cluster 5. These five clusters differed mainly by the characteristics of LV mechanics, evaluated using strain integrals. There was a significant difference in event-free survival at 4 years between cluster 1 and the other clusters. The quantitative analysis of strain curves, especially in the lateral wall, was more discriminative than apical rocking, septal flash, or myocardial work in most phenogroups. CONCLUSIONS: Five clusters are described, defining groups of below-average to excellent responders to CRT. These clusters demonstrate the complexity of LV mechanics and prediction of response to CRT. Automatic quantitative analysis of longitudinal strain curves appears to be a promising tool to improve the understanding of LV mechanics, patient characterization, and selection for CRT.",
        "Image processing plays a major role in neurologists' clinical diagnosis in the medical field. Several types of imagery are used for diagnostics, tumor segmentation, and classification. Magnetic resonance imaging (MRI) is favored among all modalities due to its noninvasive nature and better representation of internal tumor information. Indeed, early diagnosis may increase the chances of being lifesaving. However, the manual dissection and classification of brain tumors based on MRI is vulnerable to error, time-consuming, and formidable task. Consequently, this article presents a deep learning approach to classify brain tumors using an MRI data analysis to assist practitioners. The recommended method comprises three main phases: preprocessing, brain tumor segmentation using k-means clustering, and finally, classify tumors into their respective categories (benign/malignant) using MRI data through a finetuned VGG19 (i.e., 19 layered Visual Geometric Group) model. Moreover, for better classification accuracy, the synthetic data augmentation concept i s introduced to increase available data size for classifier training. The proposed approach was evaluated on BraTS 2015 benchmarks data sets through rigorous experiments. The results endorse the effectiveness of the proposed strategy and it achieved better accuracy compared to the previously reported state of the art techniques.",
        "Cross-manifold clustering is an extreme challenge learning problem. Since the low-density hypothesis is not satisfied in cross-manifold problems, many traditional clustering methods failed to discover the cross-manifold structures. In this article, we propose multiple flat projections clustering (MFPC) for cross-manifold clustering. In our MFPC, the given samples are projected into multiple localized flats to discover the global structures of implicit manifolds. Thus, the intersected clusters are distinguished in various projection flats. In MFPC, a series of nonconvex matrix optimization problems is solved by a proposed recursive algorithm. Furthermore, a nonlinear version of MFPC is extended via kernel tricks to deal with a more complex cross-manifold learning situation. The synthetic tests show that our MFPC works on the cross-manifold structures well. Moreover, experimental results on the benchmark datasets and object tracking videos show excellent performance of our MFPC compared with some state-of-the-art manifold clustering methods.",
        "BACKGROUND: Generating and analysing single-cell data has become a widespread approach to examine tissue heterogeneity, and numerous algorithms exist for clustering these datasets to identify putative cell types with shared transcriptomic signatures. However, many of these clustering workflows rely on user-tuned parameter values, tailored to each dataset, to identify a set of biologically relevant clusters. Whereas users often develop their own intuition as to the optimal range of parameters for clustering on each data set, the lack of systematic approaches to identify this range can be daunting to new users of any given workflow. In addition, an optimal parameter set does not guarantee that all clusters are equally well-resolved, given the heterogeneity in transcriptomic signatures in most biological systems. RESULTS: Here, we illustrate a subsampling-based approach (chooseR) that simultaneously guides parameter selection and characterizes cluster robustness. Through bootstrapped iterative clustering across a range of parameters, chooseR was used to select parameter values for two distinct clustering workflows (Seurat and scVI). In each case, chooseR identified parameters that produced biologically relevant clusters from both well-characterized (human PBMC) and complex (mouse spinal cord) datasets. Moreover, it provided a simple \"robustness score\" for each of these clusters, facilitating the assessment of cluster quality. CONCLUSION: chooseR is a simple, conceptually understandable tool that can be used flexibly across clustering algorithms, workflows, and datasets to guide clustering parameter selection and characterize cluster robustness.",
        "Autofluorescence microscopy is a promising label-free approach to characterize NADH and FAD metabolites in live cells, with potential applications in clinical practice. Although spectrally resolved lifetime imaging techniques can acquire multiparametric information about the biophysical and biochemical state of the metabolites, these data are evaluated at the whole-cell level, thus providing only limited insights in the activation of metabolic networks at the microscale. To overcome this issue, here we introduce an artificial intelligence-based analysis that, leveraging the multiparametric content of spectrally resolved lifetime images, allows to detect and classify, through an unsupervised learning approach, metabolic clusters, which are regions having almost uniform metabolic properties. This method contextually detects the cellular mitochondrial turnover and the metabolic activation state of intracellular compartments at the pixel level, described by two functions: the cytosolic activation state (CAF) and the mitochondrial activation state (MAF). This method was applied to investigate metabolic changes elicited in the breast cancer cell line MCF-7 by specific inhibitors of glycolysis and electron transport chain, and by the deregulation of a specific mitochondrial enzyme (ACO2) leading to defective aerobic metabolism associated with tumor growth. In this model, mitochondrial fraction undergoes to a 13% increase upon ACO2 overexpression and the MAF function changes abruptly by altering the metabolic state of about the 25% of the mitochondrial pixels.",
        "MOTIVATION: Many 'automated gating' algorithms now exist to cluster cytometry and single cell sequencing data into discrete populations. Comparative algorithm evaluations on benchmark datasets rely either on a single performance metric, or a few metrics considered independently of one another. However, single metrics emphasise different aspects of clustering performance and do not rank clustering solutions in the same order. This underlies the lack of consensus between comparative studies regarding optimal clustering algorithms and undermines the translatability of results onto other non-benchmark datasets. RESULTS: We propose the Pareto fronts framework as an integrative evaluation protocol, wherein individual metrics are instead leveraged as complementary perspectives. Judged superior are algorithms that provide the best trade-off between the multiple metrics considered simultaneously. This yields a more comprehensive and complete view of clustering performance. Moreover, by broadly and systematically sampling algorithm parameter values using the Latin Hypercube sampling method, our evaluation protocol minimises (un)fortunate parameter value selections as confounding factors. Furthermore, it reveals how meticulously each algorithm must be tuned in order to obtain good results, vital knowledge for users with novel data. We exemplify the protocol by conducting a comparative study between three clustering algorithms (ChronoClust, FlowSOM and Phenograph) using four common performance metrics applied across four cytometry benchmark datasets. To our knowledge, this is the first time Pareto fronts have been used to evaluate the performance of clustering algorithms in any application domain. AVAILABILITY: Implementation of our Pareto front methodology and all scripts to reproduce this article are available at https://github.com/ghar1821/ParetoBench.",
        "Single-cell RNA-Sequencing (scRNA-seq) is the most widely used high-throughput technology to measure genome-wide gene expression at the single-cell level. One of the most common analyses of scRNA-seq data detects distinct subpopulations of cells through the use of unsupervised clustering algorithms. However, recent advances in scRNA-seq technologies result in current datasets ranging from thousands to millions of cells. Popular clustering algorithms, such as k-means, typically require the data to be loaded entirely into memory and therefore can be slow or impossible to run with large datasets. To address this problem, we developed the mbkmeans R/Bioconductor package, an open-source implementation of the mini-batch k-means algorithm. Our package allows for on-disk data representations, such as the common HDF5 file format widely used for single-cell data, that do not require all the data to be loaded into memory at one time. We demonstrate the performance of the mbkmeans package using large datasets, including one with 1.3 million cells. We also highlight and compare the computing performance of mbkmeans against the standard implementation of k-means and other popular single-cell clustering methods. Our software package is available in Bioconductor at https://bioconductor.org/packages/mbkmeans.",
        "Clustering is a common method to identify cell types in single cell analysis, but the increasing size of scRNA-seq datasets brings challenges to single cell clustering. Therefore, it is an urgent need to design a faster and more accurate clustering method for large-scale scRNA-seq data. In this paper, we proposed a new method for single cell clustering. First, a count matrix is constructed through normalization and gene filtration. Second, the raw data of gene expression matrix are projected to feature space constructed by secondary construction of feature space based on UMAP (Uniform Manifold Approximation and Projection). Third, the low-dimensional matrix on the feature space is randomly divided into two sub-matrices according to a certain proportion for clustering and classifying, respectively. Finally, one subset is clustered by k-means algorithm and then the other subset is classified by k-nearest neighbor algorithm based on clustering results. Experimental results show that our method can cluster the scRNA-seq datasets effectively.",
        "BACKGROUND: CKD is a heterogeneous condition with multiple underlying causes, risk factors, and outcomes. Subtyping CKD with multidimensional patient data holds the key to precision medicine. Consensus clustering may reveal CKD subgroups with different risk profiles of adverse outcomes. METHODS: We used unsupervised consensus clustering on 72 baseline characteristics among 2696 participants in the prospective Chronic Renal Insufficiency Cohort (CRIC) study to identify novel CKD subgroups that best represent the data pattern. Calculation of the standardized difference of each parameter used the cutoff of +/-0.3 to show subgroup features. CKD subgroup associations were examined with the clinical end points of kidney failure, the composite outcome of cardiovascular diseases, and death. RESULTS: The algorithm revealed three unique CKD subgroups that best represented patients' baseline characteristics. Patients with relatively favorable levels of bone density and cardiac and kidney function markers, with lower prevalence of diabetes and obesity, and who used fewer medications formed cluster 1 (n=1203). Patients with higher prevalence of diabetes and obesity and who used more medications formed cluster 2 (n=1098). Patients with less favorable levels of bone mineral density, poor cardiac and kidney function markers, and inflammation delineated cluster 3 (n=395). These three subgroups, when linked with future clinical end points, were associated with different risks of CKD progression, cardiovascular disease, and death. Furthermore, patient heterogeneity among predefined subgroups with similar baseline kidney function emerged. CONCLUSIONS: Consensus clustering synthesized the patterns of baseline clinical and laboratory measures and revealed distinct CKD subgroups, which were associated with markedly different risks of important clinical outcomes. Further examination of patient subgroups and associated biomarkers may provide next steps toward precision medicine.",
        "Existing text clustering methods utilize only one representation at a time (single view), whereas multiple views can represent documents. The multiview multirepresentation method enhances clustering quality. Moreover, existing clustering methods that utilize more than one representation at a time (multiview) use representation with the same nature. Hence, using multiple views that represent data in a different representation with clustering methods is reasonable to create a diverse set of candidate clustering solutions. On this basis, an effective dynamic clustering method must consider combining multiple views of data including semantic view, lexical view (word weighting), and topic view as well as the number of clusters. The main goal of this study is to develop a new method that can improve the performance of web search result clustering (WSRC). An enhanced multiview multirepresentation consensus clustering ensemble (MMCC) method is proposed to create a set of diverse candidate solutions and select a high-quality overlapping cluster. The overlapping clusters are obtained from the candidate solutions created by different clustering methods. The framework to develop the proposed MMCC includes numerous stages: (1) acquiring the standard datasets (MORESQUE and Open Directory Project-239), which are used to validate search result clustering algorithms, (2) preprocessing the dataset, (3) applying multiview multirepresentation clustering models, (4) using the radius-based cluster number estimation algorithm, and (5) employing the consensus clustering ensemble method. Results show an improvement in clustering methods when multiview multirepresentation is used. More importantly, the proposed MMCC model improves the overall performance of WSRC compared with all single-view clustering models.",
        "OBJECTIVE: To perform cluster analysis of MRI signs of cerebral microangiopathy (small vessel disease, SVD) and to clarify the relationship between the isolated groups and circulating markers of inflammation and angiogenesis. MATERIAL AND METHODS: The identification of groups of MRI signs (MRI types) using cluster hierarchical agglomerative analysis and iterative algorithm of k-means and assessment of their relationship with serum concentrations of tumor necrosis factor-alpha (TNF-alpha), transforming growth factor-beta1 (TGF-beta1), vascular endothelial growth factor-A (VEGF-A), hypoxia-inducible factor 1-alpha (HIF1-alpha) determined by ELISA were performed in 96 patients with SVD (STRIVE, 2013) (65 women, average age 60.91+/-6.57 years). RESULTS: Cluster analysis of MRI signs identified two MRI types of SVD with Fazekas grade 3 of white matter hyperintensity (WMH). MRI type 1 (n=18; 6 women, mean age 59.1+/-6.8 years) and MRI type 2 (n=22, 15 f., mean age 63.5+/-6.2 years) did not differ by age, sex, severity of hypertension, presence of other risk factors. MRI type 1 had a statistically significantly more pronounced WMH in the periventricular regions, multiple lacunes and microbleeds, atrophy, severe cognitive impairment and gait disorders compared with MRI type 2. Its formation was associated with a decrease in VEGF-A level. MRI type 2 had the significantly more pronounced juxtacortical WMH, white matter lacunes, in the absence of microbleeds and atrophy, and less severe clinical manifestations compared with MRI type 1. Its formation was associated with an increase in TNF-alpha level. CONCLUSION: Clustering of diagnostic MRI signs into MRI types of SVD with significant differences in the severity of clinical manifestations suggests the pathogenetic heterogeneity of age-related SVD. The relationship of MRI types with circulating markers of different mechanisms of vascular wall and brain damage indicates the dominant role of depletion of angiogenesis in the formation of MRI type 1 and increased inflammation in the formation of MRI type 2. Further studies are needed to clarify the criteria and diagnostic value of differentiation of MRI types of SVD, and also their mechanisms with the definition of pathogenetically justified prevention and treatment of various forms of SVD.",
        "Extracellular vesicles (EV) are biological nanoparticles that play an important role in cell-to-cell communication. The phenotypic profile of EV populations is a promising reporter of disease, with direct clinical diagnostic relevance. Yet, robust methods for quantifying the biomarker content of EV have been critically lacking, and require a single-particle approach due to their inherent heterogeneous nature. Here, multicolor single-molecule burst analysis microscopy is used to detect multiple biomarkers present on single EV. The authors classify the recorded signals and apply the machine learning-based t-distributed stochastic neighbor embedding algorithm to cluster the resulting multidimensional data. As a proof of principle, the authors use the method to assess both the purity and the inflammatory status of EV, and compare cell culture and plasma-derived EV isolated via different purification methods. This methodology is then applied to identify intercellular adhesion molecule-1 specific EV subgroups released by inflamed endothelial cells, and to prove that apolipoprotein-a1 is an excellent marker to identify the typical lipoprotein contamination in plasma. This methodology can be widely applied on standard confocal microscopes, thereby allowing both standardized quality assessment of patient plasma EV preparations, and diagnostic profiling of multiple EV biomarkers in health and disease.",
        "BACKGROUND: Cystic fibrosis (CF) is a multisystem disease in which the assessment of disease severity based on lung function alone may not be appropriate. The aim of the study was to develop a comprehensive machine-learning algorithm to assess clinical status independent of lung function in children. METHODS: A comprehensive prospectively collected clinical database (Toronto, Canada) was used to apply unsupervised cluster analysis. The defined clusters were then compared by current and future lung function, risk of future hospitalisation, and risk of future pulmonary exacerbation treated with oral antibiotics. A k-nearest-neighbours (KNN) algorithm was used to prospectively assign clusters. The methods were validated in a paediatric clinical CF dataset from Great Ormond Street Hospital (GOSH). RESULTS: The optimal cluster model identified four (A-D) phenotypic clusters based on 12 200 encounters from 530 individuals. Two clusters (A and B) consistent with mild disease were identified with high forced expiratory volume in 1 s (FEV1), and low risk of both hospitalisation and pulmonary exacerbation treated with oral antibiotics. Two clusters (C and D) consistent with severe disease were also identified with low FEV1. Cluster D had the shortest time to both hospitalisation and pulmonary exacerbation treated with oral antibiotics. The outcomes were consistent in 3124 encounters from 171 children at GOSH. The KNN cluster allocation error rate was low, at 2.5% (Toronto) and 3.5% (GOSH). CONCLUSION: Machine learning derived phenotypic clusters can predict disease severity independent of lung function and could be used in conjunction with functional measures to predict future disease trajectories in CF patients.",
        "Among biological networks, co-expression networks have been widely studied. One of the most commonly used pipelines for the construction of co-expression networks is weighted gene co-expression network analysis (WGCNA), which can identify highly co-expressed clusters of genes (modules). WGCNA identifies gene modules using hierarchical clustering. The major drawback of hierarchical clustering is that once two objects are clustered together, it cannot be reversed; thus, re-adjustment of the unbefitting decision is impossible. In this paper, we calculate the similarity matrix with the distance correlation for WGCNA to construct a gene co-expression network, and present a new approach called the k-module algorithm to improve the WGCNA clustering results. This method can assign all genes to the module with the highest mean connectivity with these genes. This algorithm re-adjusts the results of hierarchical clustering while retaining the advantages of the dynamic tree cut method. The validity of the algorithm is verified using six datasets from microarray and RNA-seq data. The k-module algorithm has fewer iterations, which leads to lower complexity. We verify that the gene modules obtained by the k-module algorithm have high enrichment scores and strong stability. Our method improves upon hierarchical clustering, and can be applied to general clustering algorithms based on the similarity matrix, not limited to gene co-expression network analysis.",
        "Affinity propagation (AP) clustering with low complexity and high performance is suitable for radio remote head (RRH) clustering for real-time joint transmission in the cloud radio access network. The existing AP algorithms for joint transmission have the limitation of high computational complexities owing to re-sweeping preferences (diagonal components of the similarity matrix) to determine the optimal number of clusters as system parameters such as network topology. To overcome this limitation, we propose a new approach in which preferences are fixed, where the threshold changes in response to the variations in system parameters. In AP clustering, each diagonal value of a final converged matrix is mapped to the position (x,y coordinates) of a corresponding RRH to form two-dimensional image. Furthermore, an environment-adaptive threshold value is determined by adopting Otsu's method, which uses the gray-scale histogram of the image to make a statistical decision. Additionally, a simple greedy merging algorithm is proposed to resolve the problem of inter-cluster interference owing to the adjacent RRHs selected as exemplars (cluster centers). For a realistic performance assessment, both grid and uniform network topologies are considered, including exterior interference and various transmitting power levels of an RRH. It is demonstrated that with similar normalized execution times, the proposed algorithm provides better spectral and energy efficiencies than those of the existing algorithms.",
        "Magnetically guided self-assembly of nanoparticles is a promising bottom-up method to fabricate novel materials and superstructures, such as, for example, magnetic nanoparticle clusters for biomedical applications. The existence of assembled structures has been verified by numerous experiments, yet a comprehensive theoretical framework to explore design possibilities and predict emerging properties is missing. Here we present a model of magnetic nanoparticle interactions built upon a Langevin dynamics algorithm to simulate the time evolution and aggregation of colloidal suspensions. We recognise three main aggregation regimes: non-aggregated, linear and clustered. Through systematic simulations we have revealed the link between single particle parameters and which aggregates are formed, both in terms of the three regimes and the chance of finding specific aggregates, which we characterise by nanoparticle arrangement and net magnetic moment. Our findings are shown to agree with past experiments and may serve as a stepping stone to guide the design and interpretation of future studies.",
        "Clustering nonlinearly separable datasets is always an important problem in unsupervised machine learning. Graph cut models provide good clustering results for nonlinearly separable datasets, but solving graph cut models is an NP hard problem. A novel graph-based clustering algorithm is proposed for nonlinearly separable datasets. The proposed method solves the min cut model by iteratively computing only one simple formula. Experimental results on synthetic and benchmark datasets indicate the potential of the proposed method, which is able to cluster nonlinearly separable datasets with less running time.",
        "Community detection in social networks is one of the advertising methods in electronic marketing. One of the approaches to find communities in large social networks is to use greedy methods, because these methods perform very fast. Greedy methods are generally designed based on local decisions; thus, inappropriate local decisions may result in an improper global solution. The use of a greedy improved index with a futuristic approach can, to some extent, prevent inappropriate local choices. Our proposed method determines the influential nodes in the social network based on the followers and following and new futuristic greedy index. It classifies the nodes based on the influential nodes by the density-based clustering algorithm with a new distance function. The proposed method can improve clustering precision to detect communities by the futuristic greedy approach. We implemented the proposed algorithm with the map-reduce technique in the Hadoop structure. Experimental results in datasets show that the average of the rand index of clusters was accomplished by 99.32% in the proposed method. In addition, these results illustrate that there is a reduction in execution time by the proposed algorithm.",
        "BACKGROUND: Single-cell RNA-Sequencing (scRNA-Seq) has provided single-cell level insights into complex biological processes. However, the high frequency of gene expression detection failures in scRNA-Seq data make it challenging to achieve reliable identification of cell-types and Differentially Expressed Genes (DEG). Moreover, with the explosive growth of single-cell data using 10x genomics protocol, existing methods will soon reach the computation limit due to scalability issues. The single-cell transcriptomics field desperately need new tools and framework to facilitate large-scale single-cell analysis. RESULTS: In order to improve the accuracy, robustness, and speed of scRNA-Seq data processing, we propose a generalized zero-inflated negative binomial mixture model, \"JOINT,\" that can perform probability-based cell-type discovery and DEG analysis simultaneously without the need for imputation. JOINT performs soft-clustering for cell-type identification by computing the probability of individual cells, i.e. each cell can belong to multiple cell types with different probabilities. This is drastically different from existing hard-clustering methods where each cell can only belong to one cell type. The soft-clustering component of the algorithm significantly facilitates the accuracy and robustness of single-cell analysis, especially when the scRNA-Seq datasets are noisy and contain a large number of dropout events. Moreover, JOINT is able to determine the optimal number of cell-types automatically rather than specifying it empirically. The proposed model is an unsupervised learning problem which is solved by using the Expectation and Maximization (EM) algorithm. The EM algorithm is implemented using the TensorFlow deep learning framework, dramatically accelerating the speed for data analysis through parallel GPU computing. CONCLUSIONS: Taken together, the JOINT algorithm is accurate and efficient for large-scale scRNA-Seq data analysis via parallel computing. The Python package that we have developed can be readily applied to aid future advances in parallel computing-based single-cell algorithms and research in various biological and biomedical fields.",
        "The vertical collaborative clustering aims to unravel the hidden structure of data (similarity) among different sites, which will help data owners to make a smart decision without sharing actual data. For example, various hospitals located in different regions want to investigate the structure of common disease among people of different populations to identify latent causes without sharing actual data with other hospitals. Similarly, a chain of regional educational institutions wants to evaluate their students' performance belonging to different regions based on common latent constructs. The available methods used for finding hidden structures are complicated and biased to perform collaboration in measuring similarity among multiple sites. This study proposes vertical collaborative clustering using a bit plane slicing approach (VCC-BPS), which is simple and unique with improved accuracy, manages collaboration among various data sites. The VCC-BPS transforms data from input space to code space, capturing maximum similarity locally and collaboratively at a particular bit plane. The findings of this study highlight the significance of those particular bits which fit the model in correctly classifying class labels locally and collaboratively. Thenceforth, the data owner appraises local and collaborative results to reach a better decision. The VCC-BPS is validated by Geyser, Skin and Iris datasets and its results are compared with the composite dataset. It is found that the VCC-BPS outperforms existing solutions with improved accuracy in term of purity and Davies-Boulding index to manage collaboration among different data sites. It also performs data compression by representing a large number of observations with a small number of data symbols.",
        "Coronavirus pandemic (COVID-19) has infected more than ten million persons worldwide. Therefore, researchers are trying to address various aspects that may help in diagnosis this pneumonia. Image segmentation is a necessary pr-processing step that implemented in image analysis and classification applications. Therefore, in this study, our goal is to present an efficient image segmentation method for COVID-19 Computed Tomography (CT) images. The proposed image segmentation method depends on improving the density peaks clustering (DPC) using generalized extreme value (GEV) distribution. The DPC is faster than other clustering methods, and it provides more stable results. However, it is difficult to determine the optimal number of clustering centers automatically without visualization. So, GEV is used to determine the suitable threshold value to find the optimal number of clustering centers that lead to improving the segmentation process. The proposed model is applied for a set of twelve COVID-19 CT images. Also, it was compared with traditional k-means and DPC algorithms, and it has better performance using several measures, such as PSNR, SSIM, and Entropy.",
        "While application of clustering algorithms to atom probe tomography data have enabled quantification of solute clusters in terms of number density, size, and subcomposition there exist other properties (e.g., volume, surface area, and composition) that are better determined by defining an interface between the cluster and the surrounding matrix. The limitation in composition results from an ion selection step where the expected matrix ion types are omitted from the cluster search algorithm to enhance the contrast between the matrix and cluster and to reduce the complexity of the search. Previously, composition determination within solute clusters has utilized a secondary envelopment and erosion step on top of conventional methods such as maximum separation. In this work, we present a novel stochastic method that combines the particle identification fidelity of a conventional clustering algorithm with the analytical flexibility of mesh-based approaches through the generation of alpha shapes for each identified cluster. The corresponding mesh accounts for concave components of the clusters and determines the volume and surface area of the clusters; additionally, the mesh boundary is utilized to update the total composition according to the internal ions.",
        "BACKGROUND: The ability to report complete, accurate and timely data by HIV care providers and other entities is a key aspect in monitoring trends in HIV prevention, treatment and care, hence contributing to its eradication. In many low-middle-income-countries (LMICs), aggregate HIV data reporting is done through the District Health Information Software 2 (DHIS2). Nevertheless, despite a long-standing requirement to report HIV-indicator data to DHIS2 in LMICs, few rigorous evaluations exist to evaluate adequacy of health facility reporting at meeting completeness and timeliness requirements over time. The aim of this study is to conduct a comprehensive assessment of the reporting status for HIV-indicators, from the time of DHIS2 implementation, using Kenya as a case study. METHODS: A retrospective observational study was conducted to assess reporting performance of health facilities providing any of the HIV services in all 47 counties in Kenya between 2011 and 2018. Using data extracted from DHIS2, K-means clustering algorithm was used to identify homogeneous groups of health facilities based on their performance in meeting timeliness and completeness facility reporting requirements for each of the six programmatic areas. Average silhouette coefficient was used in measuring the quality of the selected clusters. RESULTS: Based on percentage average facility reporting completeness and timeliness, four homogeneous groups of facilities were identified namely: best performers, average performers, poor performers and outlier performers. Apart from blood safety reports, a distinct pattern was observed in five of the remaining reports, with the proportion of best performing facilities increasing and the proportion of poor performing facilities decreasing over time. However, between 2016 and 2018, the proportion of best performers declined in some of the programmatic areas. Over the study period, no distinct pattern or trend in proportion changes was observed among facilities in the average and outlier groups. CONCLUSIONS: The identified clusters revealed general improvements in reporting performance in the various reporting areas over time, but with noticeable decrease in some areas between 2016 and 2018. This signifies the need for continuous performance monitoring with possible integration of machine learning and visualization approaches into national HIV reporting systems.",
        "Recent studies have shown that in-depth studies on epi-transcriptomic patterns of N6-methyladenosine (m6A) may be helpful to understand its complex functions and co-regulatory mechanisms. Since most biclustering algorithms are developed in scenarios of gene expression analysis, which does not share the same characteristics with m6A methylation profile, we propose a weighted Plaid bi-clustering model (FBCwPlaid) based on Lagrange multiplier method to discover the potential functional patterns. The model seeks for one bi-cluster each time. Thus, the goal of each time turns into a binary classification problem. It initializes model parameters by k-means clustering, and then updates the parameters of the Plaid model. To address the issue that site expression level determines methylation level confidence, it uses RNA expression levels of each site as weights to make lower expressed sites less confident. FBCwPlaid also allows overlapping bi-clusters, indicating some sites may participate in multiple biological functions. FBCwPlaid was then applied on MeRIP-seq data of 69,446 methylation sites under 32 experimental conditions. Finally, 3 patterns were discovered, and further pathway analysis and enzyme specificity test showed that sites involved in each pattern are highly relevant to m6A methyltransferases. Further detailed analyses even showed that some patterns are condition relevant.",
        "Hard X-ray nanodiffraction provides a unique nondestructive technique to quantify local strain and structural inhomogeneities at nanometer length scales. However, sample mosaicity and phase separation can result in a complex diffraction pattern that can make it challenging to quantify nanoscale structural distortions. In this work, a k-means clustering algorithm was utilized to identify local maxima of intensity by partitioning diffraction data in a three-dimensional feature space of detector coordinates and intensity. This technique has been applied to X-ray nanodiffraction measurements of a patterned ferroelectric PbZr0.2Ti0.8O3 sample. The analysis reveals the presence of two phases in the sample with different lattice parameters. A highly heterogeneous distribution of lattice parameters with a variation of 0.02 A was also observed within one ferroelectric domain. This approach provides a nanoscale survey of subtle structural distortions as well as phase separation in ferroelectric domains in a patterned sample.",
        "Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has a worldwide devastating effect. The understanding of evolution and transmission of SARS-CoV-2 is of paramount importance for the COVID-19 control, combating, and prevention. Due to the rapid growth of both the number of SARS-CoV-2 genome sequences and the number of unique mutations, the phylogenetic analysis of SARS-CoV-2 genome isolates faces an emergent large-data challenge. We introduce a dimension-reduced $k$-means clustering strategy to tackle this challenge. We examine the performance and effectiveness of three dimension-reduction algorithms: principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP). By using four benchmark datasets, we found that UMAP is the best-suited technique due to its stable, reliable, and efficient performance, its ability to improve clustering accuracy, especially for large Jaccard distanced-based datasets, and its superior clustering visualization. The UMAP-assisted $k$-means clustering enables us to shed light on increasingly large datasets from SARS-CoV-2 genome isolates.",
        "In this study, we propose a deep-learning network model called the deep multi-kernel auto-encoder clustering network (DMACN) for clustering functional connectivity data for brain diseases. This model is an end-to-end clustering algorithm that can learn potentially advanced features and cluster disease categories. Unlike other auto-encoders, DMACN has an added self-expression layer and standard back-propagation is used to learn the features that are beneficial for clustering brain functional connectivity data. In the self-expression layer, the kernel matrix is constructed to extract effective features and a new loss function is proposed to constrain the clustering portion, which enables the training of a deep neural learning network that tends to cluster. To test the performance of the proposed algorithm, we applied the end-to-end deep unsupervised clustering algorithm to brain connectivity data. We then conducted experiments based on four public brain functional connectivity data sets and our own functional connectivity data set. The DMACN algorithm yielded good results in various evaluations compared with the existing clustering algorithm for brain functional connectivity data, the deep auto-encoder clustering algorithm, and several other relevant clustering algorithms. The deep-learning-based clustering algorithm has great potential for use in the unsupervised recognition of brain diseases.",
        "BACKGROUND: Protein-peptide interactions play a fundamental role in a wide variety of biological processes, such as cell signaling, regulatory networks, immune responses, and enzyme inhibition. Peptides are characterized by low toxicity and small interface areas; therefore, they are good targets for therapeutic strategies, rational drug planning and protein inhibition. Approximately 10% of the ethical pharmaceutical market is protein/peptide-based. Furthermore, it is estimated that 40% of protein interactions are mediated by peptides. Despite the fast increase in the volume of biological data, particularly on sequences and structures, there remains a lack of broad and comprehensive protein-peptide databases and tools that allow the retrieval, characterization and understanding of protein-peptide recognition and consequently support peptide design. RESULTS: We introduce Propedia, a comprehensive and up-to-date database with a web interface that permits clustering, searching and visualizing of protein-peptide complexes according to varied criteria. Propedia comprises over 19,000 high-resolution structures from the Protein Data Bank including structural and sequence information from protein-peptide complexes. The main advantage of Propedia over other peptide databases is that it allows a more comprehensive analysis of similarity and redundancy. It was constructed based on a hybrid clustering algorithm that compares and groups peptides by sequences, interface structures and binding sites. Propedia is available through a graphical, user-friendly and functional interface where users can retrieve, and analyze complexes and download each search data set. We performed case studies and verified that the utility of Propedia scores to rank promissing interacting peptides. In a study involving predicting peptides to inhibit SARS-CoV-2 main protease, we showed that Propedia scores related to similarity between different peptide complexes with SARS-CoV-2 main protease are in agreement with molecular dynamics free energy calculation. CONCLUSIONS: Propedia is a database and tool to support structure-based rational design of peptides for special purposes. Protein-peptide interactions can be useful to predict, classifying and scoring complexes or for designing new molecules as well. Propedia is up-to-date as a ready-to-use webserver with a friendly and resourceful interface and is available at: https://bioinfo.dcc.ufmg.br/propedia.",
        "MOTIVATION: Single cell data measures multiple cellular markers at the single-cell level for thousands to millions of cells. Identification of distinct cell populations is a key step for further biological understanding, usually performed by clustering this data. Dimensionality reduction based clustering tools are either not scalable to large datasets containing millions of cells, or not fully automated requiring an initial manual estimation of the number of clusters. Graph clustering tools provide automated and reliable clustering for single cell data, but suffer heavily from scalability to large datasets. RESULTS: We developed SCHNEL, a scalable, reliable and automated clustering tool for high-dimensional single-cell data. SCHNEL transforms large high-dimensional data to a hierarchy of datasets containing subsets of data points following the original data manifold. The novel approach of SCHNEL combines this hierarchical representation of the data with graph clustering, making graph clustering scalable to millions of cells. Using seven different cytometry datasets, SCHNEL outperformed three popular clustering tools for cytometry data, and was able to produce meaningful clustering results for datasets of 3.5 and 17.2 million cells within workable time frames. In addition, we show that SCHNEL is a general clustering tool by applying it to single-cell RNA sequencing data, as well as a popular machine learning benchmark dataset MNIST. AVAILABILITY AND IMPLEMENTATION: Implementation is available on GitHub (https://github.com/biovault/SCHNELpy). All datasets used in this study are publicly available. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "The gradual decline in routine patterns is a major symptom of early-stage dementia, therefore an unobtrusive real-life assessment of the elder's routine can potentially be of significant clinical importance. This article focuses on the assessment of changes in a person's daily routine using longitudinal data recorded from a network of nonintrusive motion sensors in a smart home environment. In this article, we propose to identify repeating patterns in a person's daily routine over the span of multiple days using hierarchical clustering algorithms, which provide an effective way to mitigate noise artifacts and confounding factors that contribute to the momentary variability of the sensor data. We have evaluated our proposed algorithm on both synthetic and real-world data recorded in the span of 50-100 days from four elderly adults. Our results indicate that the proposed hierarchical clustering approach can more reliably capture the gradual change in the degree of routineness compared to baseline approaches that measure the similarity between two consecutive days or capture variations in the occurrence of recognized activities.",
        "BACKGROUND: In molecular epidemiology, comparison of intra-host viral variants among infected persons is frequently used for tracing transmissions in human population and detecting viral infection outbreaks. Application of Ultra-Deep Sequencing (UDS) immensely increases the sensitivity of transmission detection but brings considerable computational challenges when comparing all pairs of sequences. We developed a new population comparison method based on convex hulls in hamming space. We applied this method to a large set of UDS samples obtained from unrelated cases infected with hepatitis C virus (HCV) and compared its performance with three previously published methods. RESULTS: The convex hull in hamming space is a data structure that provides information on: (1) average hamming distance within the set, (2) average hamming distance between two sets; (3) closeness centrality of each sequence; and (4) lower and upper bound of all the pairwise distances among the members of two sets. This filtering strategy rapidly and correctly removes 96.2% of all pairwise HCV sample comparisons, outperforming all previous methods. The convex hull distance (CHD) algorithm showed variable performance depending on sequence heterogeneity of the studied populations in real and simulated datasets, suggesting the possibility of using clustering methods to improve the performance. To address this issue, we developed a new clustering algorithm, k-hulls, that reduces heterogeneity of the convex hull. This efficient algorithm is an extension of the k-means algorithm and can be used with any type of categorical data. It is 6.8-times more accurate than k-mode, a previously developed clustering algorithm for categorical data. CONCLUSIONS: CHD is a fast and efficient filtering strategy for massively reducing the computational burden of pairwise comparison among large samples of sequences, and thus, aiding the calculation of transmission links among infected individuals using threshold-based methods. In addition, the convex hull efficiently obtains important summary metrics for intra-host viral populations.",
        "BACKGROUND AND OBJECTIVE: Accurate coronary artery tree segmentation can now be developed to assist radiologists in detecting coronary artery disease. In clinical medicine, the noise, low contrast, and uneven intensity of medical images along with complex shapes and vessel bifurcation structures make coronary artery segmentation challenging. In this work, we propose a multiobjective clustering and toroidal model-guided tracking method that can accurately extract coronary arteries from computed tomography angiography (CTA) imagery. METHODS: Utilizing integrated noise reduction, candidate region detection, geometric feature extraction, and coronary artery tracking techniques, a new segmentation framework for 3D coronary artery trees is presented. The candidate regions are extracted using a multiobjective clustering method, and the coronary arteries are tracked by a toroidal model-guided tracking method. RESULTS: The qualitative and quantitative results demonstrate the effectiveness of the presented framework, which achieves better performance than the compared segmentation methods in three widely used evaluation indices: the Dice similarity coefficient (DSC), Jaccard index and Recall across the CTA data. The proposed method can accurately identify the coronary artery tree with a mean DSC of 84%, a Jaccard index of 74%, and a Recall of 93%. CONCLUSIONS: The proposed segmentation framework effectively segments the coronary tree from the CTA volume, which improves the accuracy of 3D vascular tree segmentation.",
        "Local graph clustering is an important machine learning task that aims to find a well-connected cluster near a set of seed nodes. Recent results have revealed that incorporating higher order information significantly enhances the results of graph clustering techniques. The majority of existing research in this area focuses on spectral graph theory-based techniques. However, an alternative perspective on local graph clustering arises from using max-flow and min-cut on the objectives, which offer distinctly different guarantees. For instance, a new method called capacity releasing diffusion (CRD) was recently proposed and shown to preserve local structure around the seeds better than spectral methods. The method was also the first local clustering technique that is not subject to the quadratic Cheeger inequality by assuming a good cluster near the seed nodes. In this paper, we propose a local hypergraph clustering technique called hypergraph CRD (HG-CRD) by extending the CRD process to cluster based on higher order patterns, encoded as hyperedges of a hypergraph. Moreover, we theoretically show that HG-CRD gives results about a quantity called motif conductance, rather than a biased version used in previous experiments. Experimental results on synthetic datasets and real world graphs show that HG-CRD enhances the clustering quality.",
        "The rapid growth of the number of data brings great challenges to clustering, especially the introduction of multi-view data, which collected from multiple sources or represented by multiple features, makes these challenges more arduous. How to clustering large-scale data efficiently has become the hottest topic of current large-scale clustering tasks. Although several accelerated multi-view methods have been proposed to improve the efficiency of clustering large-scale data, they still cannot be applied to some scenarios that require high efficiency because of the high computational complexity. To cope with the issue of high computational complexity of existing multi-view methods when dealing with large-scale data, a fast multi-view clustering model via nonnegative and orthogonal factorization (FMCNOF) is proposed in this paper. Instead of constraining the factor matrices to be nonnegative as traditional nonnegative and orthogonal factorization (NOF), we constrain a factor matrix of this model to be cluster indicator matrix which can assign cluster labels to data directly without extra post-processing step to extract cluster structures from the factor matrix. Meanwhile, the F-norm instead of the L2-norm is utilized on the FMCNOF model, which makes the model very easy to optimize. Furthermore, an efficient optimization algorithm is proposed to solve the FMCNOF model. Different from the traditional NOF optimization algorithm requiring dense matrix multiplications, our algorithm can divide the optimization problem into three decoupled small size subproblems that can be solved by much less matrix multiplications. Combined with the FMCNOF model and the corresponding fast optimization method, the efficiency of the clustering process can be significantly improved, and the computational complexity is nearly O(n) . Extensive experiments on various benchmark data sets validate our approach can greatly improve the efficiency when achieve acceptable performance.",
        "In this paper, we propose a Lasso Weighted k-means ( LW-k-means) algorithm, as a simple yet efficient sparse clustering procedure for high-dimensional data where the number of features ( p) can be much higher than the number of observations (n). The LW-k-means method imposes an l1 regularization term involving the feature weights directly to induce feature selection in a sparse clustering framework. We develop a simple block-coordinate descent type algorithm with time-complexity resembling that of Lloyd's method, to optimize the proposed objective. In addition, we establish the strong consistency of the LW-k-means procedure. Such consistency proof is not available for the conventional spare k-means algorithms, in general. LW-k-means is tested on a number of synthetic and real-life datasets and through a detailed experimental analysis, we find that the performance of the method is highly competitive against the baselines as well as the state-of-the-art procedures for center-based high-dimensional clustering, not only in terms of clustering accuracy but also with respect to computational time.",
        "Clinical pathways are used to guide clinicians to provide a standardised delivery of care. Because of their standardisation, the aim of clinical pathways is to reduce variation in both care process and patient outcomes. When learning clinical pathways from data through data mining, it is common practice to represent each patient pathway as a string corresponding to their movements through activities. Clustering techniques are popular methods for pathway mining, and therefore this paper focuses on distance metrics applied to string data for k-medoids clustering. The two main aims are to firstly, develop a technique that seamlessly integrates expert information with data and secondly, to develop a string distance metric for the purpose of process data. The overall goal was to allow for more meaningful clustering results to be found by adding context into the string similarity calculation. Eight common distance metrics and their applicability are discussed. These distance metrics prove to give an arbitrary distance, without consideration for context, and each produce different results. As a result, this paper describes the development of a new distance metric, the modified Needleman-Wunsch algorithm, that allows for expert interaction with the calculation by assigning groupings and rankings to activities, which provide context to the strings. This algorithm has been developed in partnership with UK's National Health Service (NHS) with the focus on a lung cancer pathway, however the handling of the data and algorithm allows for application to any disease type. This method is contained within Sim.Pro.Flow, a publicly available decision support tool.",
        "There is a need to develop an effective data preservation scheme with minimal information loss when the patient's data are shared in public interest for different research activities. Prior studies have devised different approaches for data preservation in healthcare domains; however, there is still room for improvement in the design of an elegant data preservation approach. With that motivation behind, this study has proposed a medical healthcare-IoTs-based infrastructure with restricted access. The infrastructure comprises two algorithms. The first algorithm protects the sensitivity information of a patient with quantifying minimum information loss during the anonymization process. The algorithm has also designed the access polices comprising the public access, doctor access, and the nurse access, to access the sensitivity information of a patient based on the clustering concept. The second suggested algorithm is K-anonymity privacy preservation based on local coding, which is based on cell suppression. This algorithm utilizes a mapping method to classify the data into different regions in such a manner that the data of the same group are placed in the same region. The benefit of using local coding is to restrict third-party users, such as doctors and nurses, when trying to insert incorrect values in order to access real patient data. Efficiency of the proposed algorithm is evaluated against the state-of-the-art algorithm by performing extensive simulations. Simulation results demonstrate benefits of the proposed algorithms in terms of efficient cluster formation in minimum time, minimum information loss, and execution time for data dissemination.",
        "The object detection algorithm based on vehicle-mounted lidar is a key component of the perception system on autonomous vehicles. It can provide high-precision and highly robust obstacle information for the safe driving of autonomous vehicles. However, most algorithms are often based on a large amount of point cloud data, which makes real-time detection difficult. To solve this problem, this paper proposes a 3D fast object detection method based on three main steps: First, the ground segmentation by discriminant image (GSDI) method is used to convert point cloud data into discriminant images for ground points segmentation, which avoids the direct computing of the point cloud data and improves the efficiency of ground points segmentation. Second, the image detector is used to generate the region of interest of the three-dimensional object, which effectively narrows the search range. Finally, the dynamic distance threshold clustering (DDTC) method is designed for different density of the point cloud data, which improves the detection effect of long-distance objects and avoids the over-segmentation phenomenon generated by the traditional algorithm. Experiments have showed that this algorithm can meet the real-time requirements of autonomous driving while maintaining high accuracy.",
        "There have been numerous genetic and epigenetic datasets generated for the study of complex disease including neurodegenerative disease. However, analysis of such data often suffers from detecting the outliers of the samples, which subsequently affects the extraction of the true biological signals involved in the disease. To address this critical issue, we developed a novel framework for identifying methylation signatures using consecutive adaptation of a well-known outlier detection algorithm, density based clustering of applications with reducing noise (DBSCAN) followed by hierarchical clustering. We applied the framework to two representative neurodegenerative diseases, Alzheimer's disease (AD) and Down syndrome (DS), using DNA methylation datasets from public sources (Gene Expression Omnibus, GEO accession ID: GSE74486). We first applied DBSCAN algorithm to eliminate outliers, and then used Limma statistical method to determine differentially methylated genes. Next, hierarchical clustering technique was applied to detect gene modules. Our analysis identified a methylation signature comprising 21 genes for AD and a methylation signature comprising 89 genes for DS, respectively. Our evaluation indicated that these two signatures could lead to high classification accuracy values (92% and 70%) for these two diseases. In summary, this framework will be useful to better detect outlier-free genetic and epigenetic signatures in various complex diseases and their developmental stages.",
        "Data from smart grids are challenging to analyze due to their very large size, high dimensionality, skewness, sparsity, and number of seasonal fluctuations, including daily and weekly effects. With the data arriving in a sequential form the underlying distribution is subject to changes over the time intervals. Time series data streams have their own specifics in terms of the data processing and data analysis because, usually, it is not possible to process the whole data in memory as the large data volumes are generated fast so the processing and the analysis should be done incrementally using sliding windows. Despite the proposal of many clustering techniques applicable for grouping the observations of a single data stream, only a few of them are focused on splitting the whole data streams into the clusters. In this article we aim to explore individual characteristics of electricity usage and recommend the most suitable tariff to the customer so they can benefit from lower prices. This work investigates various algorithms (and their improvements) what allows us to formulate the clusters, in real time, based on smart meter data.",
        "Cutaneous melanoma is an aggressive malignancy with high heterogeneity. Several studies have been performed to identify cutaneous melanoma subtypes based on genomic profiling. However, few classifications based on assessments of immune-associated genes have limited clinical implications for cutaneous melanoma. Using 470 cutaneous melanoma samples from The Cancer Genome Atlas (TCGA), we calculated the enrichment levels of 29 immune-associated gene sets in each sample and hierarchically clustered them into Immunity High (Immunity_H, n=323, 68.7%), Immunity Medium (Immunity_M, n=135, 28.7%), and Immunity Low (Immunity_L, n=12, 2.6%) based on the ssGSEA score. The ESTIMATE algorithm was used to calculate stromal scores (range: -1,800.51-1,901.99), immune scores (range: -1,476.28-3,780.33), estimate scores (range: -2,618.28-5,098.14) and tumor purity (range: 0.216-0.976) and they were significantly correlated with immune subtypes (Kruskal-Wallis test, P < 0.001). The Immunity_H group tended to have higher expression levels of HLA and immune checkpoint genes (Kruskal-Wallis test, P < 0.05). The Immunity_H group had the highest level of naive B cells, resting dendritic cells, M1 macrophages, resting NK cells, plasma cells, CD4 memory activated T cells, CD8 T cells, follicular helper T cells and regulatory T cells, and the Immunity_L group had better overall survival. The GO terms identified in the Immunity_H group were mainly immune related. In conclusion, immune signature-associated cutaneous melanoma subtypes play a role in cutaneous melanoma prognosis stratification. The construction of immune signature-associated cutaneous melanoma subtypes predicted possible patient outcomes and provided possible immunotherapy candidates.",
        "Advances in technology have made it convenient to obtain a large amount of single cell RNA sequencing (scRNA-seq) data. Since that clustering is a very important step in identifying or defining cellular phenotypes, many clustering approaches have been developed recently for these applications. The general methods can be roughly divided into normal clustering methods and integrated (ensemble) clustering methods which combine more than two normal clustering methods aiming to get much more informative performance. In order to make a contrast with the integrated clustering algorithm, the normal clustering method is often called individual or base clustering method. Note that the results of many individual clustering methods are often developed to capture one aspect of the data, and the results depend on the initial parameter settings, such as cluster number, distance metric and so on. Compared with individual clustering, although integrative clustering method may get much more accurate performance, the results depend on the base clustering results and integrated systems are often not self-regulation. Therefore, how to design a robust unsupervised clustering method is still a challenge. In order to tackle above limitations, we propose a novel Ensemble Clustering algorithm based on Probability Graphical Model with Graph Regularization, which is called EC-PGMGR for short. On one hand, we use parameter controlling in Probability Graphical Model (PGM) to automatically determine the cluster number without prior knowledge. On the other hand, we add a regularization term to reduce the effect deriving from some weak base clustering results. Particularly, the integrative results collected from base clustering methods can be assembled in the form of combination with self-regulation weights through a pre-learning process, which can efficiently enhance the effect of active clustering methods while weaken the effect of inactive clustering methods. Experiments are carried out on 7 data sets generated by different platforms with the number of single cells from 822 to 5,132. Results show that EC-PGMGR performs better than 4 alternative individual clustering methods and 2 ensemble methods in terms of accuracy including Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI), robustness, effectiveness and so on. EC-PGMGR provides an effective way to integrate different clustering results for more accurate and reliable results in further biological analysis as well. It may provide some new insights to the other applications of clustering.",
        "Microbial community is ubiquitous in nature, which has a great impact on the living environment and human health. All these effects of microbial communities on the environment and their hosts are often referred to as the functions of these communities, which depend largely on the composition of the communities. The study of microbial higher-order module can help us understand the dynamic development and evolution process of microbial community and explore community function. Considering that traditional clustering methods depend on the number of clusters or the influence of data that does not belong to any cluster, this paper proposes a hypergraph clustering algorithm based on game theory to mine the microbial high-order interaction module (HCGI), and the hypergraph clustering problem naturally turns into a clustering game problem, the partition of network modules is transformed into finding the critical point of evolutionary stability strategy (ESS). The experimental results show HCGI does not depend on the number of classes, and can get more conservative and better quality microbial clustering module, which provides reference for researchers and saves time and cost. The source code of HCGI in this paper can be downloaded from https://github.com/ylm0505/HCGI.",
        "This paper deals with clustering based on feature selection of multisensor data in high-dimensional space. Spectral clustering algorithms are efficient tools in signal processing for grouping datasets sampled by multisensor systems for fault diagnosis. The effectiveness of spectral clustering stems from constructing an embedding space based on an affinity matrix. This matrix shows the pairwise similarity of the data points. Clustering is then obtained by determining the spectral decomposition of the Laplacian graph. In the manufacturing field, clustering is an essential strategy for fault diagnosis. In this study, an enhanced spectral clustering approach is presented, which is augmented with pairwise constraints, and that results in efficient identification of fault scenarios. The effectiveness of the proposed approach is described using a real case study about a diesel injection control system for fault detection.",
        "High resolution melting (HRM) is a fast closed-tube method for nucleotide variant scanning applicable for bacterial species identification or molecular typing. Recently a novel HRM-based method for Klebsiella pneumoniae typing has been proposed: it consists of an HRM protocol designed on the capsular wzi gene and an HRM-based algorithm of strains clustering. In this study, we evaluated the repeatability and reproducibility of this method by performing the HRM typing of a set of K. pneumoniae strains, on three different instruments and by two different operators. The results showed that operators do not affect melting temperatures while different instruments can. Despite this, we found that strain clustering analysis, performed using MeltingPlot separately on the data from the three instruments, remains almost perfectly consistent. The HRM method under study resulted highly repeatable and thus reliable for large studies, even when several operators are involved. Furthermore, the HRM clusters obtained from the three different instruments were highly conserved, suggesting that this method could be applied in multicenter studies, even if different instruments are used.",
        "Accurate clustering of cells from single-cell RNA sequencing (scRNA-seq) data is an essential step for biological analysis such as putative cell type identification. However, scRNA-seq data has high dimension and high sparsity, which makes traditional clustering methods less effective to reflect the similarity between cells. Since genetic network fundamentally defines the functions of cell and deep learning shows strong advantages in network representation learning, we propose a novel scRNA-seq clustering framework ScGSLC based on graph similarity learning. ScGSLC effectively integrates scRNA-seq data and protein-protein interaction network to a graph. Then graph convolution network is employed by ScGSLC to embedding graph and clustering the cells by the calculated similarity between graphs. Unsupervised clustering results of nine public data sets demonstrate that ScGSLC shows better performance than the state-of-the-art methods.",
        "This article studies the large-scale subspace clustering (LS(2)C) problem with millions of data points. Many popular subspace clustering methods cannot directly handle the LS(2)C problem although they have been considered to be state-of-the-art methods for small-scale data points. A simple reason is that these methods often choose all data points as a large dictionary to build huge coding models, which results in high time and space complexity. In this article, we develop a learnable subspace clustering paradigm to efficiently solve the LS(2)C problem. The key concept is to learn a parametric function to partition the high-dimensional subspaces into their underlying low-dimensional subspaces instead of the computationally demanding classical coding models. Moreover, we propose a unified, robust, predictive coding machine (RPCM) to learn the parametric function, which can be solved by an alternating minimization algorithm. Besides, we provide a bounded contraction analysis of the parametric function. To the best of our knowledge, this article is the first work to efficiently cluster millions of data points among the subspace clustering methods. Experiments on million-scale data sets verify that our paradigm outperforms the related state-of-the-art methods in both efficiency and effectiveness.",
        "Importance: Medically complex patients are a heterogeneous group that contribute to a substantial proportion of health care costs. Coordinated efforts to improve care and reduce costs for this patient population have had limited success to date. Objective: To define distinct patient clinical profiles among the most medically complex patients through clinical interpretation of analytically derived patient clusters. Design, Setting, and Participants: This cohort study analyzed the most medically complex patients within Kaiser Permanente Northern California, a large integrated health care delivery system, based on comorbidity score, prior emergency department admissions, and predicted likelihood of hospitalization, from July 18, 2018, to July 15, 2019. From a starting point of over 5000 clinical variables, we used both clinical judgment and analytic methods to reduce to the 97 most informative covariates. Patients were then grouped using 2 methods (latent class analysis, generalized low-rank models, with k-means clustering). Results were interpreted by a panel of clinical stakeholders to define clinically meaningful patient profiles. Main Outcomes and Measures: Complex patient profiles, 1-year health care utilization, and mortality outcomes by profile. Results: The analysis included 104869 individuals representing 3.3% of the adult population (mean [SD] age, 70.7 [14.5] years; 52.4% women; 39% non-White race/ethnicity). Latent class analysis resulted in a 7-class solution. Stakeholders defined the following complex patient profiles (prevalence): high acuity (9.4%), older patients with cardiovascular complications (15.9%), frail elderly (12.5%), pain management (12.3%), psychiatric illness (12.0%), cancer treatment (7.6%), and less engaged (27%). Patients in these groups had significantly different 1-year mortality rates (ranging from 3.0% for psychiatric illness profile to 23.4% for frail elderly profile; risk ratio, 7.9 [95% CI, 7.1-8.8], P < .001). Repeating the analysis using k-means clustering resulted in qualitatively similar groupings. Each clinical profile suggested a distinct collaborative care strategy to optimize management. Conclusions and Relevance: The findings suggest that highly medically complex patient populations may be categorized into distinct patient profiles that are amenable to varying strategies for resource allocation and coordinated care interventions.",
        "Single-cell RNA sequencing enables us to characterize the cellular heterogeneity in single cell resolution with the help of cell type identification algorithms. However, the noise inherent in single-cell RNA-sequencing data severely disturbs the accuracy of cell clustering, marker identification and visualization. We propose that clustering based on feature density profiles can distinguish informative features from noise. We named such strategy as 'entropy subspace' separation and designed a cell clustering algorithm called ENtropy subspace separation-based Clustering for nOise REduction (ENCORE) by integrating the 'entropy subspace' separation strategy with a consensus clustering method. We demonstrate that ENCORE performs superiorly on cell clustering and generates high-resolution visualization across 12 standard datasets. More importantly, ENCORE enables identification of group markers with biological significance from a hard-to-separate dataset. With the advantages of effective feature selection, improved clustering, accurate marker identification and high-resolution visualization, we present ENCORE to the community as an important tool for scRNA-seq data analysis to study cellular heterogeneity and discover group markers.",
        "The rapid development of single-cell RNA sequencing (scRNA-Seq) technology provides strong technical support for accurate and efficient analyzing single-cell gene expression data. However, the analysis of scRNA-Seq is accompanied by many obstacles, including dropout events and the curse of dimensionality. Here, we propose the scGMAI, which is a new single-cell Gaussian mixture clustering method based on autoencoder networks and the fast independent component analysis (FastICA). Specifically, scGMAI utilizes autoencoder networks to reconstruct gene expression values from scRNA-Seq data and FastICA is used to reduce the dimensions of reconstructed data. The integration of these computational techniques in scGMAI leads to outperforming results compared to existing tools, including Seurat, in clustering cells from 17 public scRNA-Seq datasets. In summary, scGMAI is an effective tool for accurately clustering and identifying cell types from scRNA-Seq data and shows the great potential of its applicative power in scRNA-Seq data analysis. The source code is available at https://github.com/QUST-AIBBDRC/scGMAI/.",
        "A hierarchical clustering algorithm was applied to magnetic resonance images (MRI) of a cohort of 751 subjects having a mild cognitive impairment (MCI), 282 subjects having received Alzheimer's disease (AD) diagnosis, and 428 normal controls (NC). MRIs were preprocessed to gray matter density maps and registered to a stereotactic space. By first rendering the gray matter density maps comparable by regressing out age, gender, and years of education, and then performing the hierarchical clustering, we found clusters displaying structural features of typical AD, cortically-driven atypical AD, limbic-predominant AD, and early-onset AD (EOAD). Among these clusters, EOAD subjects displayed marked cortical gray matter atrophy and atrophy of the precuneus. Furthermore, EOAD subjects had the highest progression rates as measured with ADAS slopes during the longitudinal follow-up of 36 months. Striking heterogeneities in brain atrophy patterns were observed with MCI subjects. We found clusters of stable MCI, clusters of diffuse brain atrophy with fast progression, and MCI subjects displaying similar atrophy patterns as the typical or atypical AD subjects. Bidirectional differences in structural phenotypes were found with MCI subjects involving the anterior cerebellum and the frontal cortex. The diversity of the MCI subjects suggests that the structural phenotypes of MCI subjects would deserve a more detailed investigation with a significantly larger cohort. Our results demonstrate that the hierarchical agglomerative clustering method is an efficient tool in dividing a cohort of subjects with gray matter atrophy into coherent clusters manifesting different structural phenotypes.",
        "Pathological synchronization of neurons is associated with symptoms of movement disorders, such as Parkinson's disease and essential tremor. High-frequency deep brain stimulation (DBS) suppresses symptoms, presumably through the desynchronization of neurons. Coordinated reset (CR) delivers trains of high-frequency stimuli to different regions in the brain through multiple electrodes and may have more persistent therapeutic effects than conventional DBS. As an alternative to CR, we present a closed-loop control setup that desynchronizes neurons in brain slices by inducing clusters using a single electrode. Our setup uses calcium fluorescence imaging to extract carbachol-induced neuronal oscillations in real time. To determine the appropriate stimulation waveform for inducing clusters in a population of neurons, we calculate the phase of the neuronal populations and then estimate the phase response curve (PRC) of those populations to electrical stimulation. The phase and PRC are then fed into a control algorithm called the input of maximal instantaneous efficiency (IMIE). By using IMIE, the synchrony across the slice is decreased by dividing the population of neurons into subpopulations without suppressing the oscillations locally. The desynchronization effect is persistent 10 s after stimulation is stopped. The IMIE control algorithm may be used as a novel closed-loop DBS approach to suppress the symptoms of Parkinson's disease and essential tremor by inducing clusters with a single electrode.NEW & NOTEWORTHY Here, we present a closed-loop controller to desynchronize neurons in brain slices by inducing clusters using a single electrode using calcium imaging feedback. Phase of neurons are estimated in real time, and from the phase response curve stimulation is applied to achieve target phase differences. This method is an alternative to coordinated reset and is a novel therapy that could be used to disrupt synchronous neuronal oscillations thought to be the mechanism underlying Parkinson's disease.",
        "Single-molecule localization microscopy (SMLM) is a powerful tool for studying intracellular structure and macromolecular organization at the nanoscale. The increasingly massive pointillistic data sets generated by SMLM require the development of new and highly efficient quantification tools. Here we present FOCAL3D, an accurate, flexible and exceedingly fast (scaling linearly with the number of localizations) density-based algorithm for quantifying spatial clustering in large 3D SMLM data sets. Unlike DBSCAN, which is perhaps the most commonly employed density-based clustering algorithm, an optimum set of parameters for FOCAL3D may be objectively determined. We initially validate the performance of FOCAL3D on simulated datasets at varying noise levels and for a range of cluster sizes. These simulated datasets are used to illustrate the parametric insensitivity of the algorithm, in contrast to DBSCAN, and clustering metrics such as the F1 and Silhouette score indicate that FOCAL3D is highly accurate, even in the presence of significant background noise and mixed populations of variable sized clusters, once optimized. We then apply FOCAL3D to 3D astigmatic dSTORM images of the nuclear pore complex (NPC) in human osteosaracoma cells, illustrating both the validity of the parameter optimization and the ability of the algorithm to accurately cluster complex, heterogeneous 3D clusters in a biological dataset. FOCAL3D is provided as an open source software package written in Python.",
        "Two well-known drawbacks in fuzzy clustering are the requirement of assigning in advance the number of clusters and random initialization of cluster centers. The quality of the final fuzzy clusters depends heavily on the initial choice of the number of clusters and the initialization of the clusters, then, it is necessary to apply a validity index to measure the compactness and the separability of the final clusters and run the clustering algorithm several times. We propose a new fuzzy C-means algorithm in which a validity index based on the concepts of maximum fuzzy energy and minimum fuzzy entropy is applied to initialize the cluster centers and to find the optimal number of clusters and initial cluster centers in order to obtain a good clustering quality, without increasing time consumption. We test our algorithm on UCI (University of California at Irvine) machine learning classification datasets comparing the results with the ones obtained by using well-known validity indices and variations of fuzzy C-means by using optimization algorithms in the initialization phase. The comparison results show that our algorithm represents an optimal trade-off between the quality of clustering and the time consumption.",
        "In this paper, we develop an unsupervised generative clustering framework that combines the variational information bottleneck and the Gaussian mixture model. Specifically, in our approach, we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the Evidence Lower Bound (ELBO) and provide a variational inference type algorithm that allows computing it. In the algorithm, the coders' mappings are parametrized using neural networks, and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.",
        "A common approach for analyzing large-scale molecular data is to cluster objects sharing similar characteristics. This assumes that genes with highly similar expression profiles are likely participating in a common molecular process. Biological systems are extremely complex and challenging to understand, with proteins having multiple functions that sometimes need to be activated or expressed in a time-dependent manner. Thus, the strategies applied for clustering of these molecules into groups are of key importance for translation of data to biologically interpretable findings. Here we implemented a multi-assignment clustering (MAsC) approach that allows molecules to be assigned to multiple clusters, rather than single ones as in commonly used clustering techniques. When applied to high-throughput transcriptomics data, MAsC increased power of the downstream pathway analysis and allowed identification of pathways with high biological relevance to the experimental setting and the biological systems studied. Multi-assignment clustering also reduced noise in the clustering partition by excluding genes with a low correlation to all of the resulting clusters. Together, these findings suggest that our methodology facilitates translation of large-scale molecular data into biological knowledge. The method is made available as an R package on GitLab (https://gitlab.com/wolftower/masc).",
        "Clustering, as an important part of data mining, is inherently a challenging problem. This article proposes a differential evolution algorithm with adaptive niching and k-means operation (denoted as DE_ANS_AKO) for partitional data clustering. Within the proposed algorithm, an adaptive niching scheme, which can dynamically adjust the size of each niche in the population, is devised and integrated to prevent premature convergence of evolutionary search, thus appropriately searching the space to identify the optimal or near-optimal solution. Furthermore, to improve the search efficiency, an adaptive k-means operation has been designed and employed at the niche level of population. The performance of the proposed algorithm has been evaluated on synthetic as well as real datasets and compared with related methods. The experimental results reveal that the proposed algorithm is able to reliably and efficiently deliver high quality clustering solutions and generally outperforms related methods implemented for comparisons.",
        "As an effective method for clustering applications, the clustering ensemble algorithm integrates different clustering solutions into a final one, thus improving the clustering efficiency. The key to designing the clustering ensemble algorithm is to improve the diversities of base learners and optimize the ensemble strategies. To address these problems, we propose a clustering ensemble framework that consists of three parts. First, three view transformation methods, including random principal component analysis, random nearest neighbor, and modified fuzzy extension model, are used as base learners to learn different clustering views. A random transformation and hybrid multiview learning-based clustering ensemble method (RTHMC) is then designed to synthesize the multiview clustering results. Second, a new random subspace transformation is integrated into RTHMC to enhance its performance. Finally, a view-based self-evolutionary strategy is developed to further improve the proposed method by optimizing random subspace sets. Experiments and comparisons demonstrate the effectiveness and superiority of the proposed method for clustering different kinds of data.",
        "In order to facilitate correlation calculation and matrix-based resolution in comprehensive two-dimensional gas chromatography - mass spectrometry (GC x GC-MS) data-set, an intelligent clustering of modulation peaks (ICMP) algorithm was developed in this paper. ICMP is start with the second -dimension ((2)D) peak restriction, then conducting the peak shape restriction in the first dimension ((1)D), finally end with the eigenvalues calculation against mass spectra in moving sub-windows. After this three-tier restriction, multi-component spectral correlative chromatography (MSCC) was applied in peak clustering result from a row-wise augmented \"two-dimension (2D) slice\" set. Then the component similarities and differences were distinguished rapidly/ accurately in chemical fingerprints from ChaiHu Shugan San and Cyperus rotundus. Faced with co-eluted phenomenon, matrix-based resolution was made in the representative sub-matrices that have been locked in ICMP procedure. From the example data shows that ICMP- multivariate curve resolution (MCR) can served as a good complement to (non) trilinear decomposition. To summarize, the GC x GC data-structure can be simplified to facilitate MSCC or MCR operation in fingerprints from herbal or biological samples.",
        "Unsupervised methods, such as clustering methods, are essential to the analysis of single-cell genomic data. The most current clustering methods are designed for one data type only, such as single-cell RNA sequencing (scRNA-seq), single-cell ATAC sequencing (scATAC-seq) or sc-methylation data alone, and a few are developed for the integrative analysis of multiple data types. The integrative analysis of multimodal single-cell genomic data sets leverages the power in multiple data sets and can deepen the biological insight. In this paper, we propose a coupled co-clustering-based unsupervised transfer learning algorithm (coupleCoC) for the integrative analysis of multimodal single-cell data. Our proposed coupleCoC builds upon the information theoretic co-clustering framework. In co-clustering, both the cells and the genomic features are simultaneously clustered. Clustering similar genomic features reduces the noise in single-cell data and facilitates transfer of knowledge across single-cell datasets. We applied coupleCoC for the integrative analysis of scATAC-seq and scRNA-seq data, sc-methylation and scRNA-seq data and scRNA-seq data from mouse and human. We demonstrate that coupleCoC improves the overall clustering performance and matches the cell subpopulations across multimodal single-cell genomic datasets. Our method coupleCoC is also computationally efficient and can scale up to large datasets. Availability: The software and datasets are available at https://github.com/cuhklinlab/coupleCoC.",
        "BACKGROUND: Computer Aided Diagnosis (CAD) systems have been developing in the last years with the aim of helping the diagnosis and monitoring of several diseases. We present a novel CAD system based on a hybrid Watershed-Clustering algorithm for the detection of lesions in Multiple Sclerosis. METHODS: Magnetic Resonance Imaging scans (FLAIR sequences without gadolinium) of 20 patients affected by Multiple Sclerosis with hyperintense lesions were studied. The CAD system consisted of the following automated processing steps: images recording, automated segmentation based on the Watershed algorithm, detection of lesions, extraction of both dynamic and morphological features, and classification of lesions by Cluster Analysis. RESULTS: The investigation was performed on 316 suspect regions including 255 lesion and 61 non-lesion cases. The Receiver Operating Characteristic analysis revealed a highly significant difference between lesions and non-lesions; the diagnostic accuracy was 87% (95% CI: 0.83-0.90), with an appropriate cut-off of 192.8; the sensitivity was 77% and the specificity was 87%. CONCLUSIONS: In conclusion, we developed a CAD system by using a modified algorithm for automated image segmentation which may discriminate MS lesions from non-lesions. The proposed method generates a detection out-put that may be support the clinical evaluation.",
        "BACKGROUND: Comprehensive molecular profiling has revealed somatic variations in cancer at genomic, epigenomic, transcriptomic, and proteomic levels. The accumulating data has shown clearly that molecular phenotypes of cancer are complex and influenced by a multitude of factors. Conventional unsupervised clustering applied to a large patient population is inevitably driven by the dominant variation from major factors such as cell-of-origin or histology. Translation of these data into clinical relevance requires more effective extraction of information directly associated with patient outcome. METHODS: Drawing from ideas in supervised text classification, we developed survClust, an outcome-weighted clustering algorithm for integrative molecular stratification focusing on patient survival. survClust was performed on 18 cancer types across multiple data modalities including somatic mutation, DNA copy number, DNA methylation, and mRNA, miRNA, and protein expression from the Cancer Genome Atlas study to identify novel prognostic subtypes. RESULTS: Our analysis identified the prognostic role of high tumor mutation burden with concurrently high CD8 T cell immune marker expression and the aggressive clinical behavior associated with CDKN2A deletion across cancer types. Visualization of somatic alterations, at a genome-wide scale (total mutation burden, mutational signature, fraction genome altered) and at the individual gene level, using circomap further revealed indolent versus aggressive subgroups in a pan-cancer setting. CONCLUSIONS: Our analysis has revealed prognostic molecular subtypes not previously identified by unsupervised clustering. The algorithm and tools we developed have direct utility toward patient stratification based on tumor genomics to inform clinical decision-making. The survClust software tool is available at https://github.com/arorarshi/survClust .",
        "Recent advances in multi-omics clustering methods enable a more fine-tuned separation of cancer patients into clinical relevant clusters. These advancements have the potential to provide a deeper understanding of cancer progression and may facilitate the treatment of cancer patients. Here, we present a simple hierarchical clustering and data fusion approach, named HC-fused, for the detection of disease subtypes. Unlike other methods, the proposed approach naturally reports on the individual contribution of each single-omic to the data fusion process. We perform multi-view simulations with disjoint and disjunct cluster elements across the views to highlight fundamentally different data integration behavior of various state-of-the-art methods. HC-fused combines the strengths of some recently published methods and shows superior performance on real world cancer data from the TCGA (The Cancer Genome Atlas) database. An R implementation of our method is available on GitHub (pievos101/HC-fused).",
        "ABSTRACT: Traditional classification and prognostic approaches for chronic pain conditions focus primarily on anatomically based clinical characteristics not based on underlying biopsychosocial factors contributing to perception of clinical pain and future pain trajectories. Using a supervised clustering approach in a cohort of temporomandibular disorder cases and controls from the Orofacial Pain: Prospective Evaluation and Risk Assessment study, we recently developed and validated a rapid algorithm (ROPA) to pragmatically classify chronic pain patients into 3 groups that differed in clinical pain report, biopsychosocial profiles, functional limitations, and comorbid conditions. The present aim was to examine the generalizability of this clustering procedure in 2 additional cohorts: a cohort of patients with chronic overlapping pain conditions (Complex Persistent Pain Conditions study) and a real-world clinical population of patients seeking treatment at duke innovative pain therapies. In each cohort, we applied a ROPA for cluster prediction, which requires only 4 input variables: pressure pain threshold and anxiety, depression, and somatization scales. In both complex persistent pain condition and duke innovative pain therapies, we distinguished 3 clusters, including one with more severe clinical characteristics and psychological distress. We observed strong concordance with observed cluster solutions, indicating the ROPA method allows for reliable subtyping of clinical populations with minimal patient burden. The ROPA clustering algorithm represents a rapid and valid stratification tool independent of anatomic diagnosis. ROPA holds promise in classifying patients based on pathophysiological mechanisms rather than structural or anatomical diagnoses. As such, this method of classifying patients will facilitate personalized pain medicine for patients with chronic pain.",
        "The huge amount of data acquired by high-throughput sequencing requires data reduction for effective analysis. Here we give a clustering algorithm for genome-wide open chromatin data using a new data reduction method. This method regards the genome as a string of 1s and 0s based on a set of peaks and calculates the Hamming distances between the strings. This algorithm with the systematically optimized set of peaks enables us to quantitatively evaluate differences between samples of hematopoietic cells and classify cell types, potentially leading to a better understanding of leukemia pathogenesis.",
        "MOTIVATION: The estimation of large multiple sequence alignments (MSAs) is a basic bioinformatics challenge. Divide-and-conquer is a useful approach that has been shown to improve the scalability and accuracy of MSA estimation in established methods such as SATe and PASTA. In these divide-and-conquer strategies, a sequence dataset is divided into disjoint subsets, alignments are computed on the subsets using base MSA methods (e.g. MAFFT), and then merged together into an alignment on the full dataset. RESULTS: We present MAGUS, Multiple sequence Alignment using Graph clUStering, a new technique for computing large-scale alignments. MAGUS is similar to PASTA in that it uses nearly the same initial steps (starting tree, similar decomposition strategy, and MAFFT to compute subset alignments), but then merges the subset alignments using the Graph Clustering Merger, a new method for combining disjoint alignments that we present in this study. Our study, on a heterogeneous collection of biological and simulated datasets, shows that MAGUS produces improved accuracy and is faster than PASTA on large datasets, and matches it on smaller datasets. AVAILABILITY AND IMPLEMENTATION: MAGUS: https://github.com/vlasmirnov/MAGUS. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "A clustered DNA damage site (cluster), in which two or more lesions exist within a few helical turns, is believed to be a key factor determining the fate of a living cell exposed to a DNA damaging agent such as ionizing radiation. However, the structural details of a cluster such as the number of included lesions and their proximity are unknown. Herein, we develop a method to characterize a cluster by fluorescence anisotropy measurements based on Forster resonance energy transfer (homo-FRET). Plasmid DNA (pUC19) was irradiated with 2.0 and 0.52 MeV/u (4)He(2+), or 0.37 MeV/u (12)C(5+) ion beams (linear energy transfer: ~ 70, ~ 150, ~ 760 keV/mum, respectively) and (60)Co gamma-rays as a standard (~ 0.2 keV/mum) in the solid state. The irradiated DNA was labeled with an aminooxyl fluorophore (Alexa Fluor 488) to the aldehyde/ketone moieties such as apurinic/apyrimidinic sites. Homo-FRET analyses provided the apparent base separation values between lesions in a cluster produced by each ion beam track as 21.1, 19.4, and 18.7 base pairs. The production frequency of a cluster increases with increasing linear energy transfer of radiation. Our results demonstrate that homo-FRET analysis has the potential to discover the qualitative and the quantitative differences of the clusters produced not only by a variety of ionizing radiation but also by other DNA damaging agents.",
        "Various machine-learning classification techniques have been employed previously to classify brain states in healthy and disease populations using functional magnetic resonance imaging (fMRI). These methods generally use supervised classifiers that are sensitive to outliers and require labeling of training data to generate a predictive model. Density-based clustering, which overcomes these issues, is a popular unsupervised learning approach whose utility for high-dimensional neuroimaging data has not been previously evaluated. Its advantages include insensitivity to outliers and ability to work with unlabeled data. Unlike the popular k-means clustering, the number of clusters need not be specified. In this study, we compare the performance of two popular density-based clustering methods, DBSCAN and OPTICS, in accurately identifying individuals with three stages of cognitive impairment, including Alzheimer's disease. We used static and dynamic functional connectivity features for clustering, which captures the strength and temporal variation of brain connectivity respectively. To assess the robustness of clustering to noise/outliers, we propose a novel method called recursive-clustering using additive-noise (R-CLAN). Results demonstrated that both clustering algorithms were effective, although OPTICS with dynamic connectivity features outperformed in terms of cluster purity (95.46%) and robustness to noise/outliers. This study demonstrates that density-based clustering can accurately and robustly identify diagnostic classes in an unsupervised way using brain connectivity.",
        "Background: A transthoracic impedance (TTI) signal is an important indicator of the quality of chest compressions (CCs) during cardiopulmonary resuscitation (CPR). We proposed an automatic detection algorithm including the wavelet decomposition, fuzzy c-means (FCM) clustering, and deep belief network (DBN) to identify the compression and ventilation waveforms for evaluating the quality of CPR. Methods: TTI signals were collected from a cardiac arrest model that electrically induced cardiac arrest in pigs. All signals were denoised using the wavelet and morphology method. The potential compression and ventilation waveforms were marked using an algorithm with a multi-resolution window. The compressions and ventilations in these waveforms were identified and classified using the FCM clustering and DBN methods. Results: Using the FCM clustering method, the positive predictive values (PPVs) for compressions and ventilations were 99.7% and 95.7%, respectively. The sensitivities of recognition were 99.8% for compressions and 95.1% for ventilations. The DBN approach exhibited similar PPV and sensitivity results to the FCM clustering method. The time cost was satisfactory using either of these techniques. Conclusions: Our findings suggest that FCM clustering and DBN can be utilized to effectively and accurately evaluate CPR quality, and provide information for improving the success rate of CPR. Our real-time algorithms using FCM clustering and DBN eliminated most distortions and noises effectively, and correctly identified the compression and ventilation waveforms with a low time cost.",
        "In an era of ubiquitous large-scale evolving data streams, data stream clustering (DSC) has received lots of attention because the scale of the data streams far exceeds the ability of expert human analysts. It has been observed that high-dimensional data are usually distributed in a union of low-dimensional subspaces. In this article, we propose a novel sparse representation-based DSC algorithm, called evolutionary dynamic sparse subspace clustering (EDSSC). It can cope with the time-varying nature of subspaces underlying the evolving data streams, such as subspace emergence, disappearance, and recurrence. The proposed EDSSC consists of two phases: 1) static learning and 2) online clustering. During the first phase, a data structure for storing the statistic summary of data streams, called EDSSC summary, is proposed which can better address the dilemma between the two conflicting goals: 1) saving more points for accuracy of subspace clustering (SC) and 2) discarding more points for the efficiency of DSC. By further proposing an algorithm to estimate the subspace number, the proposed EDSSC does not need to know the number of subspaces. In the second phase, a more suitable index, called the average sparsity concentration index (ASCI), is proposed, which dramatically promotes the clustering accuracy compared to the conventionally utilized SCI index. In addition, the subspace evolution detection model based on the Page-Hinkley test is proposed where the appearing, disappearing, and recurring subspaces can be detected and adapted. Extinct experiments on real-world data streams show that the EDSSC outperforms the state-of-the-art online SC approaches.",
        "The aim of the study was to explore emergency department transfer delays and to assess the potential of using a semantic clustering approach to augment the content analysis of transfer delay data. Data were collected over a period of 5 months from two hospitals. A set of (unique) phrases describing reasons for transfer delays (n=333) were clustered using the k-means with 1) cluster centroids initiated in an unsupervised fashion and 2) a semi-supervised version where the cluster centroids were initiated with keywords. The unsupervised algorithm clustered 77 % and the semi-supervised 86 % of the phrases to suitable clusters. We chose the better performing approach to augment our content analysis. Three main categories for transfer delays were found as a result. These included 1) insufficient staffing resources, 2) transportation and bed issues, and 3) patient and care related reasons. The findings inform the audit of organisational processes, accuracy of staffing and workflow to reduce transfer delays. Future research should explore implications of semantic clustering approaches to other narrative data sets in health service research.",
        "Incomplete multi-view clustering which aims to solve the difficult clustering challenge on incomplete multi-view data collected from diverse domains with missing views has drawn considerable attention in recent years. In this paper, we propose a novel method, called consensus guided incomplete multi-view spectral clustering (CGIMVSC), to address the incomplete clustering problem. Specifically, CGIMVSC seeks to explore the local information within every single-view and the semantic consistent information shared by all views in a unified framework simultaneously, where the local structure is adaptively obtained from the incomplete data rather than pre-constructed via a k-nearest neighbor approach in the existing methods. Considering the semantic consistency of multiple views, CGIMVSC introduces a co-regularization constraint to minimize the disagreement between the common representation and the individual representations with respect to different views, such that all views will obtain a consensus clustering result. Experimental comparisons with some state-of-the-art methods on seven datasets validate the effectiveness of the proposed method on incomplete multi-view clustering.",
        "MOTIVATION: Precision medicine is a promising field that proposes, in contrast to a one-size-fits-all approach, the tailoring of medical decisions, treatments or products. In this context, it is crucial to introduce innovative methods to stratify a population of patients on the basis of an accurate system-level knowledge of the disease. This is particularly important in very challenging conditions, where the use of standard statistical methods can be prevented by poor data availability or by the need of oversimplifying the processes regulating a complex disease. RESULTS: We define an innovative method for phenotype classification that combines experimental data and a mathematical description of the disease biology. The methodology exploits the mathematical model for inferring additional subject features relevant for the classification. Finally, the algorithm identifies the optimal number of clusters and classifies the samples on the basis of a subset of the features estimated during the model fit. We tested the algorithm in two test cases: an in silico case in the context of dyslipidemia, a complex disease for which a large population of patients has been generated, and a clinical test case, in the context of a lysosomal rare disorder, for which the amount of available data was limited. In both the scenarios, our methodology proved to be accurate and robust, and allowed the inference of an additional phenotype division that the experimental data did not show. AVAILABILITY AND IMPLEMENTATION: The code to reproduce the in silico results has been implemented in MATLAB v.2017b and it is available in the Supplementary Material. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "When some 'entities' are related by the 'features' they share they are amenable to a bipartite network representation. Plant-pollinator ecological communities, co-authorship of scientific papers, customers and purchases, or answers in a poll, are but a few examples. Analyzing clustering of such entities in the network is a useful tool with applications in many fields, like internet technology, recommender systems, or detection of diseases. The algorithms most widely applied to find clusters in bipartite networks are variants of modularity optimization. Here, we provide a hierarchical clustering algorithm based on a dissimilarity between entities that quantifies the probability that the features shared by two entities are due to mere chance. The algorithm performance is O(n^{2}) when applied to a set of n entities, and its outcome is a dendrogram exhibiting the connections of those entities. Through the introduction of a 'susceptibility' measure we can provide an 'optimal' choice for the clustering as well as quantify its quality. The dendrogram reveals further useful structural information though-like the existence of subclusters within clusters or of nodes that do not fit in any cluster. We illustrate the algorithm by applying it first to a set of synthetic networks, and then to a selection of examples. We also illustrate how to transform our algorithm into a valid alternative for one-mode networks as well, and show that it performs at least as well as the standard, modularity-based algorithms-with a higher numerical performance. We provide an implementation of the algorithm in python freely accessible from GitHub.",
        "BACKGROUND: The amount of data generated in large clinical and phenotyping studies that use single-cell cytometry is constantly growing. Recent technological advances allow the easy generation of data with hundreds of millions of single-cell data points with >40 parameters, originating from thousands of individual samples. The analysis of that amount of high-dimensional data becomes demanding in both hardware and software of high-performance computational resources. Current software tools often do not scale to the datasets of such size; users are thus forced to downsample the data to bearable sizes, in turn losing accuracy and ability to detect many underlying complex phenomena. RESULTS: We present GigaSOM.jl, a fast and scalable implementation of clustering and dimensionality reduction for flow and mass cytometry data. The implementation of GigaSOM.jl in the high-level and high-performance programming language Julia makes it accessible to the scientific community and allows for efficient handling and processing of datasets with billions of data points using distributed computing infrastructures. We describe the design of GigaSOM.jl, measure its performance and horizontal scaling capability, and showcase the functionality on a large dataset from a recent study. CONCLUSIONS: GigaSOM.jl facilitates the use of commonly available high-performance computing resources to process the largest available datasets within minutes, while producing results of the same quality as the current state-of-art software. Measurements indicate that the performance scales to much larger datasets. The example use on the data from a massive mouse phenotyping effort confirms the applicability of GigaSOM.jl to huge-scale studies.",
        "BACKGROUND: Next-generation sequencing technologies revolutionized genomics by producing high-throughput reads at low cost, and this progress has prompted the recent development of de novo assemblers. Multiple assembly methods based on de Bruijn graph have been shown to be efficient for Illumina reads. However, the sequencing errors generated by the sequencer complicate analysis of de novo assembly and influence the quality of downstream genomic researches. RESULTS: In this paper, we develop a de Bruijn assembler, called Clover (clustering-oriented de novo assembler), that utilizes a novel k-mer clustering approach from the overlap-layout-consensus concept to deal with the sequencing errors generated by the Illumina platform. We further evaluate Clover's performance against several de Bruijn graph assemblers (ABySS, SOAPdenovo, SPAdes and Velvet), overlap-layout-consensus assemblers (Bambus2, CABOG and MSR-CA) and string graph assembler (SGA) on three datasets (Staphylococcus aureus, Rhodobacter sphaeroides and human chromosome 14). The results show that Clover achieves a superior assembly quality in terms of corrected N50 and E-size while remaining a significantly competitive in run time except SOAPdenovo. In addition, Clover was involved in the sequencing projects of bacterial genomes Acinetobacter baumannii TYTH-1 and Morganella morganii KT. CONCLUSIONS: The marvel clustering-based approach of Clover that integrates the flexibility of the overlap-layout-consensus approach and the efficiency of the de Bruijn graph method has high potential on de novo assembly. Now, Clover is freely available as open source software from https://oz.nthu.edu.tw/~d9562563/src.html .",
        "The use of mobile communication devices in health care is spreading worldwide. A huge amount of health data collected by these devices (mobile health data) is nowadays available. Mobile health data may allow for real-time monitoring of patients and delivering ad-hoc treatment recommendations. This paper aims at showing how this may be done by exploiting the potentialities of fuzzy clustering techniques. In fact, such techniques can be fruitfully applied to mobile health data in order to identify clusters of patients for diagnostic classification and cluster-specific therapies. However, since mobile health data are full of noise, fuzzy clustering methods cannot be directly applied to mobile health data. Such data must be denoised prior to analyzing them. When longitudinal mobile health data are available, functional data analysis represents a powerful tool for filtering out the noise in the data. Fuzzy clustering methods for functional data can then be used to determine groups of patients. In this work we develop a fuzzy clustering method, based on the concept of medoid, for functional data and we apply it to longitudinal mHealth data on daily symptoms and consumptions of anti-symptomatic drugs collected by two sets of patients in Berlin (Germany) and Ascoli Piceno (Italy) suffering from allergic rhinoconjunctivitis. The studies showed that clusters of patients with similar changes in symptoms were identified opening the possibility of precision medicine.",
        "The interpolative separable density fitting (ISDF) is an efficient and accurate low-rank decomposition method to reduce the high computational cost and memory usage of the Hartree-Fock exchange (HFX) calculations with numerical atomic orbitals (NAOs). In this work, we present a machine learning K-means clustering algorithm to select the interpolation points in ISDF, which offers a much cheaper alternative to the expensive QR factorization with column pivoting (QRCP) procedure. We implement this K-means-based ISDF decomposition to accelerate hybrid functional calculations with NAOs in the HONPAS package. We demonstrate that this method can yield a similar accuracy for both molecules and solids at a much lower computational cost. In particular, K-means can remarkably reduce the computational cost of selecting the interpolation points by nearly two orders of magnitude compared to QRCP, resulting in a speedup of approximately 10 times for ISDF-based HFX calculations.",
        "BACKGROUND: Sepsis is a heterogenous syndrome and individualized management strategy is the key to successful treatment. Genome wide expression profiling has been utilized for identifying subclasses of sepsis, but the clinical utility of these subclasses was limited because of the classification instability, and the lack of a robust class prediction model with extensive external validation. The study aimed to develop a parsimonious class model for the prediction of class membership and validate the model for its prognostic and predictive capability in external datasets. METHODS: The Gene Expression Omnibus (GEO) and ArrayExpress databases were searched from inception to April 2020. Datasets containing whole blood gene expression profiling in adult sepsis patients were included. Autoencoder was used to extract representative features for k-means clustering. Genetic algorithms (GA) were employed to derive a parsimonious 5-gene class prediction model. The class model was then applied to external datasets (n = 780) to evaluate its prognostic and predictive performance. FINDINGS: A total of 12 datasets involving 1613 patients were included. Two classes were identified in the discovery cohort (n = 685). Class 1 was characterized by immunosuppression with higher mortality than class 2 (21.8% [70/321] vs. 12.1% [44/364]; p < 0.01 for Chi-square test). A 5-gene class model (C14orf159, AKNA, PILRA, STOM and USP4) was developed with GA. In external validation cohorts, the 5-gene class model (AUC: 0.707; 95% CI: 0.664 - 0.750) performed better in predicting mortality than sepsis response signature (SRS) endotypes (AUC: 0.610; 95% CI: 0.521 - 0.700), and performed equivalently to the APACHE II score (AUC: 0.681; 95% CI: 0.595 - 0.767). In the dataset E-MTAB-7581, the use of hydrocortisone was associated with increased risk of mortality (OR: 3.15 [1.13, 8.82]; p = 0.029) in class 2. The effect was not statistically significant in class 1 (OR: 1.88 [0.70, 5.09]; p = 0.211). INTERPRETATION: Our study identified two classes of sepsis that showed different mortality rates and responses to hydrocortisone therapy. Class 1 was characterized by immunosuppression with higher mortality rate than class 2. We further developed a 5-gene class model to predict class membership. FUNDING: The study was funded by the National Natural Science Foundation of China (Grant No. 81,901,929).",
        "Internet of Things (IoT) is becoming a new socioeconomic revolution in which data and immediacy are the main ingredients. IoT generates large datasets on a daily basis but it is currently considered as \"dark data\", i.e., data generated but never analyzed. The efficient analysis of this data is mandatory to create intelligent applications for the next generation of IoT applications that benefits society. Artificial Intelligence (AI) techniques are very well suited to identifying hidden patterns and correlations in this data deluge. In particular, clustering algorithms are of the utmost importance for performing exploratory data analysis to identify a set (a.k.a., cluster) of similar objects. Clustering algorithms are computationally heavy workloads and require to be executed on high-performance computing clusters, especially to deal with large datasets. This execution on HPC infrastructures is an energy hungry procedure with additional issues, such as high-latency communications or privacy. Edge computing is a paradigm to enable light-weight computations at the edge of the network that has been proposed recently to solve these issues. In this paper, we provide an in-depth analysis of emergent edge computing architectures that include low-power Graphics Processing Units (GPUs) to speed-up these workloads. Our analysis includes performance and power consumption figures of the latest Nvidia's AGX Xavier to compare the energy-performance ratio of these low-cost platforms with a high-performance cloud-based counterpart version. Three different clustering algorithms (i.e., k-means, Fuzzy Minimals (FM), and Fuzzy C-Means (FCM)) are designed to be optimally executed on edge and cloud platforms, showing a speed-up factor of up to 11x for the GPU code compared to sequential counterpart versions in the edge platforms and energy savings of up to 150% between the edge computing and HPC platforms.",
        "The effect of pulsed electric fields (PEFs) on transmembrane proteins is not fully understood; how do chemo-mechanical cues in the microenvironment mediate the electric field sensing by these proteins? To answer this key gap in knowledge, we have developed a kinetic Monte Carlo statistical model of the integrin proteins that integrates three components of the morphogenetic field (i.e., chemical, mechanical, and electrical cues). Specifically, the model incorporates the mechanical stiffness of the cell membrane, the ligand density of the extracellular environment, the glycocalyx stiffness, thermal Brownian motion, and electric field induced diffusion. The effects of both steady-state electric fields and transient PEF pulse trains on integrin clustering are studied. Our results reveal that electric-field-driven integrin clustering is mediated by membrane stiffness and ligand density. In addition, we explore the effects of PEF pulse-train parameters (amplitude, polarity, and pulse-width) on integrin clustering. In summary, we demonstrate a computational methodology to incorporate experimental data and simulate integrin clustering when exposed to PEFs for time-scales comparable to experiments (seconds-minutes). Thus, we propose a blueprint for understanding PEF/electric field effects on protein induced signaling and highlight key impediments to incorporating experimental values into computational models such as the kinetic Monte Carlo method.",
        "One of the visions of precision medicine has been to re-define disease taxonomies based on molecular characteristics rather than on phenotypic evidence. However, achieving this goal is highly challenging, specifically in neurology. Our contribution is a machine-learning based joint molecular subtyping of Alzheimer's (AD) and Parkinson's Disease (PD), based on the genetic burden of 15 molecular mechanisms comprising 27 proteins (e.g. APOE) that have been described in both diseases. We demonstrate that our joint AD/PD clustering using a combination of sparse autoencoders and sparse non-negative matrix factorization is reproducible and can be associated with significant differences of AD and PD patient subgroups on a clinical, pathophysiological and molecular level. Hence, clusters are disease-associated. To our knowledge this work is the first demonstration of a mechanism based stratification in the field of neurodegenerative diseases. Overall, we thus see this work as an important step towards a molecular mechanism-based taxonomy of neurological disorders, which could help in developing better targeted therapies in the future by going beyond classical phenotype based disease definitions.",
        "Using a prior biological knowledge of relationships and genetic functions for gene similarity, from repository such as the Gene Ontology (GO), has shown good results in multi-objective gene clustering algorithms. In this scenario and to obtain useful clustering results, it would be helpful to know which measure of biological similarity between genes should be employed to yield meaningful clusters that have both similar expression patterns (co-expression) and biological homogeneity. In this paper, we studied the influence of the four most used GO-based semantic similarity measures in the performance of a multi-objective gene clustering algorithm. We used four publicly available datasets and carried out comparative studies based on performance metrics for the multi-objective optimization field and clustering performance indexes. In most of the cases, using Jiang-Conrath and Wang similarities stand in terms of multi-objective metrics. In clustering properties, Resnik similarity allows to achieve the best values of compactness and separation and therefore of co-expression of groups of genes. Meanwhile, in biological homogeneity, the Wang similarity reports greater number of significant GO terms. However, statistical, visual, and biological significance tests showed that none of the GO-based semantic similarity measures stand out above the rest in order to significantly improve the performance of the multi-objective gene clustering algorithm.",
        "SUMMARY: We implemented the Self-Organizing Maps algorithm running efficiently on GPUs, and also provide several clustering methods of the resulting maps. We provide scripts and a use case to cluster macro-molecular conformations generated by molecular dynamics simulations. AVAILABILITY AND IMPLEMENTATION: The method is available on GitHub and distributed as a pip package.",
        "BACKGROUND AND OBJECTIVE: Brain tumor segmentation is a challenging issue due to noise, artifact, and intensity non-uniformity in magnetic resonance images (MRI). Manual MRI segmentation is a very tedious, time-consuming, and user-dependent task. This paper aims to presents a novel level set method to address aforementioned challenges for reliable and automatic brain tumor segmentation. METHODS: In the proposed method, a new functional, based on level set method, is presented for medical image segmentation. Firstly, we define a superpixel fuzzy clustering objective function. To create superpixel regions, multiscale morphological gradient reconstruction (MMGR) operation is used. Secondly, a novel fuzzy energy functional is defined based on superpixel segmentation and histogram computation. Then, level set equations are obtained by using gradient descent method. Finally, we solve the level set equations by using lattice Boltzmann method (LBM). To evaluate the performance of the proposed method, both synthetic image dataset and real Glioma brain tumor images from BraTS 2017 dataset are used. RESULTS: Experiments indicate that our proposed method is robust to noise, initialization, and intensity non-uniformity. Moreover, it is faster and more accurate than other state-of-the-art segmentation methods with the averages of running time is 3.25 seconds, Dice and Jaccard coefficients for automatic tumor segmentation against ground truth are 0.93 and 0.87, respectively. The mean value of Hausdorff distance, Mean absolute Distance (MAD), accuracy, sensitivity, and specificity are 2.70, 0.005, 0.9940, 0.9183, and 0.9972, respectively. CONCLUSIONS: Our proposed method shows satisfactory results for Glioma brain tumor segmentation due to superpixel fuzzy clustering accurate segmentation results. Moreover, our method is fast and robust to noise, initialization, and intensity non-uniformity. Since most of the medical images suffer from these problems, the proposed method can more effective for complicated medical image segmentation.",
        "Computer-assisted algorithms have become a mainstay of biomedical applications to improve accuracy and reproducibility of repetitive tasks like manual segmentation and annotation. We propose a novel pipeline for red blood cell detection and counting in thin blood smear microscopy images, named RBCNet, using a dual deep learning architecture. RBCNet consists of a U-Net first stage for cell-cluster or superpixel segmentation, followed by a second refinement stage Faster R-CNN for detecting small cell objects within the connected component clusters. RBCNet uses cell clustering instead of region proposals, which is robust to cell fragmentation, is highly scalable for detecting small objects or fine scale morphological structures in very large images, can be trained using non-overlapping tiles, and during inference is adaptive to the scale of cell-clusters with a low memory footprint. We tested our method on an archived collection of human malaria smears with nearly 200,000 labeled cells across 965 images from 193 patients, acquired in Bangladesh, with each patient contributing five images. Cell detection accuracy using RBCNet was higher than 97 %. The novel dual cascade RBCNet architecture provides more accurate cell detections because the foreground cell-cluster masks from U-Net adaptively guide the detection stage, resulting in a notably higher true positive and lower false alarm rates, compared to traditional and other deep learning methods. The RBCNet pipeline implements a crucial step towards automated malaria diagnosis.",
        "Crystal orientation mapping experiments typically measure orientations that are similar within grains and misorientations that are similar along grain boundaries. Such (mis)orientation data cluster in (mis)orientation space, and clusters are more pronounced if preferred orientations or special orientation relationships are present. Here, cluster analysis of (mis)orientation data is described and demonstrated using distance metrics incorporating crystal symmetry and the density-based clustering algorithm DBSCAN. Frequently measured (mis)orientations are identified as corresponding to similarly (mis)oriented grains or grain boundaries, which are visualized both spatially and in three-dimensional (mis)orientation spaces. An example is presented identifying deformation twinning modes in titanium, highlighting a key application of the clustering approach in identifying crystallographic orientation relationships and similarly oriented grains resulting from specific transformation pathways. A new open-source Python library, orix, that enabled this work is also reported.",
        "Wine data are usually characterized by high variability, in terms of compounds and concentration ranges. Chemometric methods can be efficiently used to extract and exploit the meaningful information contained in such data. Therefore, the fuzzy divisive hierarchical associative-clustering (FDHAC) method was efficiently applied in this study, for the classification of several varieties of Romanian white wines, using the elemental profile (concentrations of 30 elements analyzed by ICP-MS). The investigated wines were produced in four different geographical areas of Romania (Transylvania, Moldova, Muntenia and Oltenia). The FDHAC algorithm provided not only a fuzzy partition of the investigated white wines, but also a fuzzy partition of considered characteristics. Furthermore, this method is unique because it allows a 3D bi-plot representation of membership degrees corresponding to wine samples and elements. In this way, it was possible to identify the most specific elements (in terms of highest, smallest or intermediate concentration values) to each fuzzy partition (group) of wine samples. The chemical elements that appeared to be more powerful for the differentiation of the wines produced in different Romanian areas were: K, Rb, P, Ca, B, Na.",
        "In recent years, deep learning models have achieved remarkable successes in various applications, such as pattern recognition, computer vision, and signal processing. However, high-performance deep architectures are often accompanied by a large storage space and long computational time, which make it difficult to fully exploit many deep neural networks (DNNs), especially in scenarios in which computing resources are limited. In this paper, to tackle this problem, we introduce a method for compressing the structure and parameters of DNNs based on neuron agglomerative clustering (NAC). Specifically, we utilize the agglomerative clustering algorithm to find similar neurons, while these similar neurons and the connections linked to them are then agglomerated together. Using NAC, the number of parameters and the storage space of DNNs are greatly reduced, without the support of an extra library or hardware. Extensive experiments demonstrate that NAC is very effective for the neuron agglomeration of both the fully connected and convolutional layers, which are common building blocks of DNNs, delivering similar or even higher network accuracy. Specifically, on the benchmark CIFAR-10 and CIFAR-100 datasets, using NAC to compress the parameters of the original VGGNet by 92.96% and 81.10%, respectively, the compact network obtained still outperforms the original networks.",
        "Clustering and community detection provide a concise way of extracting meaningful information from large datasets. An ever growing plethora of data clustering and community detection algorithms have been proposed. In this paper, we address the question of ranking the performance of clustering algorithms for a given dataset. We show that, for hard clustering and community detection, Linsker's Infomax principle can be used to rank clustering algorithms. In brief, the algorithm that yields the highest value of the entropy of the partition, for a given number of clusters, is the best one. We show indeed, on a wide range of datasets of various sizes and topological structures, that the ranking provided by the entropy of the partition over a variety of partitioning algorithms is strongly correlated with the overlap with a ground truth partition The codes related to the project are available in https://github.com/Sandipan99/Ranking_cluster_algorithms.",
        "MOTIVATION: The rapid development of single-cell RNA sequencing (scRNA-seq) technologies allows us to explore tissue heterogeneity at the cellular level. The identification of cell types plays an essential role in the analysis of scRNA-seq data, which, in turn, influences the discovery of regulatory genes that induce heterogeneity. As the scale of sequencing data increases, the classical method of combining clustering and differential expression analysis to annotate cells becomes more costly in terms of both labor and resources. Existing scRNA-seq supervised classification method can alleviate this issue through learning a classifier trained on the labeled reference data and then making a prediction based on the unlabeled target data. However, such label transference strategy carries with risks, such as susceptibility to batch effect and further compromise of inherent discrimination of target data. RESULTS: In this article, inspired by unsupervised domain adaptation, we propose a flexible single cell semi-supervised clustering and annotation framework, scSemiCluster, which integrates the reference data and target data for training. We utilize structure similarity regularization on the reference domain to restrict the clustering solutions of the target domain. We also incorporates pairwise constraints in the feature learning process such that cells belonging to the same cluster are close to each other, and cells belonging to different clusters are far from each other in the latent space. Notably, without explicit domain alignment and batch effect correction, scSemiCluster outperforms other state-of-the-art, single-cell supervised classification and semi-supervised clustering annotation algorithms in both simulation and real data. To the best of our knowledge, we are the first to use both deep discriminative clustering and deep generative clustering techniques in the single-cell field. AVAILABILITYAND IMPLEMENTATION: An implementation of scSemiCluster is available from https://github.com/xuebaliang/scSemiCluster. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.",
        "BACKGROUND: Single individual haplotype problem refers to reconstructing haplotypes of an individual based on several input fragments sequenced from a specified chromosome. Solving this problem is an important task in computational biology and has many applications in the pharmaceutical industry, clinical decision-making, and genetic diseases. It is known that solving the problem is NP-hard. Although several methods have been proposed to solve the problem, it is found that most of them have low performances in dealing with noisy input fragments. Therefore, proposing a method which is accurate and scalable, is a challenging task. RESULTS: In this paper, we introduced a method, named NCMHap, which utilizes the Neutrosophic c-means (NCM) clustering algorithm. The NCM algorithm can effectively detect the noise and outliers in the input data. In addition, it can reduce their effects in the clustering process. The proposed method has been evaluated by several benchmark datasets. Comparing with existing methods indicates when NCM is tuned by suitable parameters, the results are encouraging. In particular, when the amount of noise increases, it outperforms the comparing methods. CONCLUSION: The proposed method is validated using simulated and real datasets. The achieved results recommend the application of NCMHap on the datasets which involve the fragments with a huge amount of gaps and noise.",
        "Modeling and analyzing human microbiome allows the assessment of the microbial community and its impacts on human health. Microbiome composition can be quantified using 16S rRNA technology into sequencing data, which are usually skewed and heavy-tailed with excess zeros. Clustering methods are useful in personalized medicine by identifying subgroups for patients stratification. However, there is currently a lack of standardized clustering method for the complex microbiome sequencing data. We propose a clustering algorithm with a specific beta diversity measure that can address the presence-absence bias encountered for sparse count data and effectively measure the sample distances for sample stratification. Our distance measure used for clustering is derived from a parametric based mixture model producing sample-specific distributions conditional on the observed operational taxonomic unit (OTU) counts and estimated mixture weights. The method can provide accurate estimates of the true zero proportions and thus construct a precise beta diversity measure. Extensive simulation studies have been conducted and suggest that the proposed method achieves substantial clustering improvement compared with some widely used distance measures when a large proportion of zeros is presented. The proposed algorithm was implemented to a human gut microbiome study on Parkinson's diseases to identify distinct microbiome states with biological interpretations.",
        "Color-based image segmentation classifies pixels of digital images in numerous groups for further analysis in computer vision, pattern recognition, image understanding, and image processing applications. Various algorithms have been developed for image segmentation, but clustering algorithms play an important role in the segmentation of digital images. This paper presents a novel and adaptive initialization approach to determine the number of clusters and find the initial central points of clusters for the standard K-means algorithm to solve the segmentation problem of color images. The presented scheme uses a scanning procedure of the paired Red, Green, and Blue (RGB) color-channel histograms for determining the most salient modes in every histogram. Next, the histogram thresholding is applied and a search in every histogram mode is performed to accomplish RGB pairs. These RGB pairs are used as the initial cluster centers and cluster numbers that clustered each pixel into the appropriate region for generating the homogeneous regions. The proposed technique determines the best initialization parameters for the conventional K-means clustering technique. In this paper, the proposed approach was compared with various unsupervised image segmentation techniques on various image segmentation benchmarks. Furthermore, we made use of a ranking approach inspired by the Evaluation Based on Distance from Average Solution (EDAS) method to account for segmentation integrity. The experimental results show that the proposed technique outperforms the other existing clustering techniques by optimizing the segmentation quality and possibly reducing the classification error.",
        "The misclassification error distance and the adjusted Rand index are two of the most common criteria used to evaluate the performance of clustering algorithms. This paper provides an in-depth comparison of the two criteria, with the aim of better understand exactly what they measure, their properties and their differences. Starting from their population origins, the investigation includes many data analysis examples and the study of particular cases in great detail. An exhaustive simulation study provides insight into the criteria distributions and reveals some previous misconceptions.",
        "BACKGROUND: Vitamin K antagonist (warfarin) is the most classical and widely used oral anticoagulant with assuring anticoagulant effect, wide clinical indications and low price. Warfarin dosage requirements of different patients vary largely. For warfarin daily dosage prediction, the data imbalance in dataset leads to inaccurate prediction on the patients of rare genotype, who usually have large stable dosage requirement. To balance the dataset of patients treated with warfarin and improve the predictive accuracy, an appropriate partition of majority and minority groups, together with an oversampling method, is required. METHOD: To solve the data-imbalance problem mentioned above, we developed a clustering-based oversampling technique denoted as DBCSMOTE, which combines density-based spatial clustering of application with noise (DBCSCAN) and synthetic minority oversampling technique (SMOTE). DBCSMOTE automatically finds the minority groups by acquiring the association between samples in terms of the clinical features/genotypes and the warfarin dosage, and creates an extended dataset by adding the new synthetic samples of majority and minority groups. Meanwhile, two ensemble models, boosted regression tree (BRT) and random forest (RF), which are built on the extended dataset generateed by DBCSMOTE, accomplish the task of warfarin daily dosage prediction. RESULTS: DBCSMOTE and the comparison methods were tested on the datasets derived from our Hospital and International Warfarin Pharmacogenetics Consortium (IWPC). As the results, DBCSMOTE-BRT obtained the highest R-squared (R(2)) of 0.424 and the smallest mean squared error (mse) of 1.08. In terms of the percentage of patients whose predicted dose of warfarin is within 20% of the actual stable therapeutic dose (20%-p), DBCSMOTE-BRT can achieve the largest value of 47.8% among predictive models. The more important thing is that DBCSMOTE saved about 68% computational time to achieve the same or better performance than the Evolutionary SMOTE, which was the best oversampling method in warfarin dose prediction by far. Meanwhile, in warfarin dose prediction, it is discovered that DBCSMOTE is more effective in integrating BRT than RF for warfarin dose prediction. CONCLUSION: Our finding is that the genotypes, CYP2C9 and VKORC1, no doubt contribute to the predictive accuracy. It was also discovered left atrium diameter, glutamic pyruvic transaminase and serum creatinine included in the model actually improved the predictive accuracy; When congestive heart failure, diabetes mellitus and valve replacement were absent in DBCSMOTE-BRT/RF, the predictive accuracy of DBCSMOTE-BRT/RF decreased. The oversampling ratio and number of minority clusters have a large impact on the effect of oversampling. According to our test, the predictive accuracy was high when the number of minority clusters was 6 ~ 8. The oversampling ratio for small minority clusters should be large (> 1.2) and for large minority clusters should be small (< 0.2). If the dataset becomes larger, the DBCSMOTE would be re-optimized and its BRT/RF model should be re-trained. DBCSMOTE-BRT/RF outperformed the current commonly-used tool called Warfarindosing. As compared to Evolutionary SMOTE-BRT and RF models, DBCSMOTE-BRT and RF models take only a small computational time to achieve the same or higher performance in many cases. In terms of predictive accuracy, RF is not as good as BRT. However, RF still has a powerful ability in generating a highly accurate model as the dataset increases; the software \"WarfarinSeer v2.0\" is a test version, which packed DBCSMOTE-BRT/RF. It could be a convenient tool for clinical application in warfarin treatment.",
        "Multiview clustering (MVC) has recently been the focus of much attention due to its ability to partition data from multiple views via view correlations. However, most MVC methods only learn either interfeature correlations or intercluster correlations, which may lead to unsatisfactory clustering performance. To address this issue, we propose a novel dual-correlated multivariate information bottleneck (DMIB) method for MVC. DMIB is able to explore both interfeature correlations (the relationship among multiple distinct feature representations from different views) and intercluster correlations (the close agreement among clustering results obtained from individual views). For the former, we integrate both view-shared feature correlations discovered by learning a shared discriminative feature subspace and view-specific feature information to fully explore the interfeature correlation. This allows us to attain multiple reliable local clustering results of different views. Following this, we explore the intercluster correlations by learning the shared mutual information over different local clusterings for an improved global partition. By integrating both correlations, we formulate the problem as a unified information maximization function and further design a two-step method for optimization. Moreover, we theoretically prove the convergence of the proposed algorithm, and discuss the relationships between our method and several existing clustering paradigms. The experimental results on multiple datasets demonstrate the superiority of DMIB compared to several state-of-the-art clustering methods.",
        "The robustness of networks against node failure and the response of networks to node removal has been studied extensively for networks such as transportation networks, power grids, and food webs. In many cases, a network's clustering coefficient was identified as a good indicator for network robustness. In ecology, habitat networks constitute a powerful tool to represent metapopulations or -communities, where nodes represent habitat patches and links indicate how these are connected. Current climate and land-use changes result in decline of habitat area and its connectivity and are thus the main drivers for the ongoing biodiversity loss. Conservation efforts are therefore needed to improve the connectivity and mitigate effects of habitat loss. Habitat loss can easily be modelled with the help of habitat networks and the question arises how to modify networks to obtain higher robustness. Here, we develop tools to identify which links should be added to a network to increase the robustness. We introduce two different heuristics, Greedy and Lazy Greedy, to maximize the clustering coefficient if multiple links can be added. We test these approaches and compare the results to the optimal solution for different generic networks including a variety of standard networks as well as spatially explicit landscape based habitat networks. In a last step, we simulate the robustness of habitat networks before and after adding multiple links and investigate the increase in robustness depending on both the number of added links and the heuristic used. We found that using our heuristics to add links to sparse networks such as habitat networks has a greater impact on the clustering coefficient compared to randomly adding links. The Greedy algorithm delivered optimal results in almost all cases when adding two links to the network. Furthermore, the robustness of networks increased with the number of additional links added using the Greedy or Lazy Greedy algorithm.",
        "Clustering frequency vectors is a challenging task on large data sets considering its high dimensionality and sparsity nature. Generalized Dirichlet multinomial (GDM) distribution is a competitive generative model for count data in terms of accuracy, yet its parameters estimation process is slow. The exponential-family approximation of the multivariate Polya distribution has shown to be efficient to train and cluster data directly, without dimensionality reduction. In this article, we derive an exponential-family approximation to the GDM distributions, and we call it (EGDM). A mixture model is developed based on the new member of the exponential-family of distributions, and its parameters are learned through the deterministic annealing expectation-maximization (DAEM) approach as a new clustering algorithm for count data. Moreover, we propose to estimate the optimal number of EGDM mixture components based on the minimum message length (MML) criterion. We have conducted a set of empirical experiments, concerning text, image, and video clustering, to evaluate the proposed approach performance. Results show that the new model attains a superior performance, and it is considerably faster than the corresponding method for GDM distributions.",
        "The coronavirus pandemic became a major risk in global public health. The outbreak is caused by SARS-CoV-2, a member of the coronavirus family. Though the images of the virus are familiar to us, in the present study, an attempt is made to hear the coronavirus by translating its protein spike into audio sequences. The musical features such as pitch, timbre, volume and duration are mapped based on the coronavirus protein sequence. Three different viruses Influenza, Ebola and Coronavirus were studied and compared through their auditory virus sequences by implementing Haar wavelet transform. The sonification of the coronavirus benefits in understanding the protein structures by enhancing the hidden features. Further, it makes a clear difference in the representation of coronavirus compared with other viruses, which will help in various research works related to virus sequence. This evolves as a simplified and novel way of representing the conventional computational methods.",
        "Inflammatory diseases can be treated by inhibiting 5-lipo-oxygenase activating protein (FLAP). In this study, a data set containing 2,112 FLAP inhibitors was collected. A total of 25 classification models were built by five machine learning algorithms with five different types of fingerprints. The best model, which was built by support vector machine algorithm with ECFP_4 fingerprint had an accuracy and a Matthews correlation coefficient of 0.862 and 0.722 on the test set, respectively. The predicted results were further evaluated by the application domain dSTD-PRO (a distance between one compound to models). Each compound had a dSTD-PRO value, which was calculated by the predicted probabilities obtained from all 25 models. The application domain results suggested that the reliability of predicted results depended mainly on the compounds themselves rather than algorithms or fingerprints. A group of customized 10-bit fingerprint was manually defined for clustering the molecular structures of 2,112 FLAP inhibitors into eight subsets by K-Means. According to the clustering results, most of inhibitors in two subsets (subsets 2 and 4) were highly active inhibitors. We found that aryl oxadiazole/oxazole alkanes, biaryl amino-heteroarenes, two aromatic rings (often N-containing) linked by a cyclobutene group, and 1,2,4-triazole group were typical fragments in highly active inhibitors.",
        "SUMMARY: Recent advancements in high-dimensional single-cell technologies, such as mass cytometry, enable longitudinal experiments to track dynamics of cell populations and identify change points where the proportions vary significantly. However, current research is limited by the lack of tools specialized for analyzing longitudinal mass cytometry data. In order to infer cell population dynamics from such data, we developed a statistical framework named CYBERTRACK2.0. The framework's analytic performance was validated against synthetic and real data, showing that its results are consistent with previous research. AVAILABILITY AND IMPLEMENTATION: CYBERTRACK2.0 is available at https://github.com/kodaim1115/CYBERTRACK2. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "With the enormous amount of multi-source data produced by various sensors and feature extraction approaches, multi-view clustering (MVC) has attracted developing research attention and is widely exploited in data analysis. Most of the existing multi-view clustering methods hold on the assumption that all of the views are complete. However, in many real scenarios, multi-view data are often incomplete for many reasons, e.g., hardware failure or incomplete data collection. In this paper, we propose an adaptive weighted graph fusion incomplete multi-view subspace clustering (AWGF-IMSC) method to solve the incomplete multi-view clustering problem. Firstly, to eliminate the noise existing in the original space, we transform complete original data into latent representations which contribute to better graph construction for each view. Then, we incorporate feature extraction and incomplete graph fusion into a unified framework, whereas two processes can negotiate with each other, serving for graph learning tasks. A sparse regularization is imposed on the complete graph to make it more robust to the view-inconsistency. Besides, the importance of different views is automatically learned, further guiding the construction of the complete graph. An effective iterative algorithm is proposed to solve the resulting optimization problem with convergence. Compared with the existing state-of-the-art methods, the experiment results on several real-world datasets demonstrate the effectiveness and advancement of our proposed method.",
        "Clustering is a fundamental problem that frequently arises in many fields, such as pattern recognition, data mining, and machine learning. Although various clustering algorithms have been developed in the past, traditional clustering algorithms with shallow structures cannot excavate the interdependence of complex data features in latent space. Recently, deep generative models, such as autoencoder (AE), variational AE (VAE), and generative adversarial network (GAN), have achieved remarkable success in many unsupervised applications thanks to their capabilities for learning promising latent representations from original data. In this work, first we propose a novel clustering approach based on both Wasserstein GAN with gradient penalty (WGAN-GP) and VAE with a Gaussian mixture prior. By combining the WGAN-GP with VAE, the generator of WGAN-GP is formulated by drawing samples from the probabilistic decoder of VAE. Moreover, to provide more robust clustering and generation performance when outliers are encountered in data, a variant of the proposed deep generative model is developed based on a Student's-t mixture prior. The effectiveness of our deep generative models is validated though experiments on both clustering analysis and samples generation. Through the comparison with other state-of-art clustering approaches based on deep generative models, the proposed approach can provide more stable training of the model, improve the accuracy of clustering, and generate realistic samples.",
        "Machine learning algorithms have recently shown their precision and potential in many different use cases and fields of medicine. Most of the algorithms used are supervised and need a large quantity of labeled data to achieve high accuracy. Also, most applications of machine learning in medicine are attempts to mimic or exceed human diagnostic capabilities but little work has been done to show the power of these algorithms to help collect and pre-process a large amount of data. In this study we show how unsupervised learning can extract and sort usable data from large unlabeled datasets with minimal human intervention. Our digital examination tools used in clinical practice store such databases and are largely under-exploited. We applied unsupervised algorithms to corneal topography examinations which remains the gold standard test for diagnosis and follow-up of many corneal diseases and refractive surgery screening. We could extract 7019 usable examinations which were automatically sorted in 3 common diagnoses (Normal, Keratoconus and History of Refractive Surgery) from an unlabeled database with an overall accuracy of 96.5%. Similar methods could be used on any form of digital examination database and greatly speed up the data collection process and yield to the elaboration of stronger supervised models.",
        "Characterization of lipids by matrix-assisted laser desorption ionization mass spectrometry imaging (MALDI-MSI) is of great interest because not only are lipids important structural molecules in both the cell and internal organelle membranes, but they are also important signaling molecules. MALDI-MSI combined with spatial image segmentation has been previously used to identify tumor heterogeneities within tissues with distinct anatomical regions such as the brain. However, there has been no systematic study utilizing MALDI-MSI combined with spatial image segmentation to assess the tumor microenvironment in the liver. Here, we present that image segmentation can be used to evaluate the tumor microenvironment in the liver. In particular, to better understand the molecular mechanisms of irradiation-induced hepatic carcinogenesis, we used MALDI-MSI in the negative ion mode to identify lipid changes 12 months post exposure to low dose (28)Si and (137)Cs gamma ray irradiation. We report here the changes in the lipid profiles of male C3H/HeNCrl mice liver tissues after exposure to irradiation and analyzed using the spatial shrunken centroid clustering algorithm. These findings provide valuable information as astronauts will be exposed to high-charge high-energy (HZE) particles and low-energy gamma-ray irradiation during deep space travel. Even at low doses, exposure to these irradiations can lead to cancer. Previous studies infer that irradiation of mice with low-dose HZE particles induces oxidative damage and microenvironmental changes that are thought to play roles in the pathophysiology of hepatocellular carcinoma.",
        "Component fraction (CF) is one of the most important parameters in multiple-phase flow. Due to the complexity of the solid-liquid two-phase flow, the CF estimation remains unsolved both in scientific research and industrial application for a long time. Electrical resistance tomography (ERT) is an advanced type of conductivity detection technique due to its low-cost, fast-response, non-invasive, and non-radiation characteristics. However, when the existing ERT method is used to measure the CF value in solid-liquid two-phase flow in dredging engineering, there are at least three problems: (1) the dependence of reference distribution whose CF value is zero; (2) the size of the detected objects may be too small to be found by ERT; and (3) there is no efficient way to estimate the effect of artifacts in ERT. In this paper, we proposed a method based on the clustering technique, where a fast-fuzzy clustering algorithm is used to partition the ERT image to three clusters that respond to liquid, solid phases, and their mixtures and artifacts, respectively. The clustering algorithm does not need any reference distribution in the CF estimation. In the case of small solid objects or artifacts, the CF value remains effectively computed by prior information. To validate the new method, a group of typical CF estimations in dredging engineering were implemented. Results show that the new method can effectively overcome the limitations of the existing method, and can provide a practical and more accurate way for CF estimation.",
        "This paper presents a model-based method for clustering multivariate binary observations that incorporates constraints consistent with the scientific context. The approach is motivated by the precision medicine problem of identifying autoimmune disease patient subsets or classes who may require different treatments. We start with a family of restricted latent class models or RLCMs. However, in the motivating example and many others like it, the unknown number of classes and the definition of classes using binary states are among the targets of inference. We use a Bayesian approach to RLCMs in order to use informative prior assumptions on the number and definitions of latent classes to be consistent with scientific knowledge so that the posterior distribution tends to concentrate on smaller numbers of clusters and sparser binary patterns. The paper derives a posterior sampling algorithm based on Markov chain Monte Carlo with split-merge updates to efficiently explore the space of clustering allocations. Through simulations under the assumed model and realistic deviations from it, we demonstrate greater interpretability of results and superior finite-sample clustering performance for our method compared to common alternatives. The methods are illustrated with an analysis of protein data to detect clusters representing autoantibody classes among scleroderma patients.",
        "BACKGROUND: Advances in single-cell RNA-seq technology have led to great opportunities for the quantitative characterization of cell types, and many clustering algorithms have been developed based on single-cell gene expression. However, we found that different data preprocessing methods show quite different effects on clustering algorithms. Moreover, there is no specific preprocessing method that is applicable to all clustering algorithms, and even for the same clustering algorithm, the best preprocessing method depends on the input data. RESULTS: We designed a graph-based algorithm, SC3-e, specifically for discriminating the best data preprocessing method for SC3, which is currently the most widely used clustering algorithm for single cell clustering. When tested on eight frequently used single-cell RNA-seq data sets, SC3-e always accurately selects the best data preprocessing method for SC3 and therefore greatly enhances the clustering performance of SC3. CONCLUSION: The SC3-e algorithm is practically powerful for discriminating the best data preprocessing method, and therefore largely enhances the performance of cell-type clustering of SC3. It is expected to play a crucial role in the related studies of single-cell clustering, such as the studies of human complex diseases and discoveries of new cell types.",
        "One of the major challenges in analyzing large scale intracellular calcium spiking data obtained through fluorescent imaging is to identify various patterns present in time series data. Such an analysis identifying the distinct frequency and amplitude encoding during cell-drug interaction study is expected to provide new insights into the drug action patterns over a time course. Here, we present the HDBSCAN clustering algorithm to find a clustering pattern present in calcium spiking obtained by confocal imaging of single cells. Our methodology uncovers the specific templates present in dynamic responses obtained through treatment with multiple doses of the drug. First, we attempt to visualize the clustering pattern present in time-series data corresponding to various doses of the drug. Secondly, we show that the HDBSCAN can be used for the detection of specific signatures corresponding to low and high cell density regions selected from in vitro experiments. To the best of our knowledge, this is the first attempt to optimize the clustering of calcium dynamics using HDBSCAN. Finally, we emphasize that HDBSCAN offers a high-level grasp on systems biology data, including complex spiking pattern and can be used as a visual analytic tool for drug dose selection.",
        "Cervical spinal cord injury (cSCI) can cause paralysis and impair hand function. Existing assessments in clinical settings do not reflect an individual's performance in their daily environment. Videos from wearable cameras (egocentric video) provide a novel avenue to analyze hand function in non-clinical settings. Due to the large amounts of video data generated by this approach, automated analysis methods are necessary. We propose to employ an unsupervised learning process to produce a summary of the grasping strategies used in an egocentric video. To this end, an approach was developed consisting of hand detection, pose estimation, and clustering algorithms. The performance of the method was examined with external evaluation indicators and internal evaluation indicators for an uninjured and injured participant, respectively. The results demonstrated that a Gaussian mixture model obtained the highest accuracy in terms of the maximum match, 0.63, and the Rand index, 0.26, for the uninjured participant, and a silhouette score of 0.13 for the injured participant.",
        "This work presents an effective multiple subject clustering method using whole-brain tractography datasets. The method is able to obtain fiber clusters that are representative of the population. The proposed approach first applies a fast intra-subject clustering algorithm on each subject obtaining the cluster centroids for all subjects. Second, it compresses the collection of centroids to a latent space through the encoder of a trained autoencoder. Finally, it uses a modified HDBSCAN with adjusted parameters on the encoded centroids of all subjects to obtain the final inter-subject clusters. The results shows that the proposed method outperforms other clustering strategies, and it is able to retrieve known fascicles in a reasonable execution time, achieving a precision over 87% and F1 score above 86% on a collection of 20 simulated subjects.",
        "Over a third of patients suffering from epilepsy continue to live with recurrent disabling seizures and would greatly benefit from personalized seizure forecasting. While electroencephalography (EEG) remains most popular for studying subject-specific epileptic precursors, dysfunctions of the autonomous nervous system, notably cardiac activity measured in heart rate variability (HRV), have also been associated with epileptic seizures. This work proposes an unsupervised clustering technique which aims to automatically identify preictal HRV changes in 9 patients who underwent simultaneous electrocardiography (ECG) and intracranial EEG presurgical monitoring at the University of Montreal Hospital Center. A 2-class k-means clustering combined with a quantitative preictal HRV change detection technique were adopted in a subject- and seizure-specific manner. Results indicate inter and intra-patient variability in preictal HRV changes (between 3.5 and 6.5 min before seizure onset) and a statistically significant negative correlation between the time of change in HRV state and the duration of seizures (p<0.05). The presented findings show promise for new avenues of research regarding multimodal seizure prediction and unsupervised preictal time assessment.Clinical Relevance- This study proposed an unsupervised technique for quantitatively identifying preictal HRV changes which can be eventually used to implement an ECG-based seizure forecasting algorithm.",
        "The EMG signal is very difficult to classify due to the stochastic complexity of its characteristics. A way to reduce the complexity of a signal is to use clusters to resize them to a smaller space and then perform the classification. A classification improvement was verified by clustering the electromyographic signal and comparing it with the possible movements that can be performed. In this study, the Agglomerative Hierarchical Clustering was used. The basic idea is to give prior information to the final classifier so the posterior classification has fewer classes, diminishing his complexity. Through the methodology applied in this article, an accuracy of more than 90% was achieved by using a time window of only 10 ms in a signal sampled at 2000 Hz. Experimentation confirms that the methods presented in this paper are competitive with other methods presented in the literature.",
        "Automatically detecting and removing Electroencephalogram (EEG) outliers is essential to design robust brain-computer interfaces (BCIs). In this paper, we propose a novel outlier detection method that works on the Riemannian manifold of sample covariance matrices (SCMs). Existing outlier detection methods run the risk of erroneously rejecting some samples as outliers, even if there is no outlier, due to the detection being based on a reference matrix and a threshold. To address this limitation, our method, Riemannian Spectral Clustering (RiSC), detects outliers by clustering SCMs into non-outliers and outliers, based on a proposed similarity measure. This considers the Riemannian geometry of the space and magnifies the similarity within the non-outlier cluster and weakens it between non-outlier and outlier clusters, instead of setting a threshold. To assess RiSC performance, we generated artificial EEG datasets contaminated by different outlier strengths and numbers. Comparing Hit-False (HF) difference between RiSC and existing outlier detection methods confirmed that RiSC could detect outliers significantly better (p < 0.001). In particular, RiSC improved HF difference the most for datasets with the most severe outlier contamination.",
        "Transient electrophysiological anomalies in the human brain have been associated with neurological disorders such as epilepsy, may signal impending adverse events (e.g, seizurse), or may reflect the effects of a stressor, such as insufficient sleep. These, typically brief, high-frequency and heterogeneous signal anomalies remain poorly understood, particularly at long time scales, and their morphology and variability have not been systematically characterized. In continuous neural recordings, their inherent sparsity, short duration and low amplitude makes their detection and classification difficult. In turn, this limits their evaluation as potential biomarkers of abnormal neurodynamic processes (e.g., ictogenesis) and predictors of impending adverse events. A novel algorithm is presented that leverages the inherent sparsity of high-frequency abnormalities in neural signals recorded at the scalp and uses spectral clustering to classify them in very high-dimensional signals spanning several days. It is shown that estimated clusters vary dynamically with time and their distribution changes substantially both as a function of time and space.",
        "Network analysis finds natural applications in geospatial information systems for a range of applications, notably for thermal grids, which are important for decarbonising thermal energy supply. These analyses are required to operate over a large range of geographic scales. This is a challenge for existing approaches, which face computational scaling challenges with the large datasets now available, such as building and road network data for an entire country. This work presents a system for geospatial modelling of thermal networks including their routing through the existing road network and calculation of flows through the network. This is in contrast to previous thermal network analysis work which could only work with simplified aggregated data.*We apply multi-level spatial clustering which enables parallelisation of work sets.*We develop algorithms and data processing pipelines for calculating network routing.*We use cluster-level caching to enable rapid evaluation of model variants.",
        "The literature on neuroscience has grown rapidly in recent years with the emergence of new domains of research. In the context of this progress, creating a knowledge organization system (KOS) that can quickly incorporate terms of a given domain is an important aim in the area. In this article, we develop a systematic method based on word representation and the agglomerative clustering algorithm to semi-automatically build a hierarchical KOS. We collected 35,832 research keywords and 11,497 research methods from PubMed Central database, and organized them in a hierarchical structure according to semantic distance. We show that the proposed KOS can help find terms related to the given topics, analyze articles related to specific domains of research, and characterize the features of article clusters. The proposed method can significantly reduce the manual work required by experts to organize the KOS.",
        "Many unsupervised methods are widely used for parcellating the brain. However, unsupervised methods aren't able to integrate prior information, obtained from such as exiting functional neuroanatomy studies, to parcellate the brain, whereas the prior information guided semi-supervised method can generate more reliable brain parcellation. In this study, we propose a novel semi-supervised clustering method for parcellating the brain into spatially and functionally consistent parcels based on resting state functional magnetic resonance imaging (fMRI) data. Particularly, the prior supervised and spatial information is integrated into spectral clustering to achieve reliable brain parcellation. The proposed method has been validated in the hippocampus parcellation based on resting state fMRI data of 20 healthy adult subjects. The experimental results have demonstrated that the proposed method could successfully parcellate the hippocampus into head, body and tail parcels. The distinctive functional connectivity patterns of these parcels have further demonstrated the validity of the parcellation results. The effects of aging on the three hippocampus parcels' functional connectivity were also explored across the healthy adult subjects. Compared with state-of-the-art methods, the proposed method had better performance on functional homogeneity. Furthermore, the proposed method had good test-retest reproducibility validated by parcellating the hippocampus based on three repeated resting state fMRI scans from 24 healthy adult subjects.",
        "Influence maximization research in the real world allows us to better understand, accelerate spreading processes for innovations and products, and effectively analyze, predict, and control the spread of diseases, rumors, and computer viruses. In this paper, we first put forward a new path-based node similarity measure, named the dynamic local similarity index, which can be dynamically adjusted to the optimal mode according to network topology characteristics. Compared to the Katz index with high complexity and an LP index with a limited application range, the proposed index achieves an excellent balance between complexity and precision. Second, combining the extended neighborhood coreness with the minimum distance, a novel strategy is presented for selecting initial centers of clusters, which is helpful for speeding up clustering convergence and avoiding local optimum, especially in non-connected networks. Subsequently, we present an adaptive heuristic clustering algorithm, which can find the seed set with maximum collective influence through clustering. The empirical results on four real datasets show the effectiveness and efficiency of the proposed algorithm, which compares favorably to several state-of-the-art algorithms.",
        "Single-cell RNA-sequencing (scRNA-seq) data widely exist in bioinformatics. It is crucial to devise a distance metric for scRNA-seq data. Almost all existing clustering methods based on spectral clustering algorithms work in three separate steps: similarity graph construction; continuous labels learning; discretization of the learned labels by k-means clustering. However, this common practice has potential flaws that may lead to severe information loss and degradation of performance. Furthermore, the performance of a kernel method is largely determined by the selected kernel; a self-weighted multiple kernel learning model can help choose the most suitable kernel for scRNA-seq data. To this end, we propose to automatically learn similarity information from data. We present a new clustering method in the form of a multiple kernel combination that can directly discover groupings in scRNA-seq data. The main proposition is that automatically learned similarity information from scRNA-seq data is used to transform the candidate solution into a new solution that better approximates the discrete one. The proposed model can be efficiently solved by the standard support vector machine (SVM) solvers. Experiments on benchmark scRNA-Seq data validate the superior performance of the proposed model. Spectral clustering with multiple kernels is implemented in Matlab, licensed under Massachusetts Institute of Technology (MIT) and freely available from the Github website, https://github.com/Cuteu/SMSC/.",
        "BACKGROUND: Unsupervised clustering is a common and exceptionally useful tool for large biological datasets. However, clustering requires upfront algorithm and hyperparameter selection, which can introduce bias into the final clustering labels. It is therefore advisable to obtain a range of clustering results from multiple models and hyperparameters, which can be cumbersome and slow. RESULTS: We present hypercluster, a python package and SnakeMake pipeline for flexible and parallelized clustering evaluation and selection. Users can efficiently evaluate a huge range of clustering results from multiple models and hyperparameters to identify an optimal model. CONCLUSIONS: Hypercluster improves ease of use, robustness and reproducibility for unsupervised clustering application for high throughput biology. Hypercluster is available on pip and bioconda; installation, documentation and example workflows can be found at: https://github.com/ruggleslab/hypercluster .",
        "Next generation cellular systems need efficient content-distribution schemes. Content-sharing via Device-to-Device (D2D) clustered networks has emerged as a popular approach for alleviating the burden on the cellular network. In this article, we utilize Content-Centric Networking and Network Virtualization to propose a distributed architecture, that supports efficient content delivery. We propose to use clustering at the user level for content-distribution. A weighted multifactor clustering algorithm is proposed for grouping the D2D User Equipment (DUEs) sharing a common interest. The proposed algorithm is evaluated in terms of energy efficiency, area spectral efficiency, and throughput. The effect of the number of clusters on these performance parameters is also discussed. The proposed algorithm has been further modified to allow for a tradeoff between fairness and other performance parameters. A comprehensive simulation study demonstrates that the proposed clustering algorithm is more flexible and outperforms several classical and state-of-the-art algorithms.",
        "Social networks like Twitter, Facebook have recently become the most widely used communication platforms for people to propagate information rapidly. Fast diffusion of information creates accuracy and scalability issues towards topic detection. Most of the existing approaches can detect the most popular topics on a large scale. However, these approaches are not effective for faster detection. This article proposes a novel topic detection approach - Node Significance based Label Propagation Community Detection (NSLPCD) algorithm, which detects the topic faster without compromising accuracy. The proposed algorithm analyzes the frequency distribution of keywords in the collection of tweets and finds two types of keywords: topic-identifying and topic-describing keywords, which play an important role in topic detection. Based on these defined keywords, the keyword co-occurrence graph is built, and subsequently, the NSLPCD algorithm is applied to get topic clusters in the form of communities. The experimental results using the real data of Twitter, show that the proposed method is effective in quality as well as run-time performance as compared to other existing methods.",
        "In recent years, there are many research cases for the diagnosis of Parkinson's disease (PD) with the brain magnetic resonance imaging (MRI) by utilizing the traditional unsupervised machine learning methods and the supervised deep learning models. However, unsupervised learning methods are not good at extracting accurate features among MRIs and it is difficult to collect enough data in the field of PD to satisfy the need of training deep learning models. Moreover, most of the existing studies are based on single-view MRI data, of which data characteristics are not sufficient enough. In this paper, therefore, in order to tackle the drawbacks mentioned above, we propose a novel semi-supervised learning framework called Semi-supervised Multi-view learning Clustering architecture technology (SMC). The model firstly introduces the sliding window method to grasp different features, and then uses the dimensionality reduction algorithms of Linear Discriminant Analysis (LDA) to process the data with different features. Finally, the traditional single-view clustering and multi-view clustering methods are employed on multiple feature views to obtain the results. Experiments show that our proposed method is superior to the state-of-art unsupervised learning models on the clustering effect. As a result, it may be noted that, our work could contribute to improving the effectiveness of identifying PD by previous labeled and subsequent unlabeled medical MRI data in the realistic medical environment.",
        "Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at https://github.com/DataSlingers/clustRviz.",
        "We develop a scalable multi-step Monte Carlo algorithm for inference under a large class of nonparametric Bayesian models for clustering and classification. Each step is \"embarrassingly parallel\" and can be implemented using the same Markov chain Monte Carlo sampler. The simplicity and generality of our approach makes inference for a wide range of Bayesian nonparametric mixture models applicable to large datasets. Specifically, we apply the approach to inference under a product partition model with regression on covariates. We show results for inference with two motivating data sets: a large set of electronic health records (EHR) and a bank telemarketing dataset. We find interesting clusters and competitive classification performance relative to other widely used competing classifiers. Supplementary materials for this article are available online.",
        "Unsupervised clustering of high-throughput gene expression data is widely adopted for cancer subtyping. However, cancer subtypes derived from a single dataset are usually not applicable across multiple datasets from different platforms. Merging different datasets is necessary to determine accurate and applicable cancer subtypes but is still embarrassing due to the batch effect. CrossICC is an R package designed for the unsupervised clustering of gene expression data from multiple datasets/platforms without the requirement of batch effect adjustment. CrossICC utilizes an iterative strategy to derive the optimal gene signature and cluster numbers from a consensus similarity matrix generated by consensus clustering. This package also provides abundant functions to visualize the identified subtypes and evaluate subtyping performance. We expected that CrossICC could be used to discover the robust cancer subtypes with significant translational implications in personalized care for cancer patients. AVAILABILITY AND IMPLEMENTATION: The package is implemented in R and available at GitHub (https://github.com/bioinformatist/CrossICC) and Bioconductor (http://bioconductor.org/packages/release/bioc/html/CrossICC.html) under the GPL v3 License.",
        "Clustering is an important technology of data mining, which plays a vital role in bioscience, social network and network analysis. As a clustering algorithm based on density and distance, density peak clustering is extensively used to solve practical problems. The algorithm assumes that the clustering center has a larger local density and is farther away from the higher density points. However, the density peak clustering algorithm is highly sensitive to density and distance and cannot accurately identify clusters in a dataset having significant differences in cluster structure. In addition, the density peak clustering algorithm's allocation strategy can easily cause attached allocation errors in data point allocation. To solve these problems, this study proposes a potential-field-diffusion-based density peak clustering. As compared to existing clustering algorithms, the advantages of the potential-field-diffusion-based density peak clustering algorithm is three-fold: 1) The potential field concept is introduced in the proposed algorithm, and a density measure based on the potential field's diffusion is proposed. The cluster center can be accurately selected using this measure. 2) The potential-field-diffusion-based density peak clustering algorithm defines the judgment conditions of similar points and adopts different allocation strategies for dissimilar points to avoid attached errors in data point allocation. 3) This study conducted many experiments on synthetic and real-world datasets. Results demonstrate that the proposed potential-field-diffusion-based density peak clustering algorithm achieves excellent clustering effect and is suitable for complex datasets of different sizes, dimensions, and shapes. Besides, the proposed potential-field-diffusion-based density peak clustering algorithm shows particularly excellent performance on variable density and nonconvex datasets.",
        "PURPOSE: Optic pathway gliomas (OPG) are low-grade pilocytic astrocytomas accounting for 3-5% of pediatric intracranial tumors. Accurate and quantitative follow-up of OPG using magnetic resonance imaging (MRI) is crucial for therapeutic decision making, yet is challenging due to the complex shape and heterogeneous tissue pattern which characterizes these tumors. The aim of this study was to implement automatic methods for segmentation and classification of OPG and its components, based on MRI. METHODS: A total of 202 MRI scans from 29 patients with chiasmatic OPG scanned longitudinally were retrospectively collected and included in this study. Data included T2 and post-contrast T1 weighted images. The entire tumor volume and its components were manually annotated by a senior neuro-radiologist, and inter- and intra-rater variability of the entire tumor volume was assessed in a subset of scans. Automatic tumor segmentation was performed using deep-learning method with U-Net+ResNet architecture. A fivefold cross-validation scheme was used to evaluate the automatic results relative to manual segmentation. Voxel-based classification of the tumor into enhanced, non-enhanced, and cystic components was performed using fuzzy c-means clustering. RESULTS: The results of the automatic tumor segmentation were: mean dice score = 0.736 +/- 0.025, precision = 0.918 +/- 0.014, and recall = 0.635 +/- 0.039 for the validation data, and dice score = 0.761 +/- 0.011, precision = 0.794 +/- 0.028, and recall = 0.742 +/- 0.012 for the test data. The accuracy of the voxel-based classification of tumor components was 0.94, with precision = 0.89, 0.97, and 0.85, and recall = 1.00, 0.79, and 0.94 for the non-enhanced, enhanced, and cystic components, respectively. CONCLUSION: This study presents methods for automatic segmentation of chiasmatic OPG tumors and classification into the different components of the tumor, based on conventional MRI. Automatic quantitative longitudinal assessment of these tumors may improve radiological monitoring, facilitate early detection of disease progression and optimize therapy management.",
        "BACKGROUND: The small number of samples and the curse of dimensionality hamper the better application of deep learning techniques for disease classification. Additionally, the performance of clustering-based feature selection algorithms is still far from being satisfactory due to their limitation in using unsupervised learning methods. To enhance interpretability and overcome this problem, we developed a novel feature selection algorithm. In the meantime, complex genomic data brought great challenges for the identification of biomarkers and therapeutic targets. The current some feature selection methods have the problem of low sensitivity and specificity in this field. RESULTS: In this article, we designed a multi-scale clustering-based feature selection algorithm named MCBFS which simultaneously performs feature selection and model learning for genomic data analysis. The experimental results demonstrated that MCBFS is robust and effective by comparing it with seven benchmark and six state-of-the-art supervised methods on eight data sets. The visualization results and the statistical test showed that MCBFS can capture the informative genes and improve the interpretability and visualization of tumor gene expression and single-cell sequencing data. Additionally, we developed a general framework named McbfsNW using gene expression data and protein interaction data to identify robust biomarkers and therapeutic targets for diagnosis and therapy of diseases. The framework incorporates the MCBFS algorithm, network recognition ensemble algorithm and feature selection wrapper. McbfsNW has been applied to the lung adenocarcinoma (LUAD) data sets. The preliminary results demonstrated that higher prediction results can be attained by identified biomarkers on the independent LUAD data set, and we also structured a drug-target network which may be good for LUAD therapy. CONCLUSIONS: The proposed novel feature selection method is robust and effective for gene selection, classification, and visualization. The framework McbfsNW is practical and helpful for the identification of biomarkers and targets on genomic data. It is believed that the same methods and principles are extensible and applicable to other different kinds of data sets.",
        "In unsupervised learning literature, the study of clustering using microarray gene expression datasets has been extensively conducted with nonnegative matrix factorization (NMF), spectral clustering, kmeans, and gaussian mixture model (GMM) are some of the most used methods. However, there is still a limited number of works that utilize statistical analysis to measure the significances of performance differences between these methods. In this paper, statistical analysis of performance differences between ten NMF algorithms, six spectral clustering algorithms, four GMM algorithms, and a standard kmeans algorithm in clustering eleven publicly available microarray gene expression datasets with the number of clusters ranges from two to ten is presented. The experimental results show that statistically NMF algorithms and kmeans have similar performance and outperform spectral clustering algorithms. As spectral clustering can detect some hidden manifold structures, the underperformances of spectral methods lead us to question whether the datasets have manifold structures. Visual inspection using multidimensional scaling plots indicates that such structures do not exist. Moreover, as MDS plots also indicate clusters in some datasets have elliptical boundaries, GMM is also utilized. The experimental results show that GMM methods outperform the other methods to some degree, and thus imply that the datasets follow gaussian distribution.",
        "The accurate assessment of antibody glycosylation during bioprocessing requires the high-throughput generation of large amounts of glycomics data. This allows bioprocess engineers to identify critical process parameters that control the glycosylation critical quality attributes. The advances made in protocols for capillary electrophoresis-laser-induced fluorescence (CE-LIF) measurements of antibody N-glycans have increased the potential for generating large datasets of N-glycosylation values for assessment. With large cohorts of CE-LIF data, peak picking and peak area calculations still remain a problem for fast and accurate quantitation, despite the presence of internal and external standards to reduce misalignment for the qualitative analysis. The peak picking and area calculation problems are often due to fluctuations introduced by varying process conditions resulting in heterogeneous peak shapes. Additionally, peaks with co-eluting glycans can produce peaks of a non-Gaussian nature in some process conditions and not in others. Here, we describe an approach to quantitatively and qualitatively curate large cohort CE-LIF glycomics data. For glycan identification, a previously reported method based on internal triple standards is used. For determining the glycan relative quantities our method uses a clustering algorithm to 'divide and conquer' highly heterogeneous electropherograms into similar groups, making it easier to define peaks manually. Open-source software is then used to determine peak areas of the manually defined peaks. We successfully applied this semi-automated method to a dataset (containing 391 glycoprofiles) of monoclonal antibody biosimilars from a bioreactor optimization study. The key advantage of this computational approach is that all runs can be analyzed simultaneously with high accuracy in glycan identification and quantitation and there is no theoretical limit to the scale of this method.",
        "The lack of sentiment resources in poor resource languages poses challenges for the sentiment analysis in which machine learning is involved. Cross-lingual and semi-supervised learning approaches have been deployed to represent the most common ways that can overcome this issue. However, performance of the existing methods degrades due to the poor quality of translated resources, data sparseness and more specifically, language divergence. An integrated learning model that uses a semi-supervised and an ensembled model while utilizing the available sentiment resources to tackle language divergence related issues is proposed. Additionally, to reduce the impact of translation errors and handle instance selection problem, we propose a clustering-based bee-colony-sample selection method for the optimal selection of most distinguishing features representing the target data. To evaluate the proposed model, various experiments are conducted employing an English-Arabic cross-lingual data set. Simulations results demonstrate that the proposed model outperforms the baseline approaches in terms of classification performances. Furthermore, the statistical outcomes indicate the advantages of the proposed training data sampling and target-based feature selection to reduce the negative effect of translation errors. These results highlight the fact that the proposed approach achieves a performance that is close to in-language supervised models.",
        "AIMS: Approximately 50% of patients with heart failure have preserved (>/=50%) ejection fraction (HFpEF). Improved understanding of the phenotypic heterogeneity of HFpEF might facilitate development of targeted therapies and interventions. METHODS: This retrospective study characterized a cohort of patients with HFpEF based on similar clinical profiles and evaluated 1-year heart failure related hospitalization. Enrolment, medical and pharmacy data were used to identify patients newly diagnosed with heart failure enrolled in a Medicare Advantage Prescription Drug or commercial healthcare plan. To identify only those patients with HFpEF, we used natural language processing techniques of ejection fraction values abstracted from a linked free-text clinical notes data source. The study population comprised 1515 patients newly identified with HFpEF between 1 January 2011 and 31 December 2015. RESULTS: Using unsupervised machine learning, we identified three distinguishable patient clusters representing different phenotypes: cluster-1 patients had the lowest prevalence of heart failure comorbidities and highest mean age; cluster-2 patients had higher prevalence of metabolic syndrome and pulmonary disease, despite younger mean age; and cluster-3 patients had higher prevalence of cardiac arrhythmia and renal disease. Cluster-3 had the highest 1-year heart failure related hospitalization rates. Within-cluster analysis, prior use of diuretics (cluster-1 and cluster-2) and age (cluster-2 and cluster-3) was associated with 1-year heart failure related hospitalization. Combination therapy was associated with decreased 1-year heart failure related hospitalization in cluster-1. CONCLUSION: This study demonstrated that clustering can be used to characterize subgroups of patients with newly identified HFpEF, assess differences in heart failure related hospitalization rates at 1 year and suggest patient subtypes may respond differently to treatments or interventions.",
        "BACKGROUND: Properly scoring protein-protein docking models to single out the correct ones is an open challenge, also object of assessment in CAPRI (Critical Assessment of PRedicted Interactions), a community-wide blind docking experiment. We introduced in the field CONSRANK (CONSensus RANKing), the first pure consensus method. Also available as a web server, CONSRANK ranks docking models in an ensemble based on their ability to match the most frequent inter-residue contacts in it. We have been blindly testing CONSRANK in all the latest CAPRI rounds, where we showed it to perform competitively with the state-of-the-art energy and knowledge-based scoring functions. More recently, we developed Clust-CONSRANK, an algorithm introducing a contact-based clustering of the models as a preliminary step of the CONSRANK scoring process. In the latest CASP13-CAPRI joint experiment, we participated as scorers with a novel pipeline, combining both our scoring tools, CONSRANK and Clust-CONSRANK, with our interface analysis tool COCOMAPS. Selection of the 10 models for submission was guided by the strength of the emerging consensus, and their final ranking was assisted by results of the interface analysis. RESULTS: As a result of the above approach, we were by far the first scorer in the CASP13-CAPRI top-1 ranking, having high/medium quality models ranked at the top-1 position for the majority of targets (11 out of the total 19). We were also the first scorer in the top-10 ranking, on a par with another group, and the second scorer in the top-5 ranking. Further, we topped the ranking relative to the prediction of binding interfaces, among all the scorers and predictors. Using the CASP13-CAPRI targets as case studies, we illustrate here in detail the approach we adopted. CONCLUSIONS: Introducing some flexibility in the final model selection and ranking, as well as differentiating the adopted scoring approach depending on the targets were the key assets for our highly successful performance, as compared to previous CAPRI rounds. The approach we propose is entirely based on methods made available to the community and could thus be reproduced by any user.",
        "BACKGROUND: High-dimensional flow cytometry and mass cytometry allow systemic-level characterization of more than 10 protein profiles at single-cell resolution and provide a much broader landscape in many biological applications, such as disease diagnosis and prediction of clinical outcome. When associating clinical information with cytometry data, traditional approaches require two distinct steps for identification of cell populations and statistical test to determine whether the difference between two population proportions is significant. These two-step approaches can lead to information loss and analysis bias. RESULTS: We propose a novel statistical framework, called LAMBDA (Latent Allocation Model with Bayesian Data Analysis), for simultaneous identification of unknown cell populations and discovery of associations between these populations and clinical information. LAMBDA uses specified probabilistic models designed for modeling the different distribution information for flow or mass cytometry data, respectively. We use a zero-inflated distribution for the mass cytometry data based the characteristics of the data. A simulation study confirms the usefulness of this model by evaluating the accuracy of the estimated parameters. We also demonstrate that LAMBDA can identify associations between cell populations and their clinical outcomes by analyzing real data. LAMBDA is implemented in R and is available from GitHub ( https://github.com/abikoushi/lambda ).",
        "The Narrowband Internet of Things (NB-IoT) is a very promising licensed Internet of things (IoT) technology for accommodating massive device connections in 5G and beyond. To enable network scalability, this study proposes a two-layers novel mixed approach that aims not only to create an efficient spectrum sharing among the many NB-IoT devices but also provides an energy-efficient network. On one layer, the approach uses an Adaptive Frequency Hopping Spread Spectrum (AFHSS) technique that uses a lightweight and secure pseudo-random sequence to exploit the channel diversity, to mitigate inter-link and cross-technology interference. On the second layer, the approach consists of a clustering and network coding (data aggregation) approach based on an energy-signal strength mixed gradient. The second layer contributes to offload the BS, allows for energy-efficient network scalability, helps balance the energy consumption of the network, and enhances the overall network lifetime. The proposed mixed strategy algorithm is modelled and simulated using the Matrix Laboratory (MATLAB) Long Term Evolution (LTE) toolbox. The obtained results reveal that the proposed mixed approach enhances network scalability while improving energy efficiency, transmission reliability, and network lifetime when compared to the existing spread spectrum only, nodes clustering only, and mixed approach with no network coding approaches.",
        "Deciphering patterns in the structural and functional anatomy of genes can prove to be very helpful in understanding genetic biology and genomics. Also, the availability of the multiple omics data, along with the advent of machine learning techniques, aids medical professionals in gaining insights about various biological regulations. Gene clustering is one of the many such computation techniques that can help in understanding gene behavior. However, more comprehensive and reliable insights can be gained if different modalities/views of biomedical data are considered. However, in most multi-view cases, each view contains some missing data, leading to incomplete multi-view clustering. In this study, we have presented a deep Boltzmann machine-based incomplete multi-view clustering framework for gene clustering. Here, we seek to regenerate the data of the three NCBI datasets in the incomplete modalities using Shape Boltzmann Machines. The overall performance of the proposed multi-view clustering technique has been evaluated using the Silhouette index and Davies-Bouldin index, and the comparative analysis shows an improvement over state-of-the-art methods. Finally, to prove that the improvement attained by the proposed incomplete multi-view clustering is statistically significant, we perform Welch's t-test. AVAILABILITY OF DATA AND MATERIALS: https://github.com/piyushmishra12/IMC.",
        "Simulations of deoxyribonucleic acid (DNA) molecular damage use the traversal algorithm that has the disadvantages of being time-consuming, slowly converging, and requiring high-performance computer clusters. This work presents an improved version of the algorithm, \"density-based spatial clustering of applications with noise\" (DBSCAN), using a KD-tree approach to find neighbors of each point for calculating clustered DNA damage. The resulting algorithm considers the spatial distributions for sites of energy deposition and hydroxyl radical attack, yielding the statistical probability of (single and double) DNA strand breaks. This work achieves high accuracy and high speed at calculating clustered DNA damage that has been induced by proton treatment at the molecular level while running on an i7 quad-core CPU. The simulations focus on the indirect effect generated by hydroxyl radical attack on DNA. The obtained results are consistent with those of other published experiments and simulations. Due to the array of chemical processes triggered by proton treatment, it is possible to predict the effects that different track structures of various energy protons produce on eliciting direct and indirect damage of DNA.",
        "MOTIVATION: Clustering analysis in a biological network is to group biological entities into functional modules, thus providing valuable insight into the understanding of complex biological systems. Existing clustering techniques make use of lower-order connectivity patterns at the level of individual biological entities and their connections, but few of them can take into account of higher-order connectivity patterns at the level of small network motifs. RESULTS: Here, we present a novel clustering framework, namely HiSCF, to identify functional modules based on the higher-order structure information available in a biological network. Taking advantage of higher-order Markov stochastic process, HiSCF is able to perform the clustering analysis by exploiting a variety of network motifs. When compared with several state-of-the-art clustering models, HiSCF yields the best performance for two practical clustering applications, i.e. protein complex identification and gene co-expression module detection, in terms of accuracy. The promising performance of HiSCF demonstrates that the consideration of higher-order network motifs gains new insight into the analysis of biological networks, such as the identification of overlapping protein complexes and the inference of new signaling pathways, and also reveals the rich higher-order organizational structures presented in biological networks. AVAILABILITY AND IMPLEMENTATION: HiSCF is available at https://github.com/allenv5/HiSCF. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Advanced chemometric methods, such as fuzzy c-means (FCM), a fuzzy divisive hierarchical clustering algorithm (FDHC), and fuzzy divisive hierarchical associative-clustering (FDHAC), which offer the excellent possibility to associate each fuzzy partition of samples with a fuzzy set of characteristics (features), have been successfully applied in this study. FDHAC, a method that utilizes specific regions of chromatographic fingerprints or specific peaks as a fuzzy set of characteristics, was effectively applied to the characterization and classification of medicinal plant extracts according to their antioxidant capacities, using their chromatographic profiles monitored at 242, 260, 280, 320, 340, and 380 nm via HPLC with a multistep isocratic and gradient elution system and diode array detection (HPLC-DAD). What is quite new is the partitioning of the chromatographic retention time ranges and peaks (markers) and their association with different plant extract samples with high, moderate or low antioxidant capacity. Furthermore, the degrees of membership of fingerprints (fuzzy markers) are highly relevant with respect to the (dis)similarity of samples because they indicate both the positions and degrees of association of chromatographic peaks from different classes or individual samples. The obtained results clearly demonstrate the efficiency and information power of these advanced fuzzy methods for medicinal plant characterization and authentication, and this study generates the premise for a new chemometrics approach with high-impact for use in analytical chemistry and other fields.",
        "Due to the efficiency of exploiting relationships and complex structures hidden in multi-views data, graph-oriented clustering methods have achieved remarkable progress in recent years. But most existing graph-based spectral methods still have the following demerits: (1) They regularize each view equally, which does not make sense in real applications. (2) By employing different norms, most existing methods calculate the error feature by feature, resulting in neglecting the spatial structure information and the complementary information. To tackle the aforementioned drawbacks, we propose an enhanced multi-view spectral clustering model. Our model characterizes the consistency among indicator matrices by minimizing our proposed weighted tensor nuclear norm, which explicitly exploits the salient different information between singular values of the matrix. Moreover, our model adaptively assigns a reasonable weight to each view, which helps improve robustness of the algorithm. Finally, the proposed tensor nuclear norm well exploits both high-order and complementary information, which helps mine the consistency between indicator matrices. Extensive experiments indicate the efficiency of our method.",
        "PURPOSE: Hierarchical clustering (HC), an unsupervised machine learning (ML) technique, was applied to multi-parametric MR (mp-MR) for prostate cancer (PCa). The aim of this study is to demonstrate HC can diagnose PCa in a straightforward interpretable way, in contrast to deep learning (DL) techniques. METHODS: HC was constructed using mp-MR including intravoxel incoherent motion, diffusion kurtosis imaging, and dynamic contrast-enhanced MRI from 40 tumor and normal tissues in peripheral zone (PZ) and 23 tumor and normal tissues in transition zone (TZ). HC model was optimized by assessing the combinations of several dissimilarity and linkage methods. Goodness of HC model was validated by internal methods. RESULTS: Accuracy for differentiating tumor and normal tissue by optimal HC model was 96.3% in PZ and 97.8% in TZ, comparable to current clinical standards. Relationship between input (DWI and permeability parameters) and output (tumor and normal tissue cluster) was shown by heat maps, consistent with literature. CONCLUSION: HC can accurately differentiate PCa and normal tissue, comparable to state-of-the-art diffusion based parameters. Contrary to DL techniques, HC is an operator-independent ML technique producing results that can be interpreted such that the results can be knowledgeably judged.",
        "MOTIVATION: Mendelian randomization is an epidemiological technique that uses genetic variants as instrumental variables to estimate the causal effect of a risk factor on an outcome. We consider a scenario in which causal estimates based on each variant in turn differ more strongly than expected by chance alone, but the variants can be divided into distinct clusters, such that all variants in the cluster have similar causal estimates. This scenario is likely to occur when there are several distinct causal mechanisms by which a risk factor influences an outcome with different magnitudes of causal effect. We have developed an algorithm MR-Clust that finds such clusters of variants, and so can identify variants that reflect distinct causal mechanisms. Two features of our clustering algorithm are that it accounts for differential uncertainty in the causal estimates, and it includes 'null' and 'junk' clusters, to provide protection against the detection of spurious clusters. RESULTS: Our algorithm correctly detected the number of clusters in a simulation analysis, outperforming methods that either do not account for uncertainty or do not include null and junk clusters. In an applied example considering the effect of blood pressure on coronary artery disease risk, the method detected four clusters of genetic variants. A post hoc hypothesis-generating search suggested that variants in the cluster with a negative effect of blood pressure on coronary artery disease risk were more strongly related to trunk fat percentage and other adiposity measures than variants not in this cluster. AVAILABILITY AND IMPLEMENTATION: MR-Clust can be downloaded from https://github.com/cnfoley/mrclust. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "The density-based clustering algorithm is a fundamental data clustering technique with many real-world applications. However, when the database is frequently changed, how to effectively update clustering results rather than reclustering from scratch remains a challenging task. In this work, we introduce IncAnyDBC, a unique parallel incremental data clustering approach to deal with this problem. First, IncAnyDBC can process changes in bulks rather than batches like state-of-the-art methods for reducing update overheads. Second, it keeps an underlying cluster structure called the object node graph during the clustering process and uses it as a basis for incrementally updating clusters wrt. inserted or deleted objects in the database by propagating changes around affected nodes only. In additional, IncAnyDBC actively and iteratively examines the graph and chooses only a small set of most meaningful objects to produce exact clustering results of DBSCAN or to approximate results under arbitrary time constraints. This makes it more efficient than other existing methods. Third, by processing objects in blocks, IncAnyDBC can be efficiently parallelized on multicore CPUs, thus creating a work-efficient method. It runs much faster than existing techniques using one thread while still scaling well with multiple threads. Experiments are conducted on various large real datasets for demonstrating the performance of IncAnyDBC.",
        "Exome sequencing in diabetes presents a diagnostic challenge because depending on frequency, functional impact, and genomic and environmental contexts, HNF1A variants can cause maturity-onset diabetes of the young (MODY), increase type 2 diabetes risk, or be benign. A correct diagnosis matters as it informs on treatment, progression, and family risk. We describe a multi-dimensional functional dataset of 73 HNF1A missense variants identified in exomes of 12,940 individuals. Our aim was to develop an analytical framework for stratifying variants along the HNF1A phenotypic continuum to facilitate diagnostic interpretation. HNF1A variant function was determined by four different molecular assays. Structure of the multi-dimensional dataset was explored using principal component analysis, k-means, and hierarchical clustering. Weights for tissue-specific isoform expression and functional domain were integrated. Functionally annotated variant subgroups were used to re-evaluate genetic diagnoses in national MODY diagnostic registries. HNF1A variants demonstrated a range of behaviors across the assays. The structure of the multi-parametric data was shaped primarily by transactivation. Using unsupervised learning methods, we obtained high-resolution functional clusters of the variants that separated known causal MODY variants from benign and type 2 diabetes risk variants and led to reclassification of 4% and 9% of HNF1A variants identified in the UK and Norway MODY diagnostic registries, respectively. Our proof-of-principle analyses facilitated informative stratification of HNF1A variants along the continuum, allowing improved evaluation of clinical significance, management, and precision medicine in diabetes clinics. Transcriptional activity appears a superior readout supporting pursuit of transactivation-centric experimental designs for high-throughput functional screens.",
        "This paper proposes a clustering ensemble method that introduces cascade structure into the self-organizing map (SOM) to solve the problem of the poor performance of a single clusterer. Cascaded SOM is an extension of classical SOM combined with the cascaded structure. The method combines the outputs of multiple SOM networks in a cascaded manner using them as an input to another SOM network. It also utilizes the characteristic of high-dimensional data insensitivity to changes in the values of a small number of dimensions to achieve the effect of ignoring part of the SOM network error output. Since the initial parameters of the SOM network and the sample training order are randomly generated, the model does not need to provide different training samples for each SOM network to generate a differentiated SOM clusterer. After testing on several classical datasets, the experimental results show that the model can effectively improve the accuracy of pattern recognition by 4% approximately 10%.",
        "Molecular dynamics simulations are a popular means to study biomolecules, but it is often difficult to gain insights from the trajectories due to their large size, in both time and number of features. The Sapphire (States And Pathways Projected with HIgh REsolution) plot allows a direct visual inference of the dominant states visited by high-dimensional systems and how they are interconnected in time. Here, we extend this visual inference into a clustering algorithm. Specifically, the automatic procedure derives from the Sapphire plot states that are kinetically homogeneous, structurally annotated, and of tunable granularity. We provide a relative assessment of the kinetic fidelity of the Sapphire-based partitioning in comparison to popular clustering methods. This assessment is carried out on trajectories of n-butane, a beta-sheet peptide, and the small protein BPTI. We conclude with an application of our approach to a recent 100 mus trajectory of the main protease of SARS-CoV-2.",
        "Some directly transmitted human pathogens, such as influenza and measles, generate sustained exponential growth in incidence and have a high peak incidence consistent with the rapid depletion of susceptible individuals. Many do not. While a prolonged exponential phase typically arises in traditional disease-dynamic models, current quantitative descriptions of nonstandard epidemic profiles are either abstract, phenomenological, or rely on highly skewed offspring distributions in network models. Here, we create large socio-spatial networks to represent contact behavior using human population-density data, a previously developed fitting algorithm, and gravity-like mobility kernels. We define a basic reproductive number [Formula: see text] for this system, analogous to that used for compartmental models. Controlling for [Formula: see text], we then explore networks with a household-workplace structure in which between-household contacts can be formed with varying degrees of spatial correlation, determined by a single parameter from the gravity-like kernel. By varying this single parameter and simulating epidemic spread, we are able to identify how more frequent local movement can lead to strong spatial correlation and, thus, induce subexponential outbreak dynamics with lower, later epidemic peaks. Also, the ratio of peak height to final size was much smaller when movement was highly spatially correlated. We investigate the topological properties of our networks via a generalized clustering coefficient that extends beyond immediate neighborhoods, identifying very strong correlations between fourth-order clustering and nonstandard epidemic dynamics. Our results motivate the observation of both incidence and socio-spatial human behavior during epidemics that exhibit nonstandard incidence patterns.",
        "Clustering in wireless sensor networks plays a vital role in solving energy and scalability issues. Although multiple deployment structures and cluster shapes have been implemented, they sometimes fail to produce the expected outcomes owing to different geographical area shapes. This paper proposes a clustering algorithm with a complex deployment structure called radial-shaped clustering (RSC). The deployment structure is divided into multiple virtual concentric rings, and each ring is further divided into sectors called clusters. The node closest to the midpoint of each sector is selected as the cluster head. Each sector's data are aggregated and forwarded to the sink node through angular inclination routing. We experimented and compared the proposed RSC performance against that of the existing fan-shaped clustering algorithm. Experimental results reveal that RSC outperforms the existing algorithm in scalability and network lifetime for large-scale sensor deployments.",
        "In the multi-target traffic radar scene, the clustering accuracy between vehicles with close driving distance is relatively low. In response to this problem, this paper proposes a new clustering algorithm, namely an adaptive ellipse distance density peak fuzzy (AEDDPF) clustering algorithm. Firstly, the Euclidean distance is replaced by adaptive ellipse distance, which can more accurately describe the structure of data obtained by radar measurement vehicles. Secondly, the adaptive exponential function curve is introduced in the decision graph of the fast density peak search algorithm to accurately select the density peak point, and the initialization of the AEDDPF algorithm is completed. Finally, the membership matrix and the clustering center are calculated through successive iterations to obtain the clustering result.The time complexity of the AEDDPF algorithm is analyzed. Compared with the density-based spatial clustering of applications with noise (DBSCAN), k-means, fuzzy c-means (FCM), Gustafson-Kessel (GK), and adaptive Euclidean distance density peak fuzzy (Euclid-ADDPF) algorithms, the AEDDPF algorithm has higher clustering accuracy for real measurement data sets in certain scenarios. The experimental results also prove that the proposed algorithm has a better clustering effect in some close-range vehicle scene applications. The generalization ability of the proposed AEDDPF algorithm applied to other types of data is also analyzed.",
        "OBJECTIVES: To determine whether first-year college students cluster in networks based on subjective perceptions of loneliness. Participants: 492 first-year Notre Dame students completed surveys across two semesters and provided communication data used to reconstruct their social networks. Methods: Subjective perceptions of loneliness are measured using the Social and Emotional Loneliness Scale for Adults (SELSA). Correlations between an individual's loneliness and the average loneliness of their alters are compared to associations in random networks created using a rewiring algorithm to determine statistical significance. Results: During their first semester, students are more likely than chance to form ties with other students with similar levels of family and romantic loneliness. In their second semester, students cluster on romantic loneliness but not on family or social loneliness. Conclusions: Students are more likely than chance to form ties with people with similar self-perceived levels of loneliness, but only for certain types of loneliness and during certain periods.",
        "MOTIVATION: The microbes that live in an environment can be identified from the combined genomic material, also referred to as the metagenome. Sequencing a metagenome can result in large volumes of sequencing reads. A promising approach to reduce the size of metagenomic datasets is by clustering reads into groups based on their overlaps. Clustering reads are valuable to facilitate downstream analyses, including computationally intensive strain-aware assembly. As current read clustering approaches cannot handle the large datasets arising from high-throughput metagenome sequencing, a novel read clustering approach is needed. In this article, we propose OGRE, an Overlap Graph-based Read clustEring procedure for high-throughput sequencing data, with a focus on shotgun metagenomes. RESULTS: We show that for small datasets OGRE outperforms other read binners in terms of the number of species included in a cluster, also referred to as cluster purity, and the fraction of all reads that is placed in one of the clusters. Furthermore, OGRE is able to process metagenomic datasets that are too large for other read binners into clusters with high cluster purity. CONCLUSION: OGRE is the only method that can successfully cluster reads in species-specific clusters for large metagenomic datasets without running into computation time- or memory issues. AVAILABILITYAND IMPLEMENTATION: Code is made available on Github (https://github.com/Marleen1/OGRE). SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Dynamic networks are a general language for describing time-evolving complex systems, and discrete time network models provide an emerging statistical technique for various applications. It is a fundamental research question to detect a set of nodes sharing similar connectivity patterns in time-evolving networks. Our work is primarily motivated by detecting groups based on interesting features of the time-evolving networks (e.g., stability). In this work, we propose a model-based clustering framework for time-evolving networks based on discrete time exponential-family random graph models, which simultaneously allows both modeling and detecting group structure. To choose the number of groups, we use the conditional likelihood to construct an effective model selection criterion. Furthermore, we propose an efficient variational expectation-maximization (EM) algorithm to find approximate maximum likelihood estimates of network parameters and mixing proportions. The power of our method is demonstrated in simulation studies and empirical applications to international trade networks and the collaboration networks of a large research university.",
        "In this article, we propose a novel model for constrained clustering, namely, the dissimilarity propagation-guided graph-Laplacian principal component analysis (DP-GLPCA). By fully utilizing a limited number of weakly supervisory information in the form of pairwise constraints, the proposed DP-GLPCA is capable of capturing both the local and global structures of input samples to exploit their characteristics for excellent clustering. More specifically, we first formulate a convex semisupervised low-dimensional embedding model by incorporating a new dissimilarity regularizer into GLPCA (i.e., an unsupervised dimensionality reduction model), in which both the similarity and dissimilarity between low-dimensional representations are enforced with the constraints to improve their discriminability. An efficient iterative algorithm based on the inexact augmented Lagrange multiplier is designed to solve it with the global convergence guaranteed. Furthermore, we innovatively propose to propagate the cannot-link constraints (i.e., dissimilarity) to refine the dissimilarity regularizer to be more informative. The resulting DP model is iteratively solved, and we also prove that it can converge to a Karush-Kuhn-Tucker point. Extensive experimental results over nine commonly used benchmark data sets show that the proposed DP-GLPCA can produce much higher clustering accuracy than state-of-the-art constrained clustering methods. Besides, the effectiveness and advantage of the proposed DP model are experimentally verified. To the best of our knowledge, it is the first time to investigate DP, which is contrast to existing pairwise constraint propagation that propagates similarity. The code is publicly available at https://github.com/jyh-learning/DP-GLPCA.",
        "Semisupervised clustering methods improve performance by randomly selecting pairwise constraints, which may lead to redundancy and instability. In this context, active clustering is proposed to maximize the efficacy of annotations by effectively using pairwise constraints. However, existing methods lack an overall consideration of the querying criteria and repeatedly run semisupervised clustering to update labels. In this work, we first propose an active density peak (ADP) clustering algorithm that considers both representativeness and informativeness. Representative instances are selected to capture data patterns, while informative instances are queried to reduce the uncertainty of clustering results. Meanwhile, we design a fast-update-strategy to update labels efficiently. In addition, we propose an active clustering ensemble framework that combines local and global uncertainties to query the most ambiguous instances for better separation between the clusters. A weighted voting consensus method is introduced for better integration of clustering results. We conducted experiments by comparing our methods with state-of-the-art methods on real-world data sets. Experimental results demonstrate the effectiveness of our methods.",
        "BACKGROUND: Biological networks are representative of the diverse molecular interactions that occur within cells. Some of the commonly studied biological networks are modeled through protein-protein interactions, gene regulatory, and metabolic pathways. Among these, metabolic networks are probably the most studied, as they directly influence all physiological processes. Exploration of biochemical pathways using multigraph representation is important in understanding complex regulatory mechanisms. Feature extraction and clustering of these networks enable grouping of samples obtained from different biological specimens. Clustering techniques separate networks depending on their mutual similarity. RESULTS: We present a clustering analysis on tissue-specific metabolic networks for single samples from three primary tumor sites: breast, lung, and kidney cancer. The metabolic networks were obtained by integrating genome scale metabolic models with gene expression data. We performed network simplification to reduce the computational time needed for the computation of network distances. We empirically proved that networks clustering can characterize groups of patients in multiple conditions. CONCLUSIONS: We provide a computational methodology to explore and characterize the metabolic landscape of tumors, thus providing a general methodology to integrate analytic metabolic models with gene expression data. This method represents a first attempt in clustering large scale metabolic networks. Moreover, this approach gives the possibility to get valuable information on what are the effects of different conditions on the overall metabolism.",
        "BACKGROUND: High throughput methods, in biological and biomedical fields, acquire a large number of molecular parameters or omics data by a single experiment. Combining these omics data can significantly increase the capability for recovering fine-tuned structures or reducing the effects of experimental and biological noise in data. RESULTS: In this work we propose a multi-view integration methodology (named FH-Clust) for identifying patient subgroups from different omics information (e.g., Gene Expression, Mirna Expression, Methylation). In particular, hierarchical structures of patient data are obtained in each omic (or view) and finally their topologies are merged by consensus matrix. One of the main aspects of this methodology, is the use of a measure of dissimilarity between sets of observations, by using an appropriate metric. For each view, a dendrogram is obtained by using a hierarchical clustering based on a fuzzy equivalence relation with Lukasiewicz valued fuzzy similarity. Finally, a consensus matrix, that is a representative information of all dendrograms, is formed by combining multiple hierarchical agglomerations by an approach based on transitive consensus matrix construction. Several experiments and comparisons are made on real data (e.g., Glioblastoma, Prostate Cancer) to assess the proposed approach. CONCLUSIONS: Fuzzy logic allows us to introduce more flexible data agglomeration techniques. From the analysis of scientific literature, it appears to be the first time that a model based on fuzzy logic is used for the agglomeration of multi-omic data. The results suggest that FH-Clust provides better prognostic value and clinical significance compared to the analysis of single-omic data alone and it is very competitive with respect to other techniques from literature.",
        "The COVID-19 disease has once again reiterated the impact of pandemics beyond a biomedical event with potential rapid, dramatic, sweeping disruptions to the management, and conduct of everyday life. Not only the rate and pattern of contagion that threaten our sense of healthy living but also the safety measures put in place for containing the spread of the virus may require social distancing. Three different measures to counteract this pandemic situation have emerged, namely: (i) vaccination, (ii) herd immunity development, and (iii) lockdown. As the first measure is not ready at this stage and the second measure is largely considered unreasonable on the account of the gigantic number of fatalities, a vast majority of countries have practiced the third option despite having a potentially immense adverse economic impact. To mitigate such an impact, this paper proposes a data-driven dynamic clustering framework for moderating the adverse economic impact of COVID-19 flare-up. Through an intelligent fusion of healthcare and simulated mobility data, we model lockdown as a clustering problem and design a dynamic clustering algorithm for localized lockdown by taking into account the pandemic, economic and mobility aspects. We then validate the proposed algorithms by conducting extensive simulations using the Malaysian context as a case study. The findings signify the promises of dynamic clustering for lockdown coverage reduction, reduced economic loss, and military unit deployment reduction, as well as assess potential impact of uncooperative civilians on the contamination rate. The outcome of this work is anticipated to pave a way for significantly reducing the severe economic impact of the COVID-19 spreading. Moreover, the idea can be exploited for potentially the next waves of corona virus-related diseases and other upcoming viral life-threatening calamities.",
        "The Level of Traffic Stress (LTS) is an indicator that quantifies the stress experienced by a cyclist on the segments of a road network. We propose an LTS-based classification with two components: a clustering component and an interpretative component. Our methodology is comprised of four steps: (i) compilation of a set of variables for road segments, (ii) generation of clusters of segments within a subset of the road network, (iii) classification of all segments of the road network into these clusters using a predictive model, and (iv) assignment of an LTS category to each cluster. At the core of the methodology, we couple a classifier (unsupervised clustering algorithm) with a predictive model (multinomial logistic regression) to make our approach scalable to massive data sets. Our methodology is a useful tool for policy-making, as it identifies suitable areas for interventions; and can estimate their impact on the LTS classification, according to probable changes to the input variables (e.g., traffic density). We applied our methodology on the road network of Bogota, Colombia, a city with a history of implementing innovative policies to promote biking. To classify road segments, we combined government data with open-access repositories using geographic information systems (GIS). Comparing our LTS classification with city reports, we found that the number of bicyclists' fatal and non-fatal collisions per kilometer is positively correlated with higher LTS. Finally, to support policy making, we developed a web-enabled dashboard to visualize and analyze the LTS classification and its underlying variables.",
        "INTRODUCTION: Using ultrasound to measure optic nerve sheath diameter (ONSD) has been shown to be a useful modality to detect elevated intracranial pressure. However, manual assessment of ONSD by a human operator is cumbersome and prone to human errors. We aimed to develop and test an automated algorithm for ONSD measurement using ultrasound images and compare it to measurements performed by physicians. MATERIALS AND METHODS: Patients were recruited from the Neurological Intensive Care Unit. Ultrasound images of the optic nerve sheath from both eyes were obtained using an ultrasound unit with an ocular preset. Images were processed by two attending physicians to calculate ONSD manually. The images were processed as well using a novel computerized algorithm that automatically analyzes ultrasound images and calculates ONSD. Algorithm-measured ONSD was compared with manually measured ONSD using multiple statistical measures. RESULTS: Forty-four patients with an average/Standard Deviation (SD) intracranial pressure of 14 (9.7) mmHg were recruited and tested (with a range between 1 and 57 mmHg). A t-test showed no statistical difference between the ONSD from left and right eyes (P > 0.05). Furthermore, a paired t-test showed no significant difference between the manually and algorithm-measured ONSD with a mean difference (SD) of 0.012 (0.046) cm (P > 0.05) and percentage error of difference of 6.43% (P = 0.15). Agreement between the two operators was highly correlated (interclass correlation coefficient = 0.8, P = 0.26). Bland-Altman analysis revealed mean difference (SD) of 0.012 (0.046) (P = 0.303) and limits of agreement between -0.1 and 0.08. Receiver Operator Curve analysis yielded an area under the curve of 0.965 (P < 0.0001) with high sensitivity and specificity. CONCLUSION: The automated image-analysis algorithm calculates ONSD reliably and with high precision when compared to measurements obtained by expert physicians. The algorithm may have a role in computer-aided decision support systems in acute brain injury.",
        "Dietary patterns (DPs) are known to be tied to lifestyle behaviors. Understanding DPs and their relationships with lifestyle factors can help to prevent children from engaging in unhealthy dietary practices. We aimed to describe DPs in Spanish children aged 1 to <10 years and to examine their associations with sociodemographic and lifestyle variables. The consumption of toddler and young children milk formulas, enriched and fortified milk within the Spanish pediatric population is increasing, and there is a lack of evidence whether the consumption of this type of milk is causing an impact on nutrient intakes and if they are helping to reach the nutrient recommendations. Within the Nutritional Study in the Spanish Pediatric Population (EsNuPI), we considered two study cohorts and three different age groups in three year-intervals in each of them. The study cohort included 740 children in a representative sample of the urban non-vegan Spanish population and 772 children in a convenience cohort of adapted milk consumers (AMS) (including follow-on formula, toddler's milk, growing up milk, and fortified and enriched milks) who provided information about sociodemographics, lifestyle, and dietary habits; a food frequency questionnaire was used for the latter. Principal component analysis was performed to identify DPs from 18 food groups. Food groups and sociodemographic/lifestyle variables were combined through a hierarchical cluster algorithm. Three DPs predominated in every age group and study sample: a palatable energy-dense food dietary pattern, and two Mediterranean-like DPs. However, children from the AMS showed a predominant dietary pattern markedly related to the Mediterranean diet, with high consumption of cereals, fruits and vegetables, as well as milk and dairy products. The age of children and certain lifestyle factors, namely level of physical activity, parental education, and household income, correlated closely with the dietary clusters. Thus, the findings provide insight into designing lifestyle interventions that could reverse the appearance of unhealthy DPs in the Spanish child population.",
        "The tire marking points of dynamic balance and uniformity play a crucial guiding role in tire installation. Incomplete marking points block the recognition of tire marking points, and then affect the installation of tires. It is usually necessary to evaluate the marking point completeness during the quality inspection of finished tires. In order to meet the high-precision requirements of the evaluation of tire marking point completeness in the smart factories, the K-means clustering algorithm is introduced to segment the image of marking points in this paper. The pixels within the contour of the marking point are weighted to calculate the marking point completeness on the basis of the image segmentation. The completeness is rated and evaluated by completeness calculation. The experimental results show that the accuracy of the marking point completeness ratings is 95%, and the accuracy of the marking point evaluations is 99%. The proposed method has an important guiding significance of practice to evaluate the tire marking point completeness during the tire quality inspection based on machine vision.",
        "MOTIVATION: Proteins are intrinsically dynamic entities. Flexibility sampling methods, such as molecular dynamics or those arising from integrative modeling strategies, are now commonplace and enable the study of molecular conformational landscapes in many contexts. Resulting structural ensembles increase in size as technological and algorithmic advancements take place, making their analysis increasingly demanding. In this regard, cluster analysis remains a go-to approach for their classification. However, many state-of-the-art algorithms are restricted to specific cluster properties. Combined with tedious parameter fine-tuning, cluster analysis of protein structural ensembles suffers from the lack of a generally applicable and easy to use clustering scheme. RESULTS: We present CLoNe, an original Python-based clustering scheme that builds on the Density Peaks algorithm of Rodriguez and Laio. CLoNe relies on a probabilistic analysis of local density distributions derived from nearest neighbors to find relevant clusters regardless of cluster shape, size, distribution and amount. We show its capabilities on many toy datasets with properties otherwise dividing state-of-the-art approaches and improves on the original algorithm in key aspects. Applied to structural ensembles, CLoNe was able to extract meaningful conformations from membrane binding events and ligand-binding pocket opening as well as identify dominant dimerization motifs or inter-domain organization. CLoNe additionally saves clusters as individual trajectories for further analysis and provides scripts for automated use with molecular visualization software. AVAILABILITY AND IMPLEMENTATION: www.epfl.ch/labs/lbm/resources, github.com/LBM-EPFL/CLoNe. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "MOTIVATION: Gene clustering is a widely used technique that has enabled computational prediction of unknown gene functions within a species. However, it remains a challenge to refine gene function prediction by leveraging evolutionarily conserved genes in another species. This challenge calls for a new computational algorithm to identify gene co-clusters in two species, so that genes in each co-cluster exhibit similar expression levels in each species and strong conservation between the species. RESULTS: Here, we develop the bipartite tight spectral clustering (BiTSC) algorithm, which identifies gene co-clusters in two species based on gene orthology information and gene expression data. BiTSC novelly implements a formulation that encodes gene orthology as a bipartite network and gene expression data as node covariates. This formulation allows BiTSC to adopt and combine the advantages of multiple unsupervised learning techniques: kernel enhancement, bipartite spectral clustering, consensus clustering, tight clustering and hierarchical clustering. As a result, BiTSC is a flexible and robust algorithm capable of identifying informative gene co-clusters without forcing all genes into co-clusters. Another advantage of BiTSC is that it does not rely on any distributional assumptions. Beyond cross-species gene co-clustering, BiTSC also has wide applications as a general algorithm for identifying tight node co-clusters in any bipartite network with node covariates. We demonstrate the accuracy and robustness of BiTSC through comprehensive simulation studies. In a real data example, we use BiTSC to identify conserved gene co-clusters of Drosophila melanogaster and Caenorhabditis elegans, and we perform a series of downstream analysis to both validate BiTSC and verify the biological significance of the identified co-clusters. AVAILABILITY AND IMPLEMENTATION: The Python package BiTSC is open-access and available at https://github.com/edensunyidan/BiTSC. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Autism spectrum disorder (ASD) has phenotypically and genetically heterogeneous characteristics. A simulation study demonstrated that attempts to categorize patients with a complex disease into more homogeneous subgroups could have more power to elucidate hidden heritability. We conducted cluster analyses using the k-means algorithm with a cluster number of 15 based on phenotypic variables from the Simons Simplex Collection (SSC). As a preliminary study, we conducted a conventional genome-wide association study (GWAS) with a data set of 597 ASD cases and 370 controls. In the second step, we divided cases based on the clustering results and conducted GWAS in each of the subgroups vs controls (cluster-based GWAS). We also conducted cluster-based GWAS on another SSC data set of 712 probands and 354 controls in the replication stage. In the preliminary study, which was conducted in conventional GWAS design, we observed no significant associations. In the second step of cluster-based GWASs, we identified 65 chromosomal loci, which included 30 intragenic loci located in 21 genes and 35 intergenic loci that satisfied the threshold of P < 5.0 x 10(-8). Some of these loci were located within or near previously reported candidate genes for ASD: CDH5, CNTN5, CNTNAP5, DNAH17, DPP10, DSCAM, FOXK1, GABBR2, GRIN2A5, ITPR1, NTM, SDK1, SNCA, and SRRM4. Of these 65 significant chromosomal loci, rs11064685 located within the SRRM4 gene had a significantly different distribution in the cases vs controls in the replication cohort. These findings suggest that clustering may successfully identify subgroups with relatively homogeneous disease etiologies. Further cluster validation and replication studies are warranted in larger cohorts.",
        "In this paper, we present the results of the research concerning extraction of informative gene expression profiles from high-dimensional array of gene expressions considering the state of patients' health using clustering method, ML-based binary classifiers and fuzzy inference system. Applying of the proposed stepwise procedure can allow us to extract the most informative genes taking into account both the subtypes of disease or state of the patient's health for further reconstruction of gene regulatory networks based on the allocated genes and following simulation of the reconstructed models. We used the publicly available gene expressions data as the experimental ones which were obtained using DNA microarray experiments and contained two types of patients' gene expression profiles-the patients with lung cancer tumor and healthy patients. The stepwise procedure of the data processing assumes the following steps-in the beginning, we reduce the number of genes by removing non-informative genes in terms of statistical criteria and Shannon entropy; then, we perform the stepwise hierarchical clustering of gene expression profiles at hierarchical levels from 1 to 10 using the SOTA (Self-Organizing Tree Algorithm) clustering algorithm with correlation distance metric. The quality of the obtained clustering was evaluated using the complex clustering quality criterion which is considered both the gene expression profiles distribution relative to center of the clusters where these gene expression profiles are allocated and the centers of the clusters distribution. The result of this stage execution was a selection of the optimal cluster at each of the hierarchical levels which corresponded to the minimum value of the quality criterion. At the next step, we have implemented a classification procedure of the examined objects using four well known binary classifiers-logistic regression, support-vector machine, decision trees and random forest classifier. The effectiveness of the appropriate technique was evaluated based on the use of ROC (Receiver Operating Characteristic) analysis using criteria, included as the components, the errors of both the first and the second kinds. The final decision concerning the extraction of the most informative subset of gene expression profiles was taken based on the use of the fuzzy inference system, the inputs of which are the results of the appropriate single classifiers operation and the output is the final solution concerning state of the patient's health. To our mind, the implementation of the proposed stepwise procedure of the informative gene expression profiles extraction create the conditions for the increasing effectiveness of the further procedure of gene regulatory networks reconstruction and the following simulation of the reconstructed models considering the subtypes of the disease and/or state of the patient's health.",
        "Random selection of initial centroids (centers) for clusters is a fundamental defect in K-means clustering algorithm as the algorithm's performance depends on initial centroids and may end up in local optimizations. Various hybrid methods have been introduced to resolve this defect in K-means clustering algorithm. As regards, there are no comparative studies comparing these methods in various aspects, the present paper compared three hybrid methods with K-means clustering algorithm using concepts of genetic algorithm, minimum spanning tree, and hierarchical clustering method. Although these three hybrid methods have received more attention in previous researches, fewer studies have compared their results. Hence, seven quantitative datasets with different characteristics in terms of sample size, number of features, and number of different classes are utilized in present study. Eleven indices of external and internal evaluating index were also considered for comparing the methods. Data indicated that the hybrid methods resulted in higher convergence rate in obtaining the final solution than the ordinary K-means method. Furthermore, the hybrid method with hierarchical clustering algorithm converges to the optimal solution with less iteration than the other two hybrid methods. However, hybrid methods with minimal spanning trees and genetic algorithms may not always or often be more effective than the ordinary K-means method. Therefore, despite the computational complexity, these three hybrid methods have not led to much improvement in the K-means method. However, a simulation study is required to compare the methods and complete the conclusion.",
        "The automatic detection of epilepsy is essentially the classification of EEG signals of seizures and nonseizures, and its purpose is to distinguish the different characteristics of seizure brain electrical signals and normal brain electrical signals. In order to improve the effect of automatic detection, this study proposes a new classification method based on unsupervised multiview clustering results. In addition, considering the high-dimensional characteristics of the original data samples, a deep convolutional neural network (DCNN) is introduced to extract the sample features to obtain deep features. The deep feature reduces the sample dimension and increases the sample separability. The main steps of our proposed novel EEG detection method contain the following three steps: first, a multiview FCM clustering algorithm is introduced, and the training samples are used to train the center and weight of each view. Then, the class center and weight of each view obtained by training are used to calculate the view-weighted membership value of the new prediction sample. Finally, the classification label of the new prediction sample is obtained. Experimental results show that the proposed method can effectively detect seizures.",
        "We propose a novel regularized mixture model for clustering matrix-valued data. The proposed method assumes a separable covariance structure for each cluster and imposes a sparsity structure (eg, low rankness, spatial sparsity) for the mean signal of each cluster. We formulate the problem as a finite mixture model of matrix-normal distributions with regularization terms, and then develop an expectation maximization type of algorithm for efficient computation. In theory, we show that the proposed estimators are strongly consistent for various choices of penalty functions. Simulation and two applications on brain signal studies confirm the excellent performance of the proposed method including a better prediction accuracy than the competitors and the scientific interpretability of the solution.",
        "Clustering analysis has been widely applied to single-cell RNA-sequencing (scRNA-seq) data to discover cell types and cell states. Algorithms developed in recent years have greatly helped the understanding of cellular heterogeneity and the underlying mechanisms of biological processes. However, these algorithms often use different techniques, were evaluated on different datasets and compared with some of their counterparts usually using different performance metrics. Consequently, there lacks an accurate and complete picture of their merits and demerits, which makes it difficult for users to select proper algorithms for analyzing their data. To fill this gap, we first do a review on the major existing scRNA-seq data clustering methods, and then conduct a comprehensive performance comparison among them from multiple perspectives. We consider 13 state of the art scRNA-seq data clustering algorithms, and collect 12 publicly available real scRNA-seq datasets from the existing works to evaluate and compare these algorithms. Our comparative study shows that the existing methods are very diverse in performance. Even the top-performance algorithms do not perform well on all datasets, especially those with complex structures. This suggests that further research is required to explore more stable, accurate, and efficient clustering algorithms for scRNA-seq data.",
        "Multiple object detection is challenging yet crucial in computer vision. In This study, owing to the negative effect of noise on multiple object detection, two clustering algorithms are used on both underwater sonar images and three-dimensional point cloud LiDAR data to study and improve the performance result. The outputs from using deep learning methods on both types of data are treated with K-Means clustering and density-based spatial clustering of applications with noise (DBSCAN) algorithms to remove outliers, detect and cluster meaningful data, and improve the result of multiple object detections. Results indicate the potential application of the proposed method in the fields of object detection, autonomous driving system, and so forth.",
        "In this paper, we propose four variants of the Markov random field model by using constrained clustering for breast mass segmentation. These variants were tested with a set of images extracted from a public database. The obtained results have shown that the proposed variants, which allow to include additional information in the form of constraints to the clustering process, present better visual segmentation results than the original model, as well as a lower final energy which implies a better quality in the final segmentation. Specifically, the centroid initialization method used by our variants allows us to locate about 90% of the regions of interest that contain a mass, which subsequently with the pairwise constraints helped us recover a maximum of 93% of the masses. The segmentation results are also quantitatively evaluated using three supervised segmentation measures. These measures show that the mass segmentation quality of the proposed variants, considering the breast density level, is consistent with the corresponding segmentation annotated by specialized radiologists.",
        "BACKGROUD AND OBJECTIVE: The control of clinical manifestation of vestibular system relies on an optimal diagnosis. This study aims to develop and test a new automated diagnostic scheme for vestibular disorder recognition. METHODS: In this study we stratify the Ellipse-fitting technique using the Video Nysta Gmographic (VNG) sequence to obtain the segmented pupil region. Furthermore, the proposed methodology enabled us to select the most optimum VNG features to effectively conduct quantitative evaluation of nystagmus signal. The proposed scheme using a multilayer neural network classifier (MNN) was tested using a dataset involving 98 patients affected by VD and 41 normal subjects. RESULTS: The new MNN scheme uses only five temporal and frequency parameters selected out of initial thirteen parameters. The scheme generated results reached 94% of classification accuracy. CONCLUSIONS: The developed expert system is promising in solving the problem of VNG analysis and achieving accurate results of vestibular disorder recognition or diagnosis comparing to other methods or classifiers.",
        "The aim of the study was to group the lactation curve (LC) of Holstein cows in several clusters based on their milking characteristics and to investigate physiological differences among the clusters. Milking data of 330 lactations which have a milk yield per day during entire lactation period were used. The data were obtained by refinement from 1332 lactations from 724 cows collected from commercial farms. Based on the similarity measures, clustering was performed using the k-medoids algorithm; the number of clusters was determined to be six, following the elbow method. Significant differences on parity, peak milk yield, DIM at peak milk yield, and average and total milk yield (p < 0.01) were observed among the clusters. Four clusters, which include 82% of data, show typical LC patterns. The other two clusters represent atypical patterns. Comparing to the LCs generated from the previous models, Wood, Wilmink and Dijsktra, it is observed that the prediction errors in the atypical patterns of the two clusters are much larger than those of the other four cases of typical patterns. The presented model can be used as a tool to refine characterization on the typical LC patterns, excluding atypical patterns as exceptional cases.",
        "The previous hospital acoustic literature has highlighted some important considerations and various complexities regarding objective noise measurements. However, extensive use of conventional acoustical metrics such as logarithmically averaged equivalent sound pressure levels (Leq) do not sufficiently describe hospital acoustical environments and often lack considerations of the room-based activity status that can significantly influence the soundscape. The goal of this study was to explore utilizing statistical clustering techniques in healthcare settings with a particular aim of identifying room-activity conditions. The acoustic measurements were conducted in the patient rooms of two pediatric hospital units and subsequently classified based on two room-activity conditions-active and non-active conditions-by applying statistical clustering analyses with standard k-means and fuzzy c-means algorithms. The results of this study demonstrate the most probable noise levels and degree of associations of the measured noise levels for the two room-activity conditions. The results were further validated in terms of the clustered levels, the number of conditions, and parameter dependency. The clustering approach allows for a more thorough soundscape characterization than single-number level descriptors alone by providing a method of identifying and describing the noise levels associated with typical, intrinsic activity conditions experienced by occupants.",
        "Predicting crash injury severity is a crucial constituent of reducing the consequences of traffic crashes. This study developed machine learning (ML) models to predict crash injury severity using 15 crash-related parameters. Separate ML models for each cluster were obtained using fuzzy c-means, which enhanced the predicting capability. Finally, four ML models were developed: feed-forward neural networks (FNN), support vector machine (SVM), fuzzy C-means clustering based feed-forward neural network (FNN-FCM), and fuzzy c-means based support vector machine (SVM-FCM). Features that were easily identified with little investigation on crash sites were used as an input so that the trauma center can predict the crash severity level based on the initial information provided from the crash site and prepare accordingly for the treatment of the victims. The input parameters mainly include vehicle attributes and road condition attributes. This study used the crash database of Great Britain for the years 2011-2016. A random sample of crashes representing each year was used considering the same share of severe and non-severe crashes. The models were compared based on injury severity prediction accuracy, sensitivity, precision, and harmonic mean of sensitivity and precision (i.e., F1 score). The SVM-FCM model outperformed the other developed models in terms of accuracy and F1 score in predicting the injury severity level of severe and non-severe crashes. This study concluded that the FCM clustering algorithm enhanced the prediction power of FNN and SVM models.",
        "Protein succinylation is a type of post-translational modification that occurs on lysine sites and plays a key role in protein conformation regulation and cellular function control. When training, it is difficult to designate negative samples because of the uncertainty of non-succinylation lysine sites, and if not handled properly, it may affect the performance of computational models dramatically. Therefore, we propose a new semi-supervised learning method to identify reliable non-succinylation lysine sites as negative samples. This method, named SSKM_Succ, also employs K-means clustering to divide data into 5 clusters. Besides, information of proximal PTMs and three kinds of sequence features are utilized to formulate protein. Then, we performe a two-step feature selection to remove redundant features and construct the optimization model for each cluster. Finally, support vector machine is applied to construct a prediction model for each cluster. Meanwhile, we compare the result with other existing tools, and it shows that our method is promising for predicting succinylation sites. Through analysis, we further verify that succinylated protein has potential effects on amino acid degradation and fatty acid metabolism, and speculate that protein succinylation may be closely related to neurodegenerative diseases. The code of SSKM_Succ is available on the web https://github.com/yangyq505/SSKM_Succ.git.",
        "Since protein 3D structure prediction is very important for biochemical study and drug design, researchers have developed many machine learning algorithms to predict protein 3D structures using the sequence information only. Understanding the sequence-to-structure relationship is key for the successful structure prediction. Previous approaches including the single shallow learning model, the single deep learning model and clustering algorithms all have disadvantages to understand precise sequence-to-structure relationship. In order to further improve the performance of the local protein structure prediction, a novel deep learning model called Clustering Recurrent Neural Network (CRNN) is proposed. In this model, the whole protein dataset is divided into multiple cluster subtrees. A RNN is trained for each cluster in the subtrees so that each RNN can be used to learn the computationally simpler local sequence-to-structure relationship instead of attempting to capture the global sequence-to-structure relationship. After learning the local sequence-to-structure relationship using RNN, CRNN is designed to predict distance matrices, torsion angles and secondary structures for backbone -carbon atoms of protein sequence segments. Our experimental analysis indicates that 3D structure prediction accuracy is comparable or better than other state-of-art approaches.",
        "Motif discovery and network clustering in complex networks have received a lot of attention in recent years, also they are still challenging tasks in bioinformatics, big data analytics and data mining applications. Motif discovery in big data networks has a lot of important applications in different domains such as engineering, bioinformatics, cheminformatics, genomics, sociology and ecology for revealing hidden frequent structures, functional building blocks, or knowledge discovery. In this paper, a motif localization method based on a novel clustering algorithm in complex networks is presented. In our method, for each complex network, a novel structure so-called Augmented Multiresolution Network (AMN) is generated, then it is adaptively partitioned into several clusters and their corresponding subnets. Then top ranked subnets are chosen to discover network motifs. We show that the proposed method provides an efficient solution for clustering and motif discovery; It speeds up current motif discovery algorithms by pruning non-promising regions of complex networks. Experimental results show our algorithm efficiently deals with complex networks representing large datasets with high-dimensionality such as big scientific data. Our method also provides motivations for future studies in big data and complex networks.",
        "An ordinary differential equation (ODE) model of the working of the thyroid system for euthyroidism has been presented. As clinical data for thyroid hormones is relatively scarce, such modelling offers potential benefits over wet lab procedures. Genetic algorithms developed for determining of parameters of the ODE system using the available data have been presented and evaluated. This approach enables subject specific parameter estimation towards characterisation of individual thyroid operation. Initially, a simple steady state model was used. Later a cosinor model for the circadian variation of thyroid hormones was used to obtain more reliable results, as indicated through sensitivity analysis in conjunction with other statistical methods. Our parameter determination method has been tested on groups of patients with similar observed values of thyroid stimulating hormone (TSH), free T3 and free T4 (identified through clustering) to determine their parameter values jointly. This approach appears to produce parameter sets with lower variation than parameters determined independently, thus leading to better parameter determination.",
        "The study of genetic variants(GVs) can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. ML algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. In this paper, we proposed convolutional embedded networks(CEN) in which we combine two DNN architectures called convolutional embedded clustering(CEC) and convolutional autoencoder(CAE) classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the 1000 genomes and Simons genome diversity projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index(ARI) of 0.915, the normalized mutual information(NMI) of 0.92, and the clustering accuracy(ACC) of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees and SHAP.",
        "Multiview clustering partitions data into different groups according to their heterogeneous features. Most existing methods degenerate the applicability of models due to their intractable hyper-parameters triggered by various regularization terms. Moreover, traditional spectral based methods always encounter the expensive time overheads and fail in exploring the explicit clusters from graphs. In this paper, we present a scalable and parameter-free graph fusion framework for multiview clustering, seeking for a joint graph compatible across multiple views in a self-supervised weighting manner. Our formulation coalesces multiple view-wise graphs straightforward and learns the weights as well as the joint graph interactively, which could actively release the model from any weight-related hyper-parameters. Meanwhile, we manipulate the joint graph by a connectivity constraint such that the connected components indicate clusters directly. The designed algorithm is initialization-independent and time-economical which obtains the stable performance and scales well with the data size. Substantial experiments on toy data as well as real datasets are conducted that verify the superiority of the proposed method compared to the state-of-the-arts over the clustering performance and time expenditure.",
        "This paper presents a novel accelerated exact k-means called as \"Ball k-means\" by using the ball to describe each cluster, which focus on reducing the point-centroid distance computation. The \"Ball k-means\" can exactly find its neighbor clusters for each cluster, resulting distance computations only between a point and its neighbor clusters' centroids instead of all centroids. What's more, each cluster can be divided into \"stable area\" and \"active area\", and the latter one is further divided into some exact \"annular area\". The assignment of the points in the \"stable area\" is not changed while the points in each \"annular area\" will be adjusted within a few neighbor clusters. There are no upper or lower bounds in the whole process. Moreover, ball k-means uses ball clusters and neighbor searching along with multiple novel stratagems for reducing centroid distance computations. In comparison with the current state-of-the art accelerated exact bounded methods, the Yinyang algorithm and the Exponion algorithm, as well as other top-of-the-line tree-based and bounded methods, the ball k-means attains both higher performance and performs fewer distance calculations, especially for large-k problems. The faster speed, no extra parameters and simpler design of \"Ball k-means\" make it an all-around replacement of the naive k-means.",
        "Data clustering, which is to partition the given data into different groups, has attracted much attention. Recently various effective algorithms have been developed to tackle the task. Among these methods, non-negative matrix factorization (NMF) has been demonstrated to be a powerful tool. However, there are still some problems. First, the standard NMF is sensitive to noises and outliers. Although l2,1 norm based NMF improves the robustness, it is still affected easily by large noises. Second, for most graph regularized NMF, the performance highly depends on the initial similarity graph. Third, many graph-based NMF models perform the graph construction and matrix factorization in two separated steps. Thus the learned graph structure may not be optimal. To overcome the above drawbacks, we propose a robust bi-stochastic graph regularized matrix factorization (RBSMF) framework for data clustering. Specifically, we present a general loss function, which is more robust than the commonly used L2 and L1 functions. Besides, instead of keeping the graph fixed, we learn an adaptive similarity graph. Furthermore, the graph updating and matrix factorization are processed simultaneously, which can make the learned graph more appropriate for clustering. Extensive experiments have shown the proposed RBSMF outperforms other state-of-the-art methods.",
        "Spectral clustering methods are gaining more and more interests and successfully applied in many fields because of their superior performance. However, there still exist two main problems to be solved: 1) spectral clustering methods consist of two successive optimization stages-spectral embedding and spectral rotation, which may not lead to globally optimal solutions, 2) and it is known that spectral methods are time-consuming with very high computational complexity. There are methods proposed to reduce the complexity for data vectors but not for graphs that only have information about similarity matrices. In this paper, we propose a new method to solve these two challenging problems for graph clustering. In the new method, a new framework is established to perform spectral embedding and spectral rotation simultaneously. The newly designed objective function consists of both terms of embedding and rotation, and we use an improved spectral rotation method to make it mathematically rigorous for the optimization. To further accelerate the algorithm, we derive a low-dimensional representation matrix from a graph by using label propagation, with which, in return, we can reconstruct a double-stochastic and positive semidefinite similarity matrix. Experimental results demonstrate that our method has excellent performance in time cost and accuracy.",
        "This study aimed at the shortcomings of existing fixation algorithms that are image-based only, and an effective tea fixation state monitoring algorithm was proposed. An adaptive filtering algorithm was used to automatically filter the ineffective information. Using the energy extractor, the complete energy information of each fixation image was extracted. The image energy attention mechanism was used to identify the prominent features, and based on these, the energy data was mapped to generate the data points as the training data. The cluster idea was adopted, and the training data feed the features trainer. The trend center data of the tea processing energy clustering was generated from different color channels. The corresponding decision function was designed which is based on the distance of the cluster center. The fixation degree of each monitoring image set was measured by the decision function. The Euclidean distance of the energy clustering center of the three channels with the same fixation time progressively approached. The triangle formed by these three points had a trend of gradually shrinking, which was first discovered by us. The detection results showed high accuracy compared with the common classification algorithms. It indicates that the algorithm proposed has positive guiding and reference significance.",
        "Hierarchical clustering is an important technique to organize big data for exploratory data analysis. However, existing one-size-fits-all hierarchical clustering methods often fail to meet the diverse needs of different users. To address this challenge, we present an interactive steering method to visually supervise constrained hierarchical clustering by utilizing both public knowledge (e.g., Wikipedia) and private knowledge from users. The novelty of our approach includes 1) automatically constructing constraints for hierarchical clustering using knowledge (knowledge-driven) and intrinsic data distribution (data-driven), and 2) enabling the interactive steering of clustering through a visual interface (user-driven). Our method first maps each data item to the most relevant items in a knowledge base. An initial constraint tree is then extracted using the ant colony optimization algorithm. The algorithm balances the tree width and depth and covers the data items with high confidence. Given the constraint tree, the data items are hierarchically clustered using evolutionary Bayesian rose tree. To clearly convey the hierarchical clustering results, an uncertainty-aware tree visualization has been developed to enable users to quickly locate the most uncertain sub-hierarchies and interactively improve them. The quantitative evaluation and case study demonstrate that the proposed approach facilitates the building of customized clustering trees in an efficient and effective manner.",
        "Density peaks clustering algorithm (DPC) has attracted the attention of many scholars because of its multiple advantages, including efficiently determining cluster centers, a lower number of parameters, no iterations, and no border noise. However, DPC does not provide a reliable and specific selection method of threshold (cutoff distance) and an automatic selection strategy of cluster centers. In this paper, we propose density peaks clustering by zero-pointed samples (DPC-ZPSs) of regional group borders. DPC-ZPS finds the subclusters and the cluster borders by zero-pointed samples (ZPSs). And then, subclusters are merged into individuals by comparing the density of edge samples. By iteration of the merger, the suitable dc and cluster centers are ensured. Finally, we compared state-of-the-art methods with our proposal in public datasets. Experiments show that our algorithm automatically determines cutoff distance and centers accurately.",
        "MOTIVATION: Accurate classification of patients into molecular subgroups is critical for the development of effective therapeutics and for deciphering what drives these subgroups to cancer. The availability of multiomics data catalogs for large cohorts of cancer patients provides multiple views into the molecular biology of the tumors with unprecedented resolution. RESULTS: We develop Pathway-based MultiOmic Graph Kernel clustering (PAMOGK) that integrates multiomics patient data with existing biological knowledge on pathways. We develop a novel graph kernel that evaluates patient similarities based on a single molecular alteration type in the context of a pathway. To corroborate multiple views of patients evaluated by hundreds of pathways and molecular alteration combinations, we use multiview kernel clustering. Applying PAMOGK to kidney renal clear cell carcinoma (KIRC) patients results in four clusters with significantly different survival times (P-value =1.24e-11). When we compare PAMOGK to eight other state-of-the-art multiomics clustering methods, PAMOGK consistently outperforms these in terms of its ability to partition KIRC patients into groups with different survival distributions. The discovered patient subgroups also differ with respect to other clinical parameters such as tumor stage and grade, and primary tumor and metastasis tumor spreads. The pathways identified as important are highly relevant to KIRC. AVAILABILITY AND IMPLEMENTATION: github.com/tastanlab/pamogk. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Performance analysis is an essential task in high-performance computing (HPC) systems, and it is applied for different purposes, such as anomaly detection, optimal resource allocation, and budget planning. HPC monitoring tasks generate a huge number of key performance indicators (KPIs) to supervise the status of the jobs running in these systems. KPIs give data about CPU usage, memory usage, network (interface) traffic, or other sensors that monitor the hardware. Analyzing this data, it is possible to obtain insightful information about running jobs, such as their characteristics, performance, and failures. The main contribution in this paper was to identify which metric/s (KPIs) is/are the most appropriate to identify/classify different types of jobs according to their behavior in the HPC system. With this aim, we had applied different clustering techniques (partition and hierarchical clustering algorithms) using a real dataset from the Galician computation center (CESGA). We concluded that (i) those metrics (KPIs) related to the network (interface) traffic monitoring provided the best cohesion and separation to cluster HPC jobs, and (ii) hierarchical clustering algorithms were the most suitable for this task. Our approach was validated using a different real dataset from the same HPC center.",
        "Brain tumors are one of the most deadly diseases with a high mortality rate. The shape and size of the tumor are random during the growth process. Brain tumor segmentation is a brain tumor assisted diagnosis technology that separates different brain tumor structures such as edema and active and tumor necrosis tissues from normal brain tissue. Magnetic resonance imaging (MRI) technology has the advantages of no radiation impact on the human body, good imaging effect on structural tissues, and an ability to realize tomographic imaging of any orientation. Therefore, doctors often use MRI brain tumor images to analyze and process brain tumors. In these images, the tumor structure is only characterized by grayscale changes, and the developed images obtained by different equipment and different conditions may also be different. This makes it difficult for traditional image segmentation methods to deal well with the segmentation of brain tumor images. Considering that the traditional single-mode MRI brain tumor images contain incomplete brain tumor information, it is difficult to segment the single-mode brain tumor images to meet clinical needs. In this paper, a sparse subspace clustering (SSC) algorithm is introduced to process the diagnosis of multimodal MRI brain tumor images. In the absence of added noise, the proposed algorithm has better advantages than traditional methods. Compared with the top 15 in the Brats 2015 competition, the accuracy is not much different, being basically stable between 10 and 15. In order to verify the noise resistance of the proposed algorithm, this paper adds 5%, 10%, 15%, and 20% Gaussian noise to the test image. Experimental results show that the proposed algorithm has better noise immunity than a comparable algorithm.",
        "Deciphering useful information from electrophysiological data recorded from the brain, in-vivo or in-vitro, is dependent on the capability to analyse spike patterns efficiently and accurately. The spike analysis mechanisms are heavily reliant on the clustering algorithms that enable separation of spike trends based on their spatio-temporal behaviors. Literature review report several clustering algorithms over decades focused on different applications. Although spike analysis algorithms employ only a small subset of clustering algorithms, however, not much work has been reported on the compliance and suitability of such clustering algorithms for spike analysis. In our study, we have attempted to comment on the suitability of available clustering algorithms and performance capacity when exposed to spike analysis. In this regard, the study reports a compatibility evaluation on algorithms previously employed in spike sorting as well as the algorithms yet to be investigated for application in sorting neural spikes. The performance of the algorithms is compared in terms of their accuracy, confusion matrix and accepted validation indices. Three data sets comprising of easy, difficult, and real spike similarity with known ground-truth are chosen for assessment, ensuring a uniform testbed. The procedure also employs two feature-sets, principal component analysis and wavelets. The report also presents a statistical score scheme to evaluate the performance individually and overall. The open nature of the data sets, the clustering algorithms and the evaluation criteria make the proposed evaluation framework widely accessible to the research community. We believe that the study presents a reference guide for emerging neuroscientists to select the most suitable algorithms for their spike analysis requirements.",
        "A surge in the availability of data from multiple sources and modalities is correlated with advances in how to obtain, compress, store, transfer, and process large amounts of complex high-dimensional data. The clustering challenge increases with the growth of data dimensionality which decreases the discriminate power of the distance metrics. Subspace clustering aims to group data drawn from a union of subspaces. In such a way, there is a large number of state-of-the-art approaches and we divide them into families regarding the method used in the clustering. We introduce a soft subspace clustering algorithm, a Self-organizing Map (SOM) with a time-varying structure, to cluster data without any prior knowledge of the number of categories or of the neural network topology, both determined during the training process. The model also assigns proper relevancies (weights) to different dimensions, capturing from the learning process the influence of each dimension on uncovering clusters. We employ a number of real-world datasets to validate the model. This algorithm presents a competitive performance in a diverse range of contexts among them data mining, gene expression, multi-view, computer vision and text clustering problems which include high-dimensional data. Extensive experiments suggest that our method very often outperforms the state-of-the-art approaches in all types of problems considered.",
        "Single-cell RNA-seq (scRNASeq) has become a powerful technique for measuring the transcriptome of individual cells. Unlike the bulk measurements that average the gene expressions over the individual cells, gene measurements at individual cells can be used to study several different tissues and organs at different stages. Identifying the cell types present in the sample from the single cell transcriptome data is a common goal in many single-cell experiments. Several methods have been developed to do this. However, correctly identifying the true cell types remains a challenge. We present a framework that addresses this problem. Our hypothesis is that the meaningful characteristics of the data will remain despite small perturbations of data. We validate the performance of the proposed method on eight publicly available scRNA-seq datasets with known cell types as well as five simulation datasets with different degrees of the cluster separability. We compare the proposed method with five other existing methods: RaceID, SNN-Cliq, SINCERA, SEURAT, and SC3. The results show that the proposed method performs better than the existing methods.",
        "Clustering analysis of gene expression data is essential for understanding complex biological data, and is widely used in important biological applications such as the identification of cell subpopulations and disease subtypes. In commonly used methods such as hierarchical clustering (HC) and consensus clustering (CC), holistic expression profiles of all genes are often used to assess the similarity between samples for clustering. While these methods have been proven successful in identifying sample clusters in many areas, they do not provide information about which gene sets (functions) contribute most to the clustering, thus limiting the interpretability of the resulting cluster. We hypothesize that integrating prior knowledge of annotated gene sets would not only achieve satisfactory clustering performance but also, more importantly, enable potential biological interpretation of clusters. Here we report ClusterMine, an approach that identifies clusters by assessing functional similarity between samples through integrating known annotated gene sets in functional annotation databases such as Gene Ontology. In addition to the cluster membership of each sample as provided by conventional approaches, it also outputs gene sets that most likely contribute to the clustering, thus facilitating biological interpretation. We compare ClusterMine with conventional approaches on nine real-world experimental datasets that represent different application scenarios in biology. We find that ClusterMine achieves better performances and that the gene sets prioritized by our method are biologically meaningful. ClusterMine is implemented as an R package and is freely available at: www.genemine.org/clustermine.php.",
        "BACKGROUND: The three-dimensional (3D) structure of the genome plays a crucial role in gene expression regulation. Chromatin conformation capture technologies (Hi-C) have revealed that the genome is organized in a hierarchy of topologically associated domains (TADs), sub-TADs, and chromatin loops. Identifying such hierarchical structures is a critical step in understanding genome regulation. Existing tools for TAD calling are frequently sensitive to biases in Hi-C data, depend on tunable parameters, and are computationally inefficient. METHODS: To address these challenges, we developed a novel sliding window-based spectral clustering framework that uses gaps between consecutive eigenvectors for TAD boundary identification. RESULTS: Our method, implemented in an R package, SpectralTAD, detects hierarchical, biologically relevant TADs, has automatic parameter selection, is robust to sequencing depth, resolution, and sparsity of Hi-C data. SpectralTAD outperforms four state-of-the-art TAD callers in simulated and experimental settings. We demonstrate that TAD boundaries shared among multiple levels of the TAD hierarchy were more enriched in classical boundary marks and more conserved across cell lines and tissues. In contrast, boundaries of TADs that cannot be split into sub-TADs showed less enrichment and conservation, suggesting their more dynamic role in genome regulation. CONCLUSION: SpectralTAD is available on Bioconductor, http://bioconductor.org/packages/SpectralTAD/ .",
        "In unsupervised learning, there is no apparent straightforward cost function that can capture the significant factors of variations and similarities. Since natural systems have smooth dynamics, an opportunity is lost if an unsupervised objective function remains static. The absence of concrete supervision suggests that smooth dynamics should be integrated during the training process. Compared to classical static cost functions, dynamic objective functions allow to better make use of the gradual and uncertain knowledge acquired through pseudo-supervision. In this paper, we propose Dynamic Autoencoder (DynAE), a novel model for deep clustering that addresses a clustering-reconstruction trade-off, by gradually and smoothly eliminating the reconstruction objective function in favor of a construction one. Experimental evaluations on benchmark datasets show that our approach achieves state-of-the-art results compared to the most relevant deep clustering methods.",
        "As single-cell RNA sequencing technologies mature, massive gene expression profiles can be obtained. Consequently, cell clustering and annotation become two crucial and fundamental procedures affecting other specific downstream analyses. Most existing single-cell RNA-seq (scRNA-seq) data clustering algorithms do not take into account the available cell annotation results on the same tissues or organisms from other laboratories. Nonetheless, such data could assist and guide the clustering process on the target dataset. Identifying marker genes through differential expression analysis to manually annotate large amounts of cells also costs labor and resources. Therefore, in this paper, we propose a novel end-to-end cell supervised clustering and annotation framework called scAnCluster, which fully utilizes the cell type labels available from reference data to facilitate the cell clustering and annotation on the unlabeled target data. Our algorithm integrates deep supervised learning, self-supervised learning and unsupervised learning techniques together, and it outperforms other customized scRNA-seq supervised clustering methods in both simulation and real data. It is particularly worth noting that our method performs well on the challenging task of discovering novel cell types that are absent in the reference data.",
        "The herpesvirus, polyomavirus, papillomavirus, and retrovirus families are associated with breast cancer. More effort is needed to assess the role of these viruses in the detection and diagnosis of breast cancer cases in women. The aim of this paper is to propose an efficient segmentation and classification system in the Mammography Image Analysis Society (MIAS) images of medical images. Segmentation became challenging for medical images because they are not illuminated in the correct way. The role of segmentation is essential in concern with detecting syndromes in human. This research work is on the segmentation of medical images based on intuitionistic possibilistic fuzzy c-mean (IPFCM) clustering. Intuitionist fuzzy c-mean (IFCM) and possibilistic fuzzy c-mean (PFCM) algorithms are hybridised to deal with problems of fuzzy c-mean. The introduced clustering methodology, in this article, retains the positive points of PFCM which helps to overcome the problem of the coincident clusters, thus the noise and less sensitivity to the outlier. The IPFCM improves the fundamentals of fuzzy c-mean by using intuitionist fuzzy sets. For the clustering of mammogram images for breast cancer detector of abnormal images, IPFCM technique has been applied. The proposed method has been compared with other available fuzzy clustering methods to prove the efficacy of the proposed approach. We compared support vector machine (SVM), decision tree (DT), rough set data analysis (RSDA) and Fuzzy-SVM classification algorithms for achieving an optimal classification result. The outcomes of the studies show that the proposed approach is highly effective with clustering and also with classification of breast cancer. The performance average segmentation accuracy for MIAS images with different noise level 5%, 7% and 9% of IPFCM is 91.25%, 87.50% and 85.30% accordingly. The average classification accuracy rates of the methods (Otsu, Fuzzy c-mean, IFCM, PFCM and IPFCM) for Fuzzy-SVM are 79.69%, 92.19%, 93.13%, 95.00%, and 98.85%, respectively.",
        "OBJECTIVE: Data normalization and clustering are mandatory steps in gene expression and downstream analyses, respectively. However, user-friendly implementations of these methodologies are available exclusively under expensive licensing agreements, or in stand-alone scripts developed, reflecting on a great obstacle for users with less computational skills. RESULTS: We developed an online tool called CORAZON (Correlations Analyses Zipper Online), which implements three unsupervised learning methods to cluster gene expression datasets in a friendly environment. It allows the usage of eight gene expression normalization/transformation methodologies and the attribute's influence. The normalizations requiring the gene length only could be performed to RNA-seq, meanwhile the others can be used with microarray and/or NanoString data. Clustering methodologies performances were evaluated through five models with accuracies between 92 and 100%. We applied our tool to obtain functional insights of non-coding RNAs (ncRNAs) based on Gene Ontology enrichment of clusters in a dataset generated by the ENCODE project. The clusters where the majority of transcripts are coding genes were enriched in Cellular, Metabolic, Transports, and Systems Development categories. Meanwhile, the ncRNAs were enriched in the Detection of Stimulus, Sensory Perception, Immunological System, and Digestion categories. CORAZON source-code is freely available at https://gitlab.com/integrativebioinformatics/corazon and the web-server can be accessed at http://corazon.integrativebioinformatics.me .",
        "BACKGROUND: The healthcare sector is an interesting target for fraudsters. The availability of a great amount of data makes it possible to tackle this issue with the adoption of data mining techniques, making the auditing process more efficient and effective. This research has the objective of developing a novel data mining model devoted to fraud detection among hospitals using Hospital Discharge Charts (HDC) in Administrative Databases. In particular, it is focused on the DRG upcoding practice, i.e., the tendency of registering codes for provided services and inpatients health status so to make the hospitalization fall within a more remunerative DRG class. METHODS: We propose a two-step algorithm: the first step entails kmeans clustering of providers to identify locally consistent and locally similar groups of hospitals, according to their characteristics and behavior treating a specific disease, in order to spot outliers within this groups of peers. An initial grid search for the best number of features to be selected (through Principal Feature Analysis) and the best number of local groups makes the algorithm extremely flexible. In the second step, we propose a human-decision support system that helps auditors cross-validating the identified outliers, analyzing them w.r.t. fraud-related variables, and the complexity of patients' casemix they treated. The proposed algorithm was tested on a database relative to HDC collected by Regione Lombardia (Italy) in a time period of three years (2013-2015), focusing on the treatment of Heart Failure. RESULTS: The model identified 6 clusters of hospitals and 10 outliers among the 183 units. Out of those providers, we report the in depth the application of Step Two on three Hospitals (two private and one public). Cross-validating with the patients' population and the hospitals' characteristics, the public hospital seemed justified in its outlierness, while the two private providers were deemed interesting for a further investigation by auditors. CONCLUSIONS: The proposed model is promising in identifying anomalous DRG coding behavior and it is easily transferrable to all diseases and contexts of interest. Our proposal contributes to the limited literature regarding behavioral models for fraud detection, identifying the most 'cautious' fraudsters. The results of the first and the second Steps together represent a valuable set of information for auditors in their preliminary investigation.",
        "Unmanned Aerial Vehicle (UAV) has been widely used in various applications of wireless network. A system of UAVs has the function of collecting data, offloading traffic for ground Base Stations (BSs) and illuminating coverage holes. However, inter-UAV interference is easily introduced because of the huge number of LoS paths in the air-to-ground channel. In this paper, we propose an interference management framework for UAV-assisted networks, consisting of two main modules: power control and UAV clustering. The power control is executed first to adjust the power levels of UAVs. We model the problem of power control for UAV networks as a non-cooperative game which is proved to be an exact potential game and the Nash equilibrium is reached. Next, to further improve system user rate, coordinated multi-point (CoMP) technique is implemented. The cooperative UAV sets are established to serve users and thus transforming the interfering links into useful links. Affinity propagation is applied to build clusters of UAVs based on the interference strength. Simulation results show that the proposed algorithm integrating power control with CoMP can effectively reduce the interference and improve system sum-rate, compared to Non-CoMP scenario. The law of cluster formation is also obtained where the average cluster size and the number of clusters are affected by inter-UAV distance.",
        "Multiview clustering refers to partition data according to its multiple views, where information from different perspectives can be jointly used in some certain complementary manner to produce more sensible clusters. It is believed that most of the existing multiview clustering methods technically suffer from possibly corrupted data, resulting in a dramatically decreased clustering performance. To overcome this challenge, we propose a multiview spectral clustering method based on robust subspace segmentation in this article. Our proposed algorithm is composed of three modules, that is: 1) the construction of multiple feature matrices from all views; 2) the formulation of a shared low-rank latent matrix by a low rank and sparse decomposition; and 3) the use of the Markov-chain-based spectral clustering method for producing the final clusters. To solve the optimization problem for a low rank and sparse decomposition, we develop an optimization procedure based on the scheme of the augmented Lagrangian method of multipliers. The experimental results on several benchmark datasets indicate that the proposed method outperforms favorably compared to several state-of-the-art multiview clustering techniques.",
        "MOTIVATION: Graph representations of genomes are capable of expressing more genetic variation and can therefore better represent a population than standard linear genomes. However, due to the greater complexity of genome graphs relative to linear genomes, some functions that are trivial on linear genomes become much more difficult in genome graphs. Calculating distance is one such function that is simple in a linear genome but complicated in a graph context. In read mapping algorithms such distance calculations are fundamental to determining if seed alignments could belong to the same mapping. RESULTS: We have developed an algorithm for quickly calculating the minimum distance between positions on a sequence graph using a minimum distance index. We have also developed an algorithm that uses the distance index to cluster seeds on a graph. We demonstrate that our implementations of these algorithms are efficient and practical to use for a new generation of mapping algorithms based upon genome graphs. AVAILABILITY AND IMPLEMENTATION: Our algorithms have been implemented as part of the vg toolkit and are available at https://github.com/vgteam/vg.",
        "A 2D-bubble column reactor (BCR) including gas and liquid phases is simulated, and fluid characteristics such as gas-phase volume fraction and gas-phase turbulence are extracted from the CFD simulations. A type of heuristic algorithm called adaptive network-based fuzzy inference system (ANFIS) is applied here to simulate the gas-phase volume fraction in a physical system. Indeed, the x direction, the y direction, and gas-phase turbulence are considered as the ANFIS inputs. Changes in the number of inputs as well as membership functions are evaluated and studied to obtain a high level of ANFIS intelligence. By implementing the highest ANFIS intelligence, a surface is predicted, which suggests that the gas-phase volume fraction is based on x and y directions. It provides capability to achieve the amount of gas-phase volume fraction in different points of a 2D-BCR.",
        "BACKGROUND: Early diagnosis of a brain tumor may increase life expectancy. Magnetic resonance imaging (MRI) accompanied by several segmentation algorithms is preferred as a reliable method for assessment. The availability of high-dimensional medical image data during diagnosis places a heavy computational burden and a suitable pre-processing step is required for lower- dimensional representation. The storage requirement and complexity of image data are also a concern. To address this concern, the random projection technique (RPT) is widely used as a multivariate approach for data reduction. AIM: This study mainly focuses on T1-weighted MRI image clustering for brain tumor segmentation with dimension reduction by using the conventional principal component analysis (PCA) and RPT. METHODS: Two clustering algorithms, K-means and fuzzy c-means (FCM) were used for brain tumor detection. The primary study objective was to present a comparison of the two clustering methods between MRI images subjected to PCA and RPT. In addition to the original dimension of 512 x 512, three other image sizes, 256 x 256, 128 x 128, and 64 x 64, were used to determine the effect of the methods. RESULTS: In terms of average reconstruction, Euclidean distance, and segmentation distance errors, the RPT produced better results than the PCA method for all the clustered images from clustering techniques. CONCLUSION: According to the values of performance metrics, RPT supported fuzzy c-means in achieving the best clustering performance and provided significant results for each new size of the MRI images.",
        "This paper addresses the lack of robustness of feature selection algorithms for fuzzy clustering segmentation with the Gaussian mixture model. Assuming that the neighbourhood pixels and the centre pixels obey the same distribution, a Markov method is introduced to construct the prior probability distribution and achieve the membership degree regularisation constraint for clustering sample points. Then, a noise smoothing factor is introduced to optimise the prior probability constraint. Second, a power index is constructed by combining the classification membership degree and prior probability since the Kullback-Leibler (KL) divergence of the noise smoothing factor is used to supervise the prior probability; this probability is embedded into Fuzzy Superpixels Fuzzy C-means(FSFCM) as a regular factor. This paper proposes a fuzzy clustering image segmentation algorithm based on an adaptive feature selection Gaussian mixture model with neighbourhood information constraints. To verify the segmentation performance and anti-noise robustness of the improved algorithm, the fuzzy C-means clustering algorithm Fuzzy C-means (FCM), FSFCM, Spatially Variant Finite Mixture Model(SVFMM), EGFMM, extended Gaussian mixture model (EGMM), adaptive feature selection robust fuzzy clustering segmentation algorithm (AFSFCM), fast and robust spatially constrained Gaussian mixture model (GMM) for image segmentation (FRSCGMM), and improve method are used to segment grey images containing Gaussian noise, salt-and-pepper noise, multiplicative noise and mixed noise. The peak signal-to-noise ratio (PSNR) and the error rate (MCR) are used as the theoretical basis for assessing the segmentation results. The improved algorithm indicators proposed in this paper are optimised. The improved algorithm yields increases of 0.1272-12.9803 dB, 1.5501-13.4396 dB, 1.9113-11.2613 dB and 1.0233-10.2804 dB over the other methods, and the Misclassification rate(MSR) decreases by 0.32-37.32%, 5.02-41.05%, 0.3-21.79% and 0.9-30.95% compared to that with the other algorithms. It is verified that the segmentation results of the improved algorithm have good regional consistency and strong anti-noise robustness, and they meet the needs of noisy image segmentation.",
        "Air pollution these days could cause severe effects on human health. As human health is crumbled with serious respiratory or other lung diseases, it is prominent to study air pollution. One of the ways to address this issue is by applying clustering techniques. The two main important problems that are faced in the clustering algorithm are, firstly, the exact shape of the cluster and the number of clusters that input data can produce. Secondly, choosing an appropriate algorithm for a particular problem is not clearly known. Finally, multiple replications of the same algorithm lead to alternative solutions due to the fact such as random initialization of cluster heads. Ensembling algorithms can handle these problems and overcome bias and variance in the traditional clustering process. An adequate study has not been carried out in the ensembling approach mainly for clustering. In this paper, we use an enhanced ensemble clustering method to cluster the pollution data levels. This study helps to take preventive measures that are needed to control further contamination, reduce the alarming levels, and analyze the results to find healthy and unhealthy regions in a given area. This ensemble technique also explains about uncertain objects that are found in clustering. The distinct advantage of this algorithm is that there is no requirement of prior information about the data. This experiment shows that the implemented ensemble consensus clustering has demonstrated improved performance when compared with basic clustering algorithms.",
        "Systems biology aims at holistically understanding the complexity of biological systems. In particular, nowadays with the broad availability of gene expression measurements, systems biology challenges the deciphering of the genetic cell machinery from them. In order to help researchers, reverse engineer the genetic cell machinery from these noisy datasets, interactive exploratory clustering methods, pipelines and gene clustering tools have to be specifically developed. Prior methods/tools for time series data, however, do not have the following four major ingredients in analytic and methodological view point: (i) principled time-series feature extraction methods, (ii) variety of manifold learning methods for capturing high-level view of the dataset, (iii) high-end automatic structure extraction, and (iv) friendliness to the biological user community. With a view to meet the requirements, we present AGCT (A Geometric Clustering Tool), a software package used to unravel the complex architecture of large-scale, non-necessarily synchronized time-series gene expression data. AGCT capture signals on exhaustive wavelet expansions of the data, which are then embedded on a low-dimensional non-linear map using manifold learning algorithms, where geometric proximity captures potential interactions. Post-processing techniques, including hard and soft information geometric clustering algorithms, facilitate the summarizing of the complete map as a smaller number of principal factors which can then be formally identified using embedded statistical inference techniques. Three-dimension interactive visualization and scenario recording over the processing helps to reproduce data analysis results without additional time. Analysis of the whole-cell Yeast Metabolic Cycle (YMC) moreover, Yeast Cell Cycle (YCC) datasets demonstrate AGCT's ability to accurately dissect all stages of metabolism and the cell cycle progression, independently of the time course and the number of patterns related to the signal. Analysis of Pentachlorophenol iduced dataset demonstrat how AGCT dissects data to identify two networks: Interferon signaling and NRF2-signaling networks.",
        "Cluster analysis, commonly used to explore large biomedical datasets, can be challenging, notably due to missing data or left-censored data induced by the sensitivity limits of the biochemical measurement method. Usually, complete-case analysis, simple imputation, or stochastic simple imputation are applied before clustering. More recently, consensus methods following multiple imputation have been proposed. However, they ignore left-censoring and do not allow the number of clusters to vary across the partitions of each imputed dataset. Here, we developed a consensus-based clustering algorithm in which left-censored data are taken into account using a modified multiple imputation method and the number of clusters is estimated for each imputed dataset. A simulation study was conducted to assess the performance in terms of the number of clusters, the percentage of unclassified observations, and the adjusted Rand index. The simulation results showed that the investigated method works well compared to several alternative approaches. A real-world application in breast cancer patients showed that the proposed method may reveal novel clusters of patients.",
        "Large-scale transcriptomic data is used by biologists for the discovery of new molecular patterns or cell subpopulations. Clustering is one of the most popular methods for dimensionality reduction and data analysis for large scale datasets. The major problem while clustering the data is the selection of the optimal number of clusters (k) for each dataset and to discover new insights from it. We have developed Recursive Consensus Clustering (RCC), an unsupervised clustering algorithm for novel subtype discovery from both bulk and single-cell datasets. RCC is available as an R package and facilitates the generation of new biological insights through intuitive visualization of clustering results.",
        "MOTIVATION: Chromosomal patterning of gene expression in cancer can arise from aneuploidy, genome disorganization or abnormal DNA methylation. To map such patterns, we introduce a weighted univariate clustering algorithm to guarantee linear runtime, optimality and reproducibility. RESULTS: We present the chromosome clustering method, establish its optimality and runtime and evaluate its performance. It uses dynamic programming enhanced with an algorithm to reduce search-space in-place to decrease runtime overhead. Using the method, we delineated outstanding genomic zones in 17 human cancer types. We identified strong continuity in dysregulation polarity-dominance by either up- or downregulated genes in a zone-along chromosomes in all cancer types. Significantly polarized dysregulation zones specific to cancer types are found, offering potential diagnostic biomarkers. Unreported previously, a total of 109 loci with conserved dysregulation polarity across cancer types give insights into pan-cancer mechanisms. Efficient chromosomal clustering opens a window to characterize molecular patterns in cancer genome and beyond. AVAILABILITY AND IMPLEMENTATION: Weighted univariate clustering algorithms are implemented within the R package 'Ckmeans.1d.dp' (4.0.0 or above), freely available at https://cran.r-project.org/package=Ckmeans.1d.dp. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "There are two problems in the traditional spectral clustering algorithm. Firstly, when it uses Gaussian kernel function to construct the similarity matrix, different scale parameters in Gaussian kernel function will lead to different results of the algorithm. Secondly, K-means algorithm is often used in the clustering stage of the spectral clustering algorithm. It needs to initialize the cluster center randomly, which will result in the instability of the results. In this paper, an improved spectral clustering algorithm is proposed to solve these two problems. In constructing a similarity matrix, we proposed an improved Gaussian kernel function, which is based on the distance information of some nearest neighbors and can adaptively select scale parameters. In the clustering stage, beetle antennae search algorithm with damping factor is proposed to complete the clustering to overcome the problem of instability of the clustering results. In the experiment, we use four artificial data sets and seven UCI data sets to verify the performance of our algorithm. In addition, four images in BSDS500 image data sets are segmented in this paper, and the results show that our algorithm is better than other comparison algorithms in image segmentation.",
        "Health management information systems (HMISs) in low- and middle-income countries have been used to collect large amounts of data after years of implementation, especially in support of HIV care services. National-level aggregate reporting data derived from HMISs are essential for informed decision-making. However, the optimal statistical approaches and algorithms for deriving key insights from these data are yet to be fully and adequately utilized. This paper demonstrates use of the k-means clustering algorithm as an approach in supporting monitoring of facility reporting and data-informed decision-making, using the case example of Kenya HIV national reporting data. Results reveal four homogeneous cluster categories that can be used in assessing overall facility performance and rating of that performance.",
        "Non-negative matrix factorization (NMF) has become one of the most powerful methods for clustering and feature selection. However, the performance of the traditional NMF method severely degrades when the data contain noises and outliers or the manifold structure of the data is not taken into account. In this article, a novel method called correntropy-based hypergraph regularized NMF (CHNMF) is proposed to solve the above problem. Specifically, we use the correntropy instead of the Euclidean norm in the loss term of CHNMF, which will improve the robustness of the algorithm. And the hypergraph regularization term is also applied to the objective function, which can explore the high-order geometric information in more sample points. Then, the half-quadratic (HQ) optimization technique is adopted to solve the complex optimization problem of CHNMF. Finally, extensive experimental results on multi-cancer integrated data indicate that the proposed CHNMF method is superior to other state-of-the-art methods for clustering and feature selection.",
        "Automated methods that can identify white matter bundles from large tractography datasets have several applications in neuroscience research. In these applications, clustering algorithms have shown to play an important role in the analysis and visualization of white matter structure, generating useful data which can be the basis for further studies. This work proposes FFClust, an efficient fiber clustering method for large tractography datasets containing millions of fibers. Resulting clusters describe the whole set of main white matter fascicles present on an individual brain. The method aims to identify compact and homogeneous clusters, which enables several applications. In individuals, the clusters can be used to study the local connectivity in pathological brains, while at population level, the processing and analysis of reproducible bundles, and other post-processing algorithms can be carried out to study the brain connectivity and create new white matter bundle atlases. The proposed method was evaluated in terms of quality and execution time performance versus the state-of-the-art clustering techniques used in the area. Results show that FFClust is effective in the creation of compact clusters, with a low intra-cluster distance, while keeping a good quality Davies-Bouldin index, which is a metric that quantifies the quality of clustering approaches. Furthermore, it is about 8.6 times faster than the most efficient state-of-the-art method for one million fibers dataset. In addition, we show that FFClust is able to correctly identify atlas bundles connecting different brain regions, as an example of application and the utility of compact clusters.",
        "Visit-to-visit blood pressure variability (BPV) has been shown to be a predictor of cardiovascular disease. We aimed to classify the BPV levels using different machine learning algorithms. Visit-to-visit blood pressure readings were extracted from the SPRINT study in the United States and eHealth cohort in Hong Kong (HK cohort). Patients were clustered into low, medium, and high BPV levels with the traditional quantile clustering and 5 machine learning algorithms including K-means. Clustering methods were assessed by Stability Index. Similarities were assessed by Davies-Bouldin Index and Silhouette Index. Cox proportional hazard regression models were fitted to compare the risk of myocardial infarction, stroke, and heart failure. A total of 8133 participants had average blood pressure measurement 14.7 times in 3.28 years in SPRINT and 1094 participants who had average blood pressure measurement 165.4 times in 1.37 years in HK cohort. Quantile clustering assigned one-third participants as high BPV level, but machine learning methods only assigned 10% to 27%. Quantile clustering is the most stable method (stability index: 0.982 in the SPRINT and 0.948 in the HK cohort) with some levels of clustering similarities (Davies-Bouldin Index: 0.752 and 0.764, respectively). K-means clustering is the most stable across the machine learning algorithms (stability index: 0.975 and 0.911, respectively) with the lowest clustering similarities (Davies-Bouldin Index: 0.653 and 0.680, respectively). One out of 7 in the population was classified with high BPV level, who showed to have higher risk of stroke and heart failure. Machine learning methods can improve BPV classification for better prediction of cardiovascular diseases.",
        "MOTIVATION: The high resolution of single-cell DNA sequencing (scDNA-seq) offers great potential to resolve intratumor heterogeneity (ITH) by distinguishing clonal populations based on their mutation profiles. However, the increasing size of scDNA-seq datasets and technical limitations, such as high error rates and a large proportion of missing values, complicate this task and limit the applicability of existing methods. RESULTS: Here, we introduce BnpC, a novel non-parametric method to cluster individual cells into clones and infer their genotypes based on their noisy mutation profiles. We benchmarked our method comprehensively against state-of-the-art methods on simulated data using various data sizes, and applied it to three cancer scDNA-seq datasets. On simulated data, BnpC compared favorably against current methods in terms of accuracy, runtime and scalability. Its inferred genotypes were the most accurate, especially on highly heterogeneous data, and it was the only method able to run and produce results on datasets with 5000 cells. On tumor scDNA-seq data, BnpC was able to identify clonal populations missed by the original cluster analysis but supported by Supplementary Experimental Data. With ever growing scDNA-seq datasets, scalable and accurate methods such as BnpC will become increasingly relevant, not only to resolve ITH but also as a preprocessing step to reduce data size. AVAILABILITY AND IMPLEMENTATION: BnpC is freely available under MIT license at https://github.com/cbg-ethz/BnpC. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "MOTIVATION: Diverse applications-particularly in tumour subtyping-have demonstrated the importance of integrative clustering techniques for combining information from multiple data sources. Cluster Of Clusters Analysis (COCA) is one such approach that has been widely applied in the context of tumour subtyping. However, the properties of COCA have never been systematically explored, and its robustness to the inclusion of noisy datasets is unclear. RESULTS: We rigorously benchmark COCA, and present Kernel Learning Integrative Clustering (KLIC) as an alternative strategy. KLIC frames the challenge of combining clustering structures as a multiple kernel learning problem, in which different datasets each provide a weighted contribution to the final clustering. This allows the contribution of noisy datasets to be down-weighted relative to more informative datasets. We compare the performances of KLIC and COCA in a variety of situations through simulation studies. We also present the output of KLIC and COCA in real data applications to cancer subtyping and transcriptional module discovery. AVAILABILITY AND IMPLEMENTATION: R packages klic and coca are available on the Comprehensive R Archive Network. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Bicycle sharing systems (BSSs) have established a new shared-economy mobility model. After a rapid growth they are evolving into a fully-functional mobile sensor platform for cities. The viability of BSSs is floored by their operational costs, mainly due to rebalancing operations. Rebalancing implies transporting bicycles to and from docking stations in order to guarantee the service. Rebalancing performs clustering to group docking stations by behaviour and proximity. In this paper we propose a Hierarchical Agglomerative Clustering based on an Ultra-Light Edge Computing Algorithm (HAC-ULECA). We eliminate the proximity and let Hierarchical Agglomerative Clustering (HAC) focus on behaviour. Behaviour is represented by ULECA as an activity profile based on the net flow of arrivals and departures in a docking station. This drastically reduces the computing requirements which allows ULECA to run as an edge computing functionality embedded into the physical layer of the Internet of Shared Bikes (IoSB) architecture. We have applied HAC-ULECA to real data from BiciMAD, the public BSS in Madrid (Spain). Our results, presented as dendograms, graphs, geographical maps, and colour maps, show that HAC-ULECA is capable of separating behaviour profiles related to business and residential areas and extracting meaningful spatio-temporal information about the BSS and the city's mobility.",
        "Single-cell RNA sequencing technologies have revolutionized biomedical research by providing an effective means to profile gene expressions in individual cells. One of the first fundamental steps to perform the in-depth analysis of single-cell sequencing data is cell type classification and identification. Computational methods such as clustering algorithms have been utilized and gaining in popularity because they can save considerable resources and time for experimental validations. Although selecting the optimal features (i.e., genes) is an essential process to obtain accurate and reliable single-cell clustering results, the computational complexity and dropout events that can introduce zero-inflated noise make this process very challenging. In this paper, we propose an effective single-cell clustering algorithm based on the ensemble feature selection and similarity measurements. We initially identify the set of potential features, then measure the cell-to-cell similarity based on the subset of the potentials through multiple feature sampling approaches. We construct the ensemble network based on cell-to-cell similarity. Finally, we apply a network-based clustering algorithm to obtain single-cell clusters. We evaluate the performance of our proposed algorithm through multiple assessments in real-world single-cell RNA sequencing datasets with known cell types. The results show that our proposed algorithm can identify accurate and consistent single-cell clustering. Moreover, the proposed algorithm takes relative expression as input, so it can easily be adopted by existing analysis pipelines. The source code has been made publicly available at https://github.com/jeonglab/scCLUE.",
        "We propose a multistate joint model to analyze interval-censored event-history data subject to within-unit clustering and nonignorable missing data. The model is motivated by a study of the neurocysticercosis (NC) cyst evolution at the cyst-level, taking into account the multiple cysts phases with intermittent missing data and loss to follow-up, as well as the intra-brain clustering of observations made on a predefined data collection schedule. Of particular interest in this study is the description of the process leading to cyst resolution, and whether this process varies by antiparasitic treatment. The model uses shared random effects to account for within-brain correlation and to explain the hidden heterogeneity governing the missing data mechanism. We developed a likelihood-based method using a Monte Carlo EM algorithm for the inference. The practical utility of the methods is illustrated using data from a randomized controlled trial on the effect of antiparasitic treatment with albendazole on NC cysts among patients from six hospitals in Ecuador. Simulation results demonstrate that the proposed methods perform well in the finite sample and misspecified models that ignore the data complexities could lead to substantial biases.",
        "The development of single-cell RNA-sequencing (scRNA-seq) technologies brings tremendous opportunities for quantitative research and analyses at the cellular level. In particular, as a crucial task of scRNA-seq analysis, single cell clustering shines a light on natural groupings of cells to give new insights into the biological mechanisms and disease studies. However, it remains a challenge to identify cell clusters from lots of cell mixtures effectively and accurately. In this paper, we propose a novel adaptive joint clustering framework, named the low-rank self-representation K-means method (LRSK), to learn the data representation matrix and cluster indicator matrix jointly from scRNA-seq data. Specifically, instead of calculating the similarities among cells from the original data, we seek a low-rank representation of the original data to better reflect the underlying relationships among cells. Moreover, an Augmented Lagrangian Multiplier (ALM) based optimization algorithm is adopted to solve this problem. Experimental results on various scRNA-seq datasets and case studies demonstrate that our method performs better than other state-of-the-art single cell clustering algorithms. The analysis of unlabeled large single-cell liver cancer sequencing data further shows that our prediction results are more reasonable and interpretable.",
        "In the context of increasing interest in computer-assisted diagnosis for skin lesion images and mobile applications to be used in real life settings, we propose a combined desktop-smartphone solution for dermatological image classification. Hierarchical agglomerative and divisive clustering are both implemented as methods of cluster analysis, with the RGB color histogram as descriptor for a global image analysis. The cosine similarity is employed for classifying the query image in one of the available clusters, characterized by their centroids. The solution has been tested with a public database of dermoscopic images, with an overall accuracy of 0.73, 95%CI (0.58;0.85).",
        "The fuzzy C -means (FCM) clustering procedure is an unsupervised form of grouping the homogenous pixels of an image in the feature space into clusters. A brain magnetic resonance (MR) image is affected by noise and intensity inhomogeneity (IIH) during the acquisition process. FCM has been used in MR brain tissue segmentation. However, it does not consider the neighboring pixels for computing the membership values, thereby misclassifying the noisy pixels. The inaccurate cluster centers obtained in FCM do not address the problem of IIH. A fixed value of the fuzzifier ( m ) used in FCM brings uncertainty in controlling the fuzziness of the extracted clusters. To resolve these issues, we suggest a novel type-2 adaptive weighted spatial FCM (AWSFCM) clustering algorithm for MR brain tissue segmentation. The idea of type-2 FCM applied to the problem on hand is new and is reported in this article. The application of the proposed technique to the problem of MR brain tissue segmentation replaces the fixed fuzzifier value with a fuzzy linguistic fuzzifier value ( M ). The introduction of the spatial information in the membership function reduces the misclassification of noisy pixels. Furthermore, the incorporation of adaptive weights into the cluster center update function improves the accuracy of the final cluster centers, thereby reducing the effect of IIH. The suggested algorithm is evaluated using T1-w, T2-w, and proton density (PD) brain MR image slices. The performance is justified in terms of qualitative and quantitative measures followed by statistical analysis. The outcomes demonstrate the superiority and robustness of the algorithm in comparison to the state-of-the-art methods. This article is useful for the cybernetics application.",
        "We prove that the quantum Gibbs states of spin systems above a certain threshold temperature are approximate quantum Markov networks, meaning that the conditional mutual information decays rapidly with distance. We demonstrate the exponential decay for short-ranged interacting systems and power-law decay for long-ranged interacting systems. Consequently, we establish the efficiency of quantum Gibbs sampling algorithms, a strong version of the area law, the quasilocality of effective Hamiltonians on subsystems, a clustering theorem for mutual information, and a polynomial-time algorithm for classical Gibbs state simulations.",
        "Image segmentation is still an open problem especially when intensities of the objects of interest are overlapped due to the presence of intensity inhomogeneities. A bias correction embedded level set model is proposed in this paper where inhomogeneities are estimated by orthogonal primary functions. First, an inhomogeneous intensity clustering energy is defined based on global distribution characteristics of the image intensities, and membership functions of the clusters described by the level set function are then introduced to define the data term energy of the proposed model. Second, a regularization term and an arc length term are also included to regularize the level set function and smooth its zero-level set contour, respectively. Third, the proposed model is extended to multichannel and multiphase patterns to segment colorful images and images with multiple objects, respectively. Experimental results and comparison with relevant models demonstrate the advantages of the proposed model in terms of bias correction and segmentation accuracy on widely used synthetic and real images and the BrainWeb and the IBSR image repositories.",
        "This study aimed to examine the clustering of 24-h movement behaviors (moderate to vigorous physical activity, screen time, and sleep duration) and their association with cardiorespiratory fitness among adolescents. We evaluated 561 adolescents (52.1% girls; mean age, 13.0 +/- 1.0 years) from Florianopolis, Brazil. A 20-m shuttle run was used to assess cardiorespiratory fitness, while a questionnaire was used to measure 24-h movement behaviors. A latent class analysis was performed to identify the clustering of 24-h movement behaviors, while linear Bayesian mixed-effect regression models were applied to identify their association with cardiorespiratory fitness. Two classes were identified: unhealthy (10.4%), characterized as a high probability of practicing less than 300 min/week of moderate to vigorous physical activity, spending more than 4 h/day in front of screens, and sleeping less than 8 h/day; and healthy (89.6%), characterized by a high probability of practicing more than 420 min/week of moderate to vigorous physical activity, spending less than 2 h/day in front of screens, and sleeping 8-10 h/day. Adolescents in the healthy class had a higher cardiorespiratory fitness level than those in the unhealthy class. Most adolescents were grouped in the healthy class and had higher cardiorespiratory fitness levels than those in the unhealthy class. These results suggest that families and professionals should work toward creating healthier lifestyles for adolescents by increasing opportunities to practice moderate to vigorous physical activity, reduce screen time, and favor healthy sleep to increase their cardiorespiratory fitness levels. What is Known: * Moderate to vigorous physical activity, screen time, and sleep duration are positively, negatively, and inconsistently associated with cardiorespiratory fitness, respectively, when analyzed separately. * Little is known about the clustering of 24-h movement behaviors and how they are associated with cardiorespiratory fitness levels in adolescents. What is New: * The 24-h movement behaviors clustered into almost opposite classes among adolescents (healthy and unhealthy classes). * Adolescents in the healthy class had greater cardiorespiratory fitness levels than those in the unhealthy class.",
        "Chronic medical conditions show substantial heterogeneity in their clinical features and progression. We develop the novel data-driven, network-based Trajectory Profile Clustering (TPC) algorithm for 1) identification of disease subtypes and 2) early prediction of subtype/disease progression patterns. TPC is an easily generalizable method that identifies subtypes by clustering patients with similar disease trajectory profiles, based not only on Parkinson's Disease (PD) variable severity, but also on their complex patterns of evolution. TPC is derived from bipartite networks that connect patients to disease variables. Applying our TPC algorithm to a PD clinical dataset, we identify 3 distinct subtypes/patient clusters, each with a characteristic progression profile. We show that TPC predicts the patient's disease subtype 4 years in advance with 72% accuracy for a longitudinal test cohort. Furthermore, we demonstrate that other types of data such as genetic data can be integrated seamlessly in the TPC algorithm. In summary, using PD as an example, we present an effective method for subtype identification in multidimensional longitudinal datasets, and early prediction of subtypes in individual patients.",
        "BACKGROUND: A large Midwestern state commissioned a virtual driving test (VDT) to assess driving skills preparedness before the on-road examination (ORE). Since July 2017, a pilot deployment of the VDT in state licensing centers (VDT pilot) has collected both VDT and ORE data from new license applicants with the aim of creating a scoring algorithm that could predict those who were underprepared. OBJECTIVE: Leveraging data collected from the VDT pilot, this study aimed to develop and conduct an initial evaluation of a novel machine learning (ML)-based classifier using limited domain knowledge and minimal feature engineering to reliably predict applicant pass/fail on the ORE. Such methods, if proven useful, could be applicable to the classification of other time series data collected within medical and other settings. METHODS: We analyzed an initial dataset that comprised 4308 drivers who completed both the VDT and the ORE, in which 1096 (25.4%) drivers went on to fail the ORE. We studied 2 different approaches to constructing feature sets to use as input to ML algorithms: the standard method of reducing the time series data to a set of manually defined variables that summarize driving behavior and a novel approach using time series clustering. We then fed these representations into different ML algorithms to compare their ability to predict a driver's ORE outcome (pass/fail). RESULTS: The new method using time series clustering performed similarly compared with the standard method in terms of overall accuracy for predicting pass or fail outcome (76.1% vs 76.2%) and area under the curve (0.656 vs 0.682). However, the time series clustering slightly outperformed the standard method in differentially predicting failure on the ORE. The novel clustering method yielded a risk ratio for failure of 3.07 (95% CI 2.75-3.43), whereas the standard variables method yielded a risk ratio for failure of 2.68 (95% CI 2.41-2.99). In addition, the time series clustering method with logistic regression produced the lowest ratio of false alarms (those who were predicted to fail but went on to pass the ORE; 27.2%). CONCLUSIONS: Our results provide initial evidence that the clustering method is useful for feature construction in classification tasks involving time series data when resources are limited to create multiple, domain-relevant variables.",
        "Biclustering is an important exploratory analysis tool that simultaneously clusters rows (e.g., samples) and columns (e.g., variables) of a data matrix. Checkerboard-like biclusters reveal intrinsic associations between rows and columns. However, most existing methods rely on Gaussian assumptions and only apply to matrix data. In practice, non-Gaussian and/or multi-way tensor data are frequently encountered. A new CO-clustering method via Regularized Alternating Least Squares (CORALS) is proposed, which generalizes biclustering to non-Gaussian data and multi-way tensor arrays. Non-Gaussian data are modeled with single-parameter exponential family distributions and co-clusters are identified in the natural parameter space via sparse CANDECOMP/PARAFAC tensor decomposition. A regularized alternating (iteratively reweighted) least squares algorithm is devised for model fitting and a deflation procedure is exploited to automatically determine the number of co-clusters. Comprehensive simulation studies and three real data examples demonstrate the efficacy of the proposed method. The data and code are publicly available.",
        "AIMS: This study aimed to explore nursing and midwifery students' evaluation of the clinical learning environment and mentoring and to identify distinct student profiles relating to their perceptions. DESIGN: This study employed a cross-sectional design. SETTINGS: The study population included nursing and midwifery students in a university hospital in Finland. PARTICIPANTS: All nursing and midwifery students who completed their clinical placement were invited to take part in the study in the academic year 2017-2018. METHODS: The data (N = 2,609) were gathered through an online survey using the Clinical Learning Environment, Supervision and Nurse Teacher scale. The data were analysed using a K-mean cluster algorithm to identify nursing and midwifery students' profiles. RESULTS: The findings from this study indicate four distinct profiles (A, B, C, & D) of nursing and midwifery students in relation to the clinical learning environment and mentoring. Profile A (N = 1,352) students evaluated their clinical learning environment and mentoring to the highest level (mean varied from 9.44-8.38); and Profile D (N = 151)- to the lowest (mean varied from 5.93-4.00). CONCLUSION: The findings highlight that nursing and midwifery students evaluate their clinical learning environment and mentoring more highly when: they have a named mentor, student and mentor discuss learning goals, there is a final assessment in clinical learning, the mentor's guidance skills support student learning, the clinical learning supports the student's professional development and pre-clinical teaching in an educational institution supports learning in the clinical placement. IMPACT: Clinical learning plays an important role in nurse and midwifery education. Mentoring of clinical practice was shown to have a great influence on students' perceptions of their success in clinical learning. We suggest that clinical practice should be strengthened by the building of collaboration between nursing teachers and registered nurses.",
        "Single-cell RNA sequencing (scRNA-seq) is a recent technology that enables fine-grained discovery of cellular subtypes and specific cell states. Analysis of scRNA-seq data routinely involves machine learning methods, such as feature learning, clustering, and classification, to assist in uncovering novel information from scRNA-seq data. However, current methods are not well suited to deal with the substantial amount of noise that is created by the experiments or the variation that occurs due to differences in the cells of the same type. To address this, we developed a new hybrid approach, deep unsupervised single-cell clustering (DUSC), which integrates feature generation based on a deep learning architecture by using a new technique to estimate the number of latent features, with a model-based clustering algorithm, to find a compact and informative representation of the single-cell transcriptomic data generating robust clusters. We also include a technique to estimate an efficient number of latent features in the deep learning model. Our method outperforms both classical and state-of-the-art feature learning and clustering methods, approaching the accuracy of supervised learning. We applied DUSC to a single-cell transcriptomics data set obtained from a triple-negative breast cancer tumor to identify potential cancer subclones accentuated by copy-number variation and investigate the role of clonal heterogeneity. Our method is freely available to the community and will hopefully facilitate our understanding of the cellular atlas of living organisms as well as provide the means to improve patient diagnostics and treatment.",
        "The text clustering is considered as one of the most effective text document analysis methods, which is applied to cluster documents as a consequence of the expanded big data and online information. Based on the review of the related work of the text clustering algorithms, these algorithms achieved reasonable clustering results for some datasets, while they failed on a wide variety of benchmark datasets. Furthermore, the performance of these algorithms was not robust due to the inefficient balance between the exploitation and exploration capabilities of the clustering algorithm. Accordingly, this research proposes a Memetic Differential Evolution algorithm (MDETC) to solve the text clustering problem, which aims to address the effect of the hybridization between the differential evolution (DE) mutation strategy with the memetic algorithm (MA). This hybridization intends to enhance the quality of text clustering and improve the exploitation and exploration capabilities of the algorithm. Our experimental results based on six standard text clustering benchmark datasets (i.e. the Laboratory of Computational Intelligence (LABIC)) have shown that the MDETC algorithm outperformed other compared clustering algorithms based on AUC metric, F-measure, and the statistical analysis. Furthermore, the MDETC is compared with the state of art text clustering algorithms and obtained almost the best results for the standard benchmark datasets.",
        "There is a worldwide effort of the research community to explore the medical, economic and sociologic impact of the COVID-19 pandemic. Many different disciplines try to find solutions and drive strategies to a great variety of different very crucial problems. The present study presents a novel analysis which results to clustering countries with respect to active cases, active cases per population and active cases per population and per area based on Johns Hopkins epidemiological data. The presented cluster results could be useful to a variety of different policy makers, such as physicians and managers of the health sector, economy/finance experts, politicians and even to sociologists. In addition, our work suggests a new specially designed clustering algorithm adapted to the request for comparison of the various COVID time-series of different countries.",
        "The coal pulverizing system is an important auxiliary system in thermal power generation systems. The working condition of a coal pulverizing system may directly affect the safety and economy of power generation. Prognostics and health management is an effective approach to ensure the reliability of coal pulverizing systems. As the coal pulverizing system is a typical dynamic and nonlinear high-dimensional system, it is difficult to construct accurate mathematical models used for anomaly detection. In this paper, a novel data-driven integrated framework for anomaly detection of the coal pulverizing system is proposed. A neural network model based on gated recurrent unit (GRU) networks, a type of recurrent neural network (RNN), is constructed to describe the temporal characteristics of high-dimensional data and predict the system condition value. Then, aiming at the prediction error, a novel unsupervised clustering algorithm for anomaly detection is proposed. The proposed framework is validated by a real case study from an industrial coal pulverizing system. The results show that the proposed framework can detect the anomaly successfully.",
        "Data Streams create new challenges for fuzzy clustering algorithms, specifically Interval Type-2 Fuzzy C-Means (IT2FCM). One problem associated with IT2FCM is that it tends to be sensitive to initialization conditions and therefore, fails to return global optima. This problem has been addressed by optimizing IT2FCM using Ant Colony Optimization approach. However, IT2FCM-ACO obtain clusters for the whole dataset which is not suitable for clustering large streaming datasets that may be coming continuously and evolves with time. Thus, the clusters generated will also evolve with time. Additionally, the incoming data may not be available in memory all at once because of its size. Therefore, to encounter the challenges of a large data stream environment we propose improvising IT2FCM-ACO to generate clusters incrementally. The proposed algorithm produces clusters by determining appropriate cluster centers on a certain percentage of available datasets and then the obtained cluster centroids are combined with new incoming data points to generate another set of cluster centers. The process continues until all the data are scanned. The previous data points are released from memory which reduces time and space complexity. Thus, the proposed incremental method produces data partitions comparable to IT2FCM-ACO. The performance of the proposed method is evaluated on large real-life datasets. The results obtained from several fuzzy cluster validity index measures show the enhanced performance of the proposed method over other clustering algorithms. The proposed algorithm also improves upon the run time and produces excellent speed-ups for all datasets.",
        "BACKGROUND: Maintaining a healthy weight can reduce the risk of developing many diseases, including type 2 diabetes, hypertension, and certain types of cancers. Online social media platforms are popular among people seeking social support regarding weight loss and sharing their weight loss experiences, which provides opportunities for learning about weight loss behaviors. OBJECTIVE: This study aimed to investigate the extent to which the content posted by users in the r/loseit subreddit, an online community for discussing weight loss, and online interactions were associated with their weight loss in terms of the number of replies and votes that these users received. METHODS: All posts that were published before January 2018 in r/loseit were collected. We focused on users who revealed their start weight, current weight, and goal weight and were active in this online community for at least 30 days. A topic modeling technique and a hierarchical clustering algorithm were used to obtain both global topics and local word semantic clusters. Finally, we used a regression model to learn the association between weight loss and topics, word semantic clusters, and online interactions. RESULTS: Our data comprised 477,904 posts that were published by 7660 users within a span of 7 years. We identified 25 topics, including food and drinks, calories, exercises, family members and friends, and communication. Our results showed that the start weight (beta=.823; P<.001), active days (beta=.017; P=.009), and median number of votes (beta=.263; P=.02), mentions of exercises (beta=.145; P<.001), and nutrition (beta=.120; P<.001) were associated with higher weight loss. Users who lost more weight might be motivated by the negative emotions (beta=-.098; P<.001) that they experienced before starting the journey of weight loss. In contrast, users who mentioned vacations (beta=-.108; P=.005) and payments (beta=-.112; P=.001) tended to experience relatively less weight loss. Mentions of family members (beta=-.031; P=.03) and employment status (beta=-.041; P=.03) were associated with less weight loss as well. CONCLUSIONS: Our study showed that both online interactions and offline activities were associated with weight loss, suggesting that future interventions based on existing online platforms should focus on both aspects. Our findings suggest that online personal health data can be used to learn about health-related behaviors effectively.",
        "Kawasaki disease (KD) is the leading cause of acquired heart disease in children. Its prompt treatment can effectively lower the risk of severe complications, such as coronary aneurysms. However, accurately diagnosing KD at its early stage is impracticable given its unknown pathogenesis and lack of pathognomonic features. In this study, we investigated data-driven approaches by using a cohort of 10,367 patients extracted from electronic health records for early KD assessment. The incompleteness of clinical data presents group-based missing patterns associated with different clinical assessment measures. To address this problem, we developed a method integrating feature clustering to enable matrix-based representation and convolutional neural networks (CNN) for feature extraction and fusion to explicitly exploit the multi-source data structure. Integrating missing data imputation methods with the proposed method demonstrated superior accuracy (an AUC of 0.97) compared with a number of benchmark methods. The present method shows potential to improve clinical data mining. Our study highlighted the feasible utilization of matrix-based feature representation and CNN-based feature extraction for incomplete clinical data mining to support medical decision-making.",
        "A methodology for evolving fuzzy Kalman filter identification, is proposed in this paper. The mathematical formulation contemplates the following aspects: for initial estimation, an offline GK clustering algorithm and an offline fuzzy version of OKID algorithm are used to estimate antecedent and consequent parameters, respectively. From each new sample of input-output experimental data from dynamical system, the evolving eTS algorithm and an evolving fuzzy version of OKID algorithm are used to estimate the antecedent and consequent parameters of the evolving fuzzy Kalman filter, respectively. Computational and experimental results considering the estimation of states and outputs of a nonlinear dynamic system and a 2DoF helicopter, respectively, show the efficiency and applicability of the proposed methodology.",
        "BACKGROUND AND OBJECTIVE: Nowadays, the number of pathologies related to food are multiplied. Mycotoxins are one of the most severe food contaminants that cause serious effects on the human health. Therefore, it is necessary to develop an assessment tool for evaluating their impact on the immune response. Recently, a new investigational method using human dendritic cells was endorsed by biologists. Nevertheless, analysis of the morphological features and the behavior of these cells remains merely visual. In addition, this manual analysis is difficult and time-consuming. Here, we focus mainly on automating the evaluation process by using advanced image processing technology. METHODS: An automatic segmentation approach of microscopic dendritic cell images is developed to provide a fast and objective evaluation. First, a combination of K-means clustering and mathematical morphology is used to detect dendritic cells. Second, a region-based Chan-Vese active contour model is used to segment the detected cells more precisely. Finally, dendritic cells are extracted by a filtering based on eccentricity measure. RESULTS: The proposed scheme is tested on an actual dataset containing 421 microscopic dendritic cell images. The experimental results show high conformity between the results of the proposed scheme and ground-truth elaborated by biological expert. Moreover, a comparative study with other state-of-art segmentation schemes demonstrates the efficiency of the proposed method. It gives the highest average accuracy rate (99.42 %) compared to recent studied approaches. CONCLUSIONS: The proposed image segmentation method for morphological analysis of dendrite inhibition can consistently be used as an assessment tool for biologists to facilitate the evaluation of serious health impacts of mycotoxins.",
        "BACKGROUND: Diffusion MRI is the preferred non-invasive in vivo modality for the study of brain white matter connections. Tractography datasets contain 3D streamlines that can be analyzed to study the main brain white matter tracts. Fiber clustering methods have been used to automatically group similar fibers into clusters. However, due to inter-subject variability and artifacts, the resulting clusters are difficult to process for finding common connections across subjects, specially for superficial white matter. METHODS: We present an automatic method for labeling of short association bundles on a group of subjects. The method is based on an intra-subject fiber clustering that generates compact fiber clusters. Posteriorly, the clusters are labeled based on the cortical connectivity of the fibers, taking as reference the Desikan-Killiany atlas, and named according to their relative position along one axis. Finally, two different strategies were applied and compared for the labeling of inter-subject bundles: a matching with the Hungarian algorithm, and a well-known fiber clustering algorithm, called QuickBundles. RESULTS: Individual labeling was executed over four subjects, with an execution time of 3.6 min. An inspection of individual labeling based on a distance measure showed good correspondence among the four tested subjects. Two inter-subject labeling were successfully implemented and applied to 20 subjects and compared using a set of distance thresholds, ranging from a conservative value of 10 mm to a moderate value of 21 mm. Hungarian algorithm led to a high correspondence, but low reproducibility for all the thresholds, with 96 s of execution time. QuickBundles led to better correspondence, reproducibility and short execution time of 9 s. Hence, the whole processing for the inter-subject labeling over 20 subjects takes 1.17 h. CONCLUSION: We implemented a method for the automatic labeling of short bundles in individuals, based on an intra-subject clustering and the connectivity of the clusters with the cortex. The labels provide useful information for the visualization and analysis of individual connections, which is very difficult without any additional information. Furthermore, we provide two fast inter-subject bundle labeling methods. The obtained clusters could be used for performing manual or automatic connectivity analysis in individuals or across subjects.",
        "In multitask networks, neighboring agents that belong to different clusters pursue different goals, and therefore arbitrary cooperation will lead to a degradation in estimation performance. In this paper, an adaptive clustering method is proposed for distributed estimation that enables agents to distinguish between subneighbors that belong to the same cluster and those that belong to a different cluster. This creates an appropriate degree of cooperation to improve parameter estimation accuracy, especially for the case where the prior information of a cluster is unknown. In contrast to the static and quantitative threshold that is imposed in traditional clustering methods, we devise a method for real-time clustering hypothesis detection, which is constructed through the use of a reliable adaptive clustering threshold as reference and the averaged element-wise distance between tasks as real-time clustering detection statistic. Meanwhile, we relax the clustering conditions to maintain maximum cooperation without sacrificing accuracy. Simulations are presented to compare the proposed algorithm and some traditional clustering strategies in both stationary and nonstationary environments. The effects of task difference on performance are also obtained to demonstrate the superiority of our proposed clustering strategy in terms of accuracy, robustness, and suitability.",
        "Evidence accumulation clustering (EAC) is an ensemble clustering algorithm that can cluster data for arbitrary shapes and numbers of clusters. Here, we present a variant of EAC in which we aimed to better cluster data with a large number of features, many of which may be uninformative. Our new method builds on the existing EAC algorithm by populating the clustering ensemble with clusterings based on combinations of fewer features than the original dataset at a time. Our method also calls for prewhitening the recombined data and weighting the influence of each individual clustering by an estimate of its informativeness. We provide code of an example implementation of the algorithm in Matlab and demonstrate its effectiveness compared to ordinary evidence accumulation clustering with synthetic data.*The clustering ensemble is made by clustering on subset combinations of features from the data*The recombined data may be prewhitened*Evidence accumulation can be improved by weighting the evidence with a goodness-of-clustering measure.",
        "The need to organize a large collection in a manner that facilitates human comprehension is crucial given the ever-increasing volumes of information. In this work, we present PDC (probabilistic distributional clustering), a novel algorithm that, given a document collection, computes disjoint term sets representing topics in the collection. The algorithm relies on probabilities of word co-occurrences to partition the set of terms appearing in the collection of documents into disjoint groups of related terms. In this work, we also present an environment to visualize the computed topics in the term space and retrieve the most related PubMed articles for each group of terms. We illustrate the algorithm by applying it to PubMed documents on the topic of suicide. Suicide is a major public health problem identified as the tenth leading cause of death in the US. In this application, our goal is to provide a global view of the mental health literature pertaining to the subject of suicide, and through this, to help create a rich environment of multifaceted data to guide health care researchers in their endeavor to better understand the breadth, depth and scope of the problem. We demonstrate the usefulness of the proposed algorithm by providing a web portal that allows mental health researchers to peruse the suicide-related literature in PubMed.",
        "In cognitive diagnostic assessment (CDA), clustering analysis is an efficient approach to classify examinees into attribute-homogeneous groups. Many researchers have proposed different methods, such as the nonparametric method with Hamming distance, K-means method, and hierarchical agglomerative cluster analysis, to achieve the classification goal. In this paper, according to their responses, we introduce a spectral clustering algorithm (SCA) to cluster examinees. Simulation studies are used to compare the classification accuracy of the SCA, K-means algorithm, G-DINA model and its related reduced cognitive diagnostic models. A real data analysis is also conducted to evaluate the feasibility of the SCA. Some research directions are discussed in the final section.",
        "Most multi-view clustering algorithms apply to data with complete instances and clusters in the views. Recently, multi-view clustering on data with partial instances has been studied. In this paper, we study the more general version of the problem, i.e., multi-view clustering on data with partial instances and clusters in the views. We propose a non-negative matrix factorization (NMF) based algorithm. For the special case with partial instances, it introduces an instance-view-indicator matrix to indicate whether an instance exists in a view. Then, it maps the instances representing the same object to the same vector, and maps the instances representing different objects to different vectors. For the general case with partial instances and clusters, it further introduces a cluster-view-indicator matrix to indicate whether a cluster exists in a view. In each view, it also maps the instances representing the same object to the same vector, but it further makes the elements of the vector 0 if the elements correspond to missing clusters. Then it minimizes the disagreements between the approximated indicator vectors of instances representing the same object. Experimental results show that the proposed algorithm performs well on data with partial instances and clusters, and outperforms existing algorithms on data with partial instances.",
        "Antibody V domain clustering is of paramount importance to a repertoire of immunology-related areas. Although several approaches have been proposed for antibody clustering, still no consensus has been reached. Numerous attempts use information from genes, protein sequences, 3D structures, and 3D surfaces in an effort to elucidate unknown action mechanisms directly related to their function and to either link them directly to diseases or drive the discovery of new medicines, such as antibody drug conjugates (ADC). Herein, we describe a new V domain antibody clustering method based on the comparison of the interaction sites between each antibody and its antigen. A more specific clustering analysis of the antibody's V domain was provided using deep learning and data mining techniques. The multidimensional information was extracted from the structural resolved antibodies when they were captured to interact with other proteins. The available 3D structures of protein antigen-antibody (Ag-Ab) interfaces contain information about how antibody V domains recognize antigens as well as about which amino acids are involved in the recognition. As such, the antibody surface holds information about antigens' folding that reside with the Ab-Ag interface residues and how they interact. In order to gain insight into the nature of such interactions, we propose a new simple philosophy to transform the conserved framework (fragment regions, complementarity-determining regions) of antibody V domain in a binary form using structural features of antibody-antigen interactions, toward identifying new antibody signatures in V domain binding activity. Finally, an advanced three-level hybrid classification scheme has been set for clustering antibodies in subgroups, which can combine the information from the protein sequences, the three-dimensional structures, and specific \"key patterns\" of recognized interactions. The clusters provide multilevel information about antibodies and antibody-antigen complexes.",
        "BACKGROUND: In the current era of personalized medicine, there is increasing interest in understanding the heterogeneity in disease populations. Cluster analysis is a method commonly used to identify subtypes in heterogeneous disease populations. The clinical data used in such applications are typically multimodal, which can make the application of traditional cluster analysis methods challenging. OBJECTIVE: This study aimed to review the research literature on the application of clustering multimodal clinical data to identify asthma subtypes. We assessed common problems and shortcomings in the application of cluster analysis methods in determining asthma subtypes, such that they can be brought to the attention of the research community and avoided in future studies. METHODS: We searched PubMed and Scopus bibliographic databases with terms related to cluster analysis and asthma to identify studies that applied dissimilarity-based cluster analysis methods. We recorded the analytic methods used in each study at each step of the cluster analysis process. RESULTS: Our literature search identified 63 studies that applied cluster analysis to multimodal clinical data to identify asthma subtypes. The features fed into the cluster algorithms were of a mixed type in 47 (75%) studies and continuous in 12 (19%), and the feature type was unclear in the remaining 4 (6%) studies. A total of 23 (37%) studies used hierarchical clustering with Ward linkage, and 22 (35%) studies used k-means clustering. Of these 45 studies, 39 had mixed-type features, but only 5 specified dissimilarity measures that could handle mixed-type features. A further 9 (14%) studies used a preclustering step to create small clusters to feed on a hierarchical method. The original sample sizes in these 9 studies ranged from 84 to 349. The remaining studies used hierarchical clustering with other linkages (n=3), medoid-based methods (n=3), spectral clustering (n=1), and multiple kernel k-means clustering (n=1), and in 1 study, the methods were unclear. Of 63 studies, 54 (86%) explained the methods used to determine the number of clusters, 24 (38%) studies tested the quality of their cluster solution, and 11 (17%) studies tested the stability of their solution. Reporting of the cluster analysis was generally poor in terms of the methods employed and their justification. CONCLUSIONS: This review highlights common issues in the application of cluster analysis to multimodal clinical data to identify asthma subtypes. Some of these issues were related to the multimodal nature of the data, but many were more general issues in the application of cluster analysis. Although cluster analysis may be a useful tool for investigating disease subtypes, we recommend that future studies carefully consider the implications of clustering multimodal data, the cluster analysis process itself, and the reporting of methods to facilitate replication and interpretation of findings.",
        "The diagnosis and treatment of epilepsy is a significant direction for both machine learning and brain science. This paper newly proposes a fast enhanced exemplar-based clustering (FEEC) method for incomplete EEG signal. The algorithm first compresses the potential exemplar list and reduces the pairwise similarity matrix. By processing the most complete data in the first stage, FEEC then extends the few incomplete data into the exemplar list. A new compressed similarity matrix will be constructed and the scale of this matrix is greatly reduced. Finally, FEEC optimizes the new target function by the enhanced alpha-expansion move method. On the other hand, due to the pairwise relationship, FEEC also improves the generalization of this algorithm. In contrast to other exemplar-based models, the performance of the proposed clustering algorithm is comprehensively verified by the experiments on two datasets.",
        "One of the major problems in cancer subtype discovery from multimodal omic data is that all the available modalities may not encode relevant and homogeneous information about the subtypes. Moreover, the high-dimensional nature of the modalities makes sample clustering computationally expensive. In this regard, a novel algorithm is proposed to extract a low-rank joint subspace of the integrated data matrix. The proposed algorithm first evaluates the quality of subtype information provided by each of the modalities, and then judiciously selects only relevant ones to construct the joint subspace. The problem of incrementally updating the singular value decomposition of a data matrix is formulated for the multimodal data framework. The analytical formulation enables efficient construction of the joint subspace of integrated data from low-rank subspaces of the individual modalities. The construction of joint subspace by the proposed method is shown to be computationally more efficient compared to performing the principal component analysis (PCA) on the integrated data matrix. Some new quantitative indices are introduced to measure theoretically the accuracy of subspace construction by the proposed approach with respect to the principal subspace extracted by the PCA. The efficacy of clustering on the joint subspace constructed by the proposed algorithm is established over existing integrative clustering approaches on several real-life multimodal cancer data sets.",
        "Standardization, data mining techniques, and comparison to normality are changing the landscape of multiparameter flow cytometry in clinical hematology. On the basis of these principles, a strategy was developed for measurable residual disease (MRD) assessment. Herein, suspicious cell clusters are first identified at diagnosis using a clustering algorithm. Subsequently, automated multidimensional spaces, named \"Clouds\", are created around these clusters on the basis of density calculations. This step identifies the immunophenotypic pattern of the suspicious cell clusters. Thereafter, using reference samples, the \"Abnormality Ratio\" (AR) of each Cloud is calculated, and major malignant Clouds are retained, known as \"Leukemic Clouds\" (L-Clouds). In follow-up samples, MRD is identified when more cells fall into a patient's L-Cloud compared to reference samples (AR concept). This workflow was applied on simulated data and real-life leukemia flow cytometry data. On simulated data, strong patient-dependent positive correlation (R(2) = 1) was observed between the AR and spiked-in leukemia cells. On real patient data, AR kinetics was in line with the clinical evolution for five out of six patients. In conclusion, we present a convenient flow cytometry data analysis approach for the follow-up of hematological malignancies. Further evaluation and validation on more patient samples and different flow cytometry panels is required before implementation in clinical practice.",
        "Dynamical interplays in emotions have been investigated using vector autoregressive (VAR) models, whose estimates can be used to cluster participants into unknown groups. The present study evaluated a clustering algorithm, the alternating least square (ALS) algorithm, for accuracy in predicting individual group membership. We systematically manipulated (a) the number of variables in a model, (b) the size of group differences in regression coefficients, and (c) the number of regression coefficients that vary across the groups (i.e., effective features). The ALS algorithm works reliably when there are at least 5 effective features with very large group differences in a 5-variable model; and 9 effective features with very large group differences in a 10-variable model. These findings suggest that the ALS algorithm is sensitive to group differences that are present only in several coefficients of a VAR model, but that the group differences have to be large. We also found that the ALS algorithm outperforms another clustering method, Gaussian mixture modeling. The ALS algorithm was further evaluated with unbalanced sample sizes between groups and with a greater number of groups in data (Study 2). A real data application was provided to illustrate how to interpret the detected group differences (Study 3).",
        "Muscle strength testing has long been an important assessment procedure in rehabilitation setups, though the subjectivity and standardization of this procedure has been widely debated. To address this issue, this study involves the use of Electromyogram (EMG) features that are intuitively related to muscle strength to classify Manual muscle testing (MMT) grades of '4 -', '4', '4 + ' and '5' of the Medical Research Council scale. MMT was performed on Tibialis anterior muscle of 50 healthy participants whose MMT grades and EMG were simultaneously acquired. Chi square goodness of fit and Spectrum Decomposition of Graph Laplacian (SPEC) feature selection algorithms are used in selecting five features, namely Integrated EMG, Root Mean Square EMG, Waveform Length, Wilsons' amplitude and Energy. Gaussian Mixture Model (GMM) approach is used for unsupervised clustering into one of the grades. Internal cluster evaluation resulted in Silhouette score of 0.76 and Davies Bouldin Index of 0.42 indicating good cluster separability. Agreement between the machine-based grade and manual grade has been quantified using Cohens' Kappa coefficient. A value of '0.44' has revealed a moderate agreement, with greater differences reported in grading '4' and '4 + ' strength levels. The comparative advantage of EMG based grading over the manual method has been proved. The suggested method can be extended for muscle strength testing of all muscles across different age groups to assist physicians in evaluating patient strength and plan appropriate strength conditioning exercises as a part of rehabilitative assessment.",
        "The technical advancements in big data have become popular and most desirable among users for storing, processing, and handling huge data sets. However, clustering using these big data sets has become a major challenge in big data analysis. The conventional clustering algorithms used scalable solutions for managing huge data sets. Thus, this study proposes a technique for big data clustering using the spark architecture. The proposed technique undergoes two steps for clustering the big data, involving feature selection and clustering, performed in the initial cluster nodes of spark architecture. At first, the initial cluster nodes read the big data from various distributed systems, and the optimal features are selected and placed in the feature vector based on the proposed moth-flame optimization-based bat (MFO-Bat) algorithm, which is designed by integrating MFO and Bat algorithms. Then, the selected features are fed to the final cluster nodes of spark, which uses the sparse-fuzzy C-means method for performing optimal clustering. The performance of proposed MFO-Bat outperformed other existing methods with a maximal classification accuracy of 95.806%, Dice coefficient of 99.181%, and Jaccard coefficient of 98.376%, respectively.",
        "BACKGROUND: Cancer typically exhibits genotypic and phenotypic heterogeneity, which can have prognostic significance and influence therapy response. Computed Tomography (CT)-based radiomic approaches calculate quantitative features of tumour heterogeneity at a mesoscopic level, regardless of macroscopic areas of hypo-dense (i.e., cystic/necrotic), hyper-dense (i.e., calcified), or intermediately dense (i.e., soft tissue) portions. METHOD: With the goal of achieving the automated sub-segmentation of these three tissue types, we present here a two-stage computational framework based on unsupervised Fuzzy C-Means Clustering (FCM) techniques. No existing approach has specifically addressed this task so far. Our tissue-specific image sub-segmentation was tested on ovarian cancer (pelvic/ovarian and omental disease) and renal cell carcinoma CT datasets using both overlap-based and distance-based metrics for evaluation. RESULTS: On all tested sub-segmentation tasks, our two-stage segmentation approach outperformed conventional segmentation techniques: fixed multi-thresholding, the Otsu method, and automatic cluster number selection heuristics for the K-means clustering algorithm. In addition, experiments showed that the integration of the spatial information into the FCM algorithm generally achieves more accurate segmentation results, whilst the kernelised FCM versions are not beneficial. The best spatial FCM configuration achieved average Dice similarity coefficient values starting from 81.94+/-4.76 and 83.43+/-3.81 for hyper-dense and hypo-dense components, respectively, for the investigated sub-segmentation tasks. CONCLUSIONS: The proposed intelligent framework could be readily integrated into clinical research environments and provides robust tools for future radiomic biomarker validation.",
        "BACKGROUND: In psoriasis skin disease, psoriatic cells develop rapidly than the normal healthy cells. This speedy growth causes accumulation of dead skin cells on the skin's surface, resulting in thick patches of red, dry, and itchy skin. This patches or psoriatic skin legions may exhibit similar characteristics as healthy skin, which makes lesion detection more challenging. However, for accurate disease diagnosis and severity detection, lesion segmentation has prime importance. In that context, our group had previously performed psoriasis lesion segmentation using the conventional clustering algorithm. However, it suffers from the constraint of falling into the local sub-optimal centroids of the clusters. OBJECTIVE: The main objective of this paper is to implement an optimal lesion segmentation technique with aims at global convergence by reducing the probability of trapping into the local optima. This has been achieved by integrating swarm intelligence based algorithms with conventional K-means and Fuzzy C-means (FCMs) clustering algorithms. METHODOLOGY: There are a total of eight different suitable combinations of conventional clustering (i.e., K-means and Fuzzy C-means (FCMs)) and four swarm intelligence (SI) techniques (i.e., seeker optimization (SO), artificial bee colony (ABC), ant colony optimization (ACO) and particle swarm optimization (PSO)) have been implemented in this study. The experiments are performed on the dataset of 780 psoriasis images from 74 patients collected at Psoriasis Clinic and Research Centre, Psoriatreat, Pune, Maharashtra, India. In this study, we are employing swarm intelligence optimization techniques in combination with the conventional clustering algorithms to increase the probability of convergence to the optimal global solution and hence improved clustering and detection. RESULTS: The performance has been quantified in terms of four indices, namely accuracy (A), sensitivity (SN), specificity (SP), and Jaccard index (JI). Among the eight different combinations of clustering and optimization techniques considered in this study, FCM + SO outperformed with mean JI = 0.83, mean A = 90.89, mean SN = 92.84, and mean SP = 88.27. FCM + SO found statistical significant than other approaches with 96.67 % of the reliability index. CONCLUSION: The results obtained reflect the superiority of the proposed techniques over conventional clustering techniques. Hence our research development will lead to an objective analysis for automatic, accurate, and quick diagnosis of psoriasis.",
        "BACKGROUND: Considering the increasing volume of text document information on Internet pages, dealing with such a tremendous amount of knowledge becomes totally complex due to its large size. Text clustering is a common optimization problem used to manage a large amount of text information into a subset of comparable and coherent clusters. AIMS: This paper presents a novel local clustering technique, namely, beta-hill climbing, to solve the problem of the text document clustering through modeling the beta-hill climbing technique for partitioning the similar documents into the same cluster. METHODS: The beta parameter is the primary innovation in beta-hill climbing technique. It has been introduced in order to perform a balance between local and global search. Local search methods are successfully applied to solve the problem of the text document clustering such as; k-medoid and kmean techniques. RESULTS: Experiments were conducted on eight benchmark standard text datasets with different characteristics taken from the Laboratory of Computational Intelligence (LABIC). The results proved that the proposed beta-hill climbing achieved better results in comparison with the original hill climbing technique in solving the text clustering problem. CONCLUSION: The performance of the text clustering is useful by adding the beta operator to the hill climbing.",
        "Multiple kernel learning (MKL) is generally recognized to perform better than single kernel learning (SKL) in handling nonlinear clustering problem, largely thanks to MKL avoids selecting and tuning predefined kernel. By integrating the self-expression learning framework, the graph-based MKL subspace clustering has recently attracted considerable attention. However, the graph structure of data in kernel space is largely ignored by previous MKL methods, which is a key concept of affinity graph construction for spectral clustering purposes. In order to address this problem, a novel MKL method is proposed in this article, namely, structure-preserving multiple kernel clustering (SPMKC). Specifically, SPMKC proposes a new kernel affine weight strategy to learn an optimal consensus kernel from a predefined kernel pool, which can assign a suitable weight for each base kernel automatically. Furthermore, SPMKC proposes a kernel group self-expressiveness term and a kernel adaptive local structure learning term to preserve the global and local structure of the input data in kernel space, respectively, rather than the original space. In addition, an efficient algorithm is proposed to solve the resulting unified objective function, which iteratively updates the consensus kernel and the affinity graph so that collaboratively promoting each of them to reach the optimum condition. Experiments on both image and text clustering demonstrate that SPMKC outperforms the state-of-the-art MKL clustering methods in terms of clustering performance and computational cost.",
        "Medical data have the characteristics of particularity and complexity. Big data clustering plays a significant role in the area of medicine. The traditional clustering algorithms are easily falling into local extreme value. It will generate clustering deviation, and the clustering effect is poor. Therefore, we propose a new medical big data clustering algorithm based on the modified immune evolutionary method under cloud computing environment to overcome the above disadvantages in this paper. Firstly, we analyze the big data structure model under cloud computing environment. Secondly, we give the detailed modified immune evolutionary method to cluster medical data including encoding, constructing fitness function, and selecting genetic operators. Finally, the experiments show that this new approach can improve the accuracy of data classification, reduce the error rate, and improve the performance of data mining and feature extraction for medical data clustering.",
        "Currently, many applications have emerged from the implementation of software development and hardware use, known as the Internet of things. One of the most important application areas of this type of technology is in health care. Various applications arise daily in order to improve the quality of life and to promote an improvement in the treatments of patients at home that suffer from different pathologies. That is why there has emerged a line of work of great interest, focused on the study and analysis of daily life activities, on the use of different data analysis techniques to identify and to help manage this type of patient. This article shows the result of the systematic review of the literature on the use of the Clustering method, which is one of the most used techniques in the analysis of unsupervised data applied to activities of daily living, as well as the description of variables of high importance as a year of publication, type of article, most used algorithms, types of dataset used, and metrics implemented. These data will allow the reader to locate the recent results of the application of this technique to a particular area of knowledge.",
        "An important underlying assumption that guides the success of the existing multiview learning algorithms is the full observation of the multiview data. However, such rigorous precondition clearly violates the common-sense knowledge in practical applications, where in most cases, only incomplete fractions of the multiview data are given. The presence of the incomplete settings generally disables the conventional multiview clustering methods. In this article, we propose a simple but effective incomplete multiview clustering (IMC) framework, which simultaneously considers the local geometric information and the unbalanced discriminating powers of these incomplete multiview observations. Specifically, a novel graph-regularized matrix factorization model, on the one hand, is developed to preserve the local geometric similarities of the learned common representations from different views. On the other hand, the semantic consistency constraint is introduced to stimulate these view-specific representations toward a unified discriminative representation. Moreover, the importance of different views is adaptively determined to reduce the negative influence of the unbalanced incomplete views. Furthermore, an efficient learning algorithm is proposed to solve the resulting optimization problem. Extensive experimental results performed on several incomplete multiview datasets demonstrate that the proposed method can achieve superior clustering performance in comparison with some state-of-the-art multiview learning methods.",
        "Single-cell RNA sequencing (scRNA-seq) can characterize cell types and states through unsupervised clustering, but the ever increasing number of cells and batch effect impose computational challenges. We present DESC, an unsupervised deep embedding algorithm that clusters scRNA-seq data by iteratively optimizing a clustering objective function. Through iterative self-learning, DESC gradually removes batch effects, as long as technical differences across batches are smaller than true biological variations. As a soft clustering algorithm, cluster assignment probabilities from DESC are biologically interpretable and can reveal both discrete and pseudotemporal structure of cells. Comprehensive evaluations show that DESC offers a proper balance of clustering accuracy and stability, has a small footprint on memory, does not explicitly require batch information for batch effect removal, and can utilize GPU when available. As the scale of single-cell studies continues to grow, we believe DESC will offer a valuable tool for biomedical researchers to disentangle complex cellular heterogeneity.",
        "In this paper, we introduce a neural network framework for semi-supervised clustering with pairwise (must-link or cannot-link) constraints. In contrast to existing approaches, we decompose semi-supervised clustering into two simpler classification tasks: the first stage uses a pair of Siamese neural networks to label the unlabeled pairs of points as must-link or cannot-link; the second stage uses the fully pairwise-labeled dataset produced by the first stage in a supervised neural-network-based clustering method. The proposed approach is motivated by the observation that binary classification (such as assigning pairwise relations) is usually easier than multi-class clustering with partial supervision. On the other hand, being classification-based, our method solves only well-defined classification problems, rather than less well specified clustering tasks. Extensive experiments on various datasets demonstrate the high performance of the proposed method.",
        "Multiview data processing has attracted sustained attention as it can provide more information for clustering. To integrate this information, one often utilizes the non-negative matrix factorization (NMF) scheme which can reduce the data from different views into the subspace with the same dimension. Motivated by the clustering performance being affected by the distribution of the data in the learned subspace, a tri-factorization-based NMF model with an embedding matrix is proposed in this article. This model tends to generate decompositions with uniform distribution, such that the learned representations are more discriminative. As a result, the obtained consensus matrix can be a better representative of the multiview data in the subspace, leading to higher clustering performance. Also, a new lemma is proposed to provide the formulas about the partial derivation of the trace function with respect to an inner matrix, together with its theoretical proof. Based on this lemma, a gradient-based algorithm is developed to solve the proposed model, and its convergence and computational complexity are analyzed. Experiments on six real-world datasets are performed to show the advantages of the proposed algorithm, with comparison to the existing baseline methods.",
        "Wireless Sensor Networks (WSNs) are key elements of Internet of Things (IoT) networks which provide sensing and wireless connectivity. Disaster management in smart cities is classified as a safety-critical application. Thus, it is important to ensure system availability by increasing the lifetime of WSNs. Clustering is one of the routing techniques that benefits energy efficiency in WSNs. This paper provides an evolutionary clustering and routing method which is capable of managing the energy consumption of nodes while considering the characteristics of a disaster area. The proposed method consists of two phases. First, we present a model with improved hybrid Particle Swarm Optimization (PSO) and Harmony Search Algorithm (HSA) for cluster head (CH) selection. Second, we design a PSO-based multi-hop routing system with enhanced tree encoding and a modified data packet format. The simulation results for disaster scenarios prove the efficiency of the proposed method in comparison with the state-of-the-art approaches in terms of the overall residual energy, number of live nodes, network coverage, and the packet delivery ratio.",
        "The accurate and reproducible detection and description of thermodynamic states in computational data is a nontrivial problem, particularly when the number of states is unknown a priori and for large, flexible chemical systems and complexes. To this end, we report a novel clustering protocol that combines high-resolution structural representation, brute-force repeat clustering, and optimization of clustering statistics to reproducibly identify the number of clusters present in a data set (k) for simulated ensembles of butyrylcholinesterase in complex with two previously studied organophosphate inhibitors. Each structure within our simulated ensembles was depicted as a high-dimensionality vector with components defined by specific protein-inhibitor contacts at the chemical group level and the magnitudes of these components defined by their respective extents of pair-wise atomic contact, thus allowing for algorithmic differentiation between varying degrees of interaction. These surface-weighted interaction fingerprints were tabulated for each of over 1 million structures from more than 100 mus of all-atom molecular dynamics simulation per complex and used as the input for repetitive k-means clustering. Minimization of cluster population variance and range afforded accurate and reproducible identification of k, thereby allowing for the characterization of discrete binding modes from molecular simulation data in the form of contact tables that concisely encapsulate the observed intermolecular contact motifs. While the protocol presented herein to determine k and achieve non-heuristic clustering is demonstrated on data from massive atomistic simulation, our approach is generalizable to other data types and clustering algorithms, and is tractable with limited computational resources.",
        "The Fundamental Clustering Problems Suite (FCPS) offers a variety of clustering challenges that any algorithm should be able to handle given real-world data. The FCPS consists of datasets with known a priori classifications that are to be reproduced by the algorithm. The datasets are intentionally created to be visualized in two or three dimensions under the hypothesis that objects can be grouped unambiguously by the human eye. Each dataset represents a certain problem that can be solved by known clustering algorithms with varying success. In the R package \"Fundamental Clustering Problems Suite\" on CRAN, user-defined sample sizes can be drawn for the FCPS. Additionally, the distances of two high-dimensional datasets called Leukemia and Tetragonula are provided here. This collection is useful for investigating the shortcomings of clustering algorithms and the limitations of dimensionality reduction methods in the case of three-dimensional or higher datasets. This article is a simultaneous co-submission with Swarm Intelligence for Self-Organized Clustering [1].",
        "Specialized processing in the brain is performed by multiple groups of brain regions organized as functional modules. Although, in vivo studies of brain functional modules involve multiple functional Magnetic Resonance Imaging (fMRI) scans, the methods used to derive functional modules from functional networks of the brain ignore individual differences in the functional architecture and use incomplete functional connectivity information. To correct this, we propose an Iterative Consensus Spectral Clustering (ICSC) algorithm that detects the most representative modules from individual dense weighted connectivity matrices derived from multiple scans. The ICSC algorithm derives group-level modules from modules of multiple individuals by iteratively minimizing the consensus-cost between the two. We demonstrate that the ICSC algorithm can be used to derive biologically plausible group-level (for multiple subjects) and subject-level (for multiple subject scans) brain modules, using resting-state fMRI scans of 589 subjects from the Human Connectome Project. We employed a multipronged strategy to show the validity of the modularizations obtained from the ICSC algorithm. We show a heterogeneous variability in the modular structure across subjects where modules involved in visual and motor processing were highly stable across subjects. Conversely, we found a lower variability across scans of the same subject. The performance of our algorithm was compared with existing functional brain modularization methods and we show that our method detects group-level modules that are more representative of the modules of multiple individuals. Finally, the experiments on synthetic images quantitatively demonstrate that the ICSC algorithm detects group-level and subject-level modules accurately under varied conditions. Therefore, besides identifying functional modules for a population of subjects, the proposed method can be used for applications in personalized neuroscience. The ICSC implementation is available at https://github.com/SCSE-Biomedical-Computing-Group/ICSC.",
        "This paper presents a new model-based generalized functional clustering method for discrete longitudinal data, such as multivariate binomial and Poisson distributed data. For this purpose, we propose a multivariate functional principal component analysis (MFPCA)-based clustering procedure for a latent multivariate Gaussian process instead of the original functional data directly. The main contribution of this study is two-fold: modeling of discrete longitudinal data with the latent multivariate Gaussian process and developing of a clustering algorithm based on MFPCA coupled with the latent multivariate Gaussian process. Numerical experiments, including real data analysis and a simulation study, demonstrate the promising empirical properties of the proposed approach.",
        "Methods to deconvolve single-cell RNA-sequencing (scRNA-seq) data are necessary for samples containing a mixture of genotypes, whether they are natural or experimentally combined. Multiplexing across donors is a popular experimental design that can avoid batch effects, reduce costs and improve doublet detection. By using variants detected in scRNA-seq reads, it is possible to assign cells to their donor of origin and identify cross-genotype doublets that may have highly similar transcriptional profiles, precluding detection by transcriptional profile. More subtle cross-genotype variant contamination can be used to estimate the amount of ambient RNA. Ambient RNA is caused by cell lysis before droplet partitioning and is an important confounder of scRNA-seq analysis. Here we develop souporcell, a method to cluster cells using the genetic variants detected within the scRNA-seq reads. We show that it achieves high accuracy on genotype clustering, doublet detection and ambient RNA estimation, as demonstrated across a range of challenging scenarios.",
        "We investigated the impact of nutrient intake on hydration biomarkers in cyclists before and after a 161 km ride, including one hour after a 650 mL water bolus consumed post-ride. To control for multicollinearity, we chose a clustering-based, machine learning statistical approach. Five hydration biomarkers (urine color, urine specific gravity, plasma osmolality, plasma copeptin, and body mass change) were configured as raw- and percent change. Linear regressions were used to test for associations between hydration markers and eight predictor terms derived from 19 nutrients merged into a reduced-dimensionality dataset through serial k-means clustering. Most predictor groups showed significant association with at least one hydration biomarker: 1) Glycemic Load + Carbohydrates + Sodium, 2) Protein + Fat + Zinc, 3) Magnesium + Calcium, 4) Pinitol, 5) Caffeine, 6) Fiber + Betaine, and 7) Water; potassium + three polyols, and mannitol + sorbitol showed no significant associations with any hydration biomarker. All five hydration biomarkers were associated with at least one nutrient predictor in at least one configuration. We conclude that in a real-life scenario, some nutrients may serve as mediators of body water, and urine-specific hydration biomarkers may be more responsive to nutrient intake than measures derived from plasma or body mass.",
        "Single-cell RNA sequencing technologies have enabled us to study tissue heterogeneity at cellular resolution. Fast-developing sequencing platforms like droplet-based sequencing make it feasible to parallel process thousands of single cells effectively. Although a unique molecular identifier (UMI) can remove bias from amplification noise to a certain extent, clustering for such sparse and high-dimensional large-scale discrete data remains intractable and challenging. Most existing deep learning-based clustering methods utilize the mean square error or negative binomial distribution with or without zero inflation to denoise single-cell UMI count data, which may underfit or overfit the gene expression profiles. In addition, neglecting the molecule sampling mechanism and extracting representation by simple linear dimension reduction with a hard clustering algorithm may distort data structure and lead to spurious analytical results. In this paper, we combined the deep autoencoder technique with statistical modeling and developed a novel and effective clustering method, scDMFK, for single-cell transcriptome UMI count data. ScDMFK utilizes multinomial distribution to characterize data structure and draw support from neural network to facilitate model parameter estimation. In the learned low-dimensional latent space, we proposed an adaptive fuzzy k-means algorithm with entropy regularization to perform soft clustering. Various simulation scenarios and the analysis of 10 real datasets have shown that scDMFK outperforms other state-of-the-art methods with respect to data modeling and clustering algorithms. Besides, scDMFK has excellent scalability for large-scale single-cell datasets.",
        "For a suspected forgery that involves the falsification of a document or its contents, the investigator will primarily analyze the document's paper and ink in order to establish the authenticity of the subject under investigation. As a non-destructive and contactless technique, Hyperspectral Imaging (HSI) is gaining popularity in the field of forensic document analysis. HSI returns more information compared to conventional three channel imaging systems due to the vast number of narrowband images recorded across the electromagnetic spectrum. As a result, HSI can provide better classification results. In this publication, we present results of an approach known as the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm, which we have applied to HSI paper data analysis. Even though t-SNE has been widely accepted as a method for dimensionality reduction and visualization of high dimensional data, its usefulness has not yet been evaluated for the classification of paper data. In this research, we present a hyperspectral dataset of paper samples, and evaluate the clustering quality of the proposed method both visually and quantitatively. The t-SNE algorithm shows exceptional discrimination power when compared to traditional PCA with k-means clustering, in both visual and quantitative evaluations.",
        "Single-cell RNA transcriptome data present a tremendous opportunity for studying the cellular heterogeneity. Identifying subpopulations based on scRNA-seq data is a hot topic in recent years, although many researchers have been focused on designing elegant computational methods for identifying new cell types; however, the performance of these methods is still unsatisfactory due to the high dimensionality, sparsity and noise of scRNA-seq data. In this study, we propose a new cell type detection method by learning a robust and accurate similarity matrix, named SCCLRR. The method simultaneously captures both global and local intrinsic properties of data based on a low rank representation (LRR) framework mathematical model. The integrated normalized Euclidean distance and cosine similarity are used to balance the intrinsic linear and nonlinear manifold of data in the local regularization term. To solve the non-convex optimization model, we present an iterative optimization procedure using the alternating direction method of multipliers (ADMM) algorithm. We evaluate the performance of the SCCLRR method on nine real scRNA-seq datasets and compare it with seven state-of-the-art methods. The simulation results show that the SCCLRR outperforms other methods and is robust and effective for clustering scRNA-seq data. (The code of SCCLRR is free available for academic https://github.com/wzhangwhu/SCCLRR).",
        "K-Means is one of the most popular clustering algorithms that partitions observations into nonoverlapping subgroups based on a predefined similarity metric. Its drawbacks include a sensitivity to noisy features and a dependency of its resulting clusters upon the initial selection of cluster centroids resulting in the algorithm converging to local optima. Minkowski weighted K-Means (MWK-Means) addresses the issue of sensitivity to noisy features, but is sensitive to the initialization of clusters, and so the algorithm may similarly converge to local optima. Particle Swarm Optimization (PSO) uses a globalized search method to solve this issue. We present a hybrid Particle Swarm Optimization (PSO) + MWK-Means clustering algorithm to address all the above problems in a single framework, while maintaining benefits of PSO and MWK Means methods. This study investigated the utility of this approach in lateralizing the epileptogenic hemisphere for temporal lobe epilepsy (TLE) cases using magnetoencephalography (MEG) coherence source imaging (CSI) and diffusion tensor imaging (DTI). Using MEG-CSI, we analyzed preoperative resting state MEG data from 17 adults TLE patients with Engel class I outcomes to determine coherence at 54 anatomical sites and compared the results with 17 age- and gender-matched controls. Fiber-tracking was performed through the same anatomical sites using DTI data. Indices of both MEG coherence and DTI nodal degree were calculated. A PSO + MWK-Means clustering algorithm was applied to identify the side of temporal lobe epileptogenicity and distinguish between normal and TLE cases. The PSO module was aimed at identifying initial cluster centroids and assigning initial feature weights to cluster centroids and, hence, transferring to the MWK-Means module for the final optimal clustering solution. We demonstrated improvements with the use of the PSO + MWK-Means clustering algorithm compared to that of K-Means and MWK-Means independently. PSO + MWK-Means was able to successfully distinguish between normal and TLE in 97.2% and 82.3% of cases for DTI and MEG data, respectively. It also lateralized left and right TLE in 82.3% and 93.6% of cases for DTI and MEG data, respectively. The proposed optimization and clustering methodology for MEG and DTI features, as they relate to focal epileptogenicity, would enhance the identification of the TLE laterality in cases of unilateral epileptogenicity.",
        "Vehicle evaluation parameters, which are increasingly of concern for governments and consumers, quantify performance indicators, such as vehicle performance, emissions, and driving experience to help guide consumers in purchasing cars. While past approaches for driving cycle prediction have been proven effective and used in many countries, these algorithms are difficult to use in China with its complex traffic environment and increasingly high frequency of traffic jams. Meanwhile, we found that the vehicle dataset used by the driving cycle prediction problem is usually unbalanced in real cases, which means that there are more medium and high speed samples and very few samples at low and ultra-high speeds. If the ordinary clustering algorithm is directly applied to the unbalanced data, it will have a huge impact on the performance to build driving cycle maps, and the parameters of the map will deviate considerable from actual ones. In order to address these issues, this paper propose a novel driving cycle map algorithm framework based on an ensemble learning method named multi-clustering algorithm, to improve the performance of traditional clustering algorithms on unbalanced data sets. It is noteworthy that our model framework can be easily extended to other complicated structure areas due to its flexible modular design and parameter configuration. Finally, we tested our method based on actual traffic data generated in Fujian Province in China. The results prove the multi-clustering algorithm has excellent performance on our dataset.",
        "CD4(+) T cells are critical to fighting pathogens, but a comprehensive analysis of human T-cell specificities is hindered by the diversity of HLA alleles (>20,000) and the complexity of many pathogen genomes. We previously described GLIPH, an algorithm to cluster T-cell receptors (TCRs) that recognize the same epitope and to predict their HLA restriction, but this method loses efficiency and accuracy when >10,000 TCRs are analyzed. Here we describe an improved algorithm, GLIPH2, that can process millions of TCR sequences. We used GLIPH2 to analyze 19,044 unique TCRbeta sequences from 58 individuals latently infected with Mycobacterium tuberculosis (Mtb) and to group them according to their specificity. To identify the epitopes targeted by clusters of Mtb-specific T cells, we carried out a screen of 3,724 distinct proteins covering 95% of Mtb protein-coding genes using artificial antigen-presenting cells (aAPCs) and reporter T cells. We found that at least five PPE (Pro-Pro-Glu) proteins are targets for T-cell recognition in Mtb.",
        "BACKGROUND: Previous studies have shown that regulating the target region by real-time fMRI-based neurofeedback training can influence the activation of other regions and the functional connectivity between them. However, it is not clear whether the training effect of neurofeedback, especially in emotion regulation, is manifested in local network specialization or global network integration. In the current study, we chose the left amygdala (LA) as the target region to regulate positive emotion through real-time fMRI training. Average-linkage hierarchical clustering was employed to cluster the fMRI data recorded during the training to construct whole-brain networks and the LA network to which the LA belongs. RESULTS: The activation in the LA and those in some other regions were significantly up-regulated during the training. The clustering analysis at group level showed that the feedback training did not affect the number of networks in the whole brain but altered the distribution and functional connectivity in the LA network. CONCLUSION: These findings suggested that the feedback training effects in emotion regulation pattern reflected by the activity of the target brain network and the connections within the network were robustly embodied in local network specialization instead of in global network integration.",
        "PURPOSE: The plan-class specific reference field concept could theoretically improve the calibration of radiation detectors in a beam environment much closer to clinical deliveries than existing broad beam dosimetry protocols. Due to a lack of quantitative guidelines and representative data, however, the pcsr field concept has not yet been widely implemented. This work utilizes quantitative plan complexity metrics from modulated clinical treatments in order to investigate the establishment of potential plan classes using two different clustering methodologies. The utility of these potential plan clusters is then further explored by analyzing their relevance to actual dosimetric correction factors. METHODS: Two clinical databases containing several hundred modulated plans originally delivered on two Varian linear accelerators were analyzed using 21 plan complexity metrics. In the first approach, each database's plans were further subdivided into groups based on the anatomic site of treatment and then compared to one another using a series of nonparametric statistical tests. In the second approach, objective clustering algorithms were used to seek potential plan clusters in the multidimensional complexity-metric space. Concurrently, beam- and detector-specific dosimetric corrections for a subset of the modulated clinical plans were determined using Monte Carlo for three different ionization chambers. The distributions of the dosimetric correction factors were compared to the derived plan clusters to see which plan clusters, if any, could help predict the correction factor magnitudes. Ultimately, a simplified volume averaging metric (SVAM) is shown to be much more relevant to the total dosimetric correction factor than the established plan clusters. RESULTS: Plan groups based on the site of treatment did not show noticeable distinction from one another in the context of the metrics investigated. An objective clustering algorithm was able to discriminate volumetric modulated arc therapy (VMAT) plans from step-and-shoot intensity-modulated radiation therapy plans with an accuracy of 90.8%, but no clusters were found to exist at any level more specific than delivery modality. Monte Carlo determined correction factors for the modulated plans ranged from 0.970 to 1.104, 0.983 to 1.027, and 0.986 to 1.009 for the A12, A1SL, and A26 chambers, respectively, and were highly variable even within the treatment modality plan clusters. The magnitudes of these correction factors were explained almost entirely by volume averaging with SVAM demonstrating positive correlation with all Monte Carlo established total correction factors. CONCLUSIONS: Plan complexity metrics do provide some quantitative basis for the investigation of plan clusters, but an objective clustering algorithm demonstrated that quantifiable differences could only be found between VMAT and step-and-shoot beams delivered on the same treatment machine. The inherent variability of the Monte Carlo determined correction factors could not be explained solely by the modality of the treatment but were instead almost entirely dependent upon the volume averaging correction, which itself depends on the detector position within the dose distribution, dose gradients, and other factors. Considering the continued difficulty of determining a relevant plan metric to base plan clusters on, case-by-case corrections may instead obviate the need for the pcsr field concept in the future.",
        "BACKGROUND: It is well recognized that diabetes and peripheral neuropathy have a detrimental effect on gait. However, there are large variations in the results of studies addressing this aspect due to the heterogeneity of diabetic population in relation to presence and severity of diabetes complications. The aim of this study is to adopt an unsupervised classification technique to better elucidate the gait changes throughout the entire spectrum of diabetes and neuropathy. METHODS: Sixty subjects were assessed and classified into four groups using a fuzzy logic model: 13 controls (55 +/- 7years), 18 diabetics subjects without neuropathy (59 +/- 6 years, 11 +/- 7 diabetes years), 7 with mild neuropathy (56 +/- 4years, 19 +/- 7 diabetes years), and 22 with moderate to severe neuropathy (57 +/- 5 years, 14 +/- 8 diabetes years). Data were gathered by six infrared cameras at 100 Hz regarding lower limb joint kinematics (angles and angular velocities) and the relative phase for the hip-ankle, hip-knee, and knee-ankle were calculated. The K-means clustering algorithm was adopted to classify subjects considering the whole kinematics time series. A one-way ANOVA test was used to compare both clinical and kinematics parameters across clusters. RESULTS: Only the classification based on the intralimb coordination variables succeeded in defining 5 well separated clusters with the following clinical characteristics: controls were grouped mainly in Cluster 2, diabetics in Cluster 4, and neuropathic subjects in Cluster 5 (which included various degrees of severity). Hip-ankle coordination in Clusters 4 and 5 were significantly different (p < 0.05) with respect to Cluster 2, mainly in the stance phase. During the swing phase, differences were observed in the ankle-knee coordination (p < 0.05) across clusters. CONCLUSION: Classification based on intralimb coordination patterns succeeded in efficiently categorize gait alterations in diabetic subjects. It can be speculated that variables extracted from sagittal plane kinematics might be adopted as a support to clinical decision making in diabetes.",
        "Since the fuzzy local information C-means (FLICM) segmentation algorithm cannot take into account the impact of different features on clustering segmentation results, a local fuzzy clustering segmentation algorithm based on a feature selection Gaussian mixture model was proposed. First, the constraints of the membership degree on the spatial distance were added to the local information function. Second, the feature saliency was introduced into the objective function. By using the Lagrange multiplier method, the optimal expression of the objective function was solved. Neighborhood weighting information was added to the iteration expression of the classification membership degree to obtain a local feature selection based on feature selection. Each of the improved FLICM algorithm, the fuzzy C-means with spatial constraints (FCM_S) algorithm, and the original FLICM algorithm were then used to cluster and segment the interference images of Gaussian noise, salt-and-pepper noise, multiplicative noise, and mixed noise. The performances of the peak signal-to-noise ratio and error rate of the segmentation results were compared with each other. At the same time, the iteration time and number of iterations used to converge the objective function of the algorithm were compared. In summary, the improved algorithm significantly improved the ability of image noise suppression under strong noise interference, improved the efficiency of operation, facilitated remote sensing image capture under strong noise interference, and promoted the development of a robust anti-noise fuzzy clustering algorithm.",
        "Clustering algorithms are necessary in Wireless Sensor Networks to reduce the energy consumption of the overall nodes. The decision of which nodes are the cluster heads (CHs) greatly affects the network performance. The centralized clustering algorithms rely on a sink or Base Station (BS) to select the CHs. To do so, the BS requires extensive data from the nodes, which sometimes need complex hardware inside each node or a significant number of control messages. Alternatively, the nodes in distributed clustering algorithms decide about which the CHs are by exchanging information among themselves. Both centralized and distributed clustering algorithms usually alternate the nodes playing the role of the CHs to dynamically balance the energy consumption among all the nodes in the network. This paper presents a distributed approach to form the clusters dynamically, but it is occasionally supported by the Base Station. In particular, the Base Station sends three messages during the network lifetime to reconfigure the s k i p value of the network. The s k i p , which stands out as the number of rounds in which the same CHs are kept, is adapted to the network status in this way. At the beginning of each group of rounds, the nodes decide about their convenience to become a CH according to a fuzzy-logic system. As a novelty, the fuzzy controller is as a Tagaki-Sugeno-Kang model and not a Mandami-one as other previous proposals. The clustering algorithm has been tested in a wide set of scenarios, and it has been compared with other representative centralized and distributed fuzzy-logic based algorithms. The simulation results demonstrate that the proposed clustering method is able to extend the network operability.",
        "Fast and accurate obstacle detection is essential for accurate perception of mobile vehicles' environment. Because point clouds sensed by light detection and ranging (LiDAR) sensors are sparse and unstructured, traditional obstacle clustering on raw point clouds are inaccurate and time consuming. Thus, to achieve fast obstacle clustering in an unknown terrain, this paper proposes an elevation-reference connected component labeling (ER-CCL) algorithm using graphic processing unit (GPU) programing. LiDAR points are first projected onto a rasterized x-z plane so that sparse points are mapped into a series of regularly arranged small cells. Based on the height distribution of the LiDAR point, the ground cells are filtered out and a flag map is generated. Next, the ER-CCL algorithm is implemented on the label map generated from the flag map to mark individual clusters with unique labels. Finally, obstacle labeling results are inverse transformed from the x-z plane to 3D points to provide clustering results. For real-time 3D point cloud clustering, ER-CCL is accelerated by running it in parallel with the aid of GPU programming technology.",
        "Chronic Obstructive Pulmonary Disease (COPD) is a common chronic respiratory disease related to inflammation affected by harmful gas and particulate matter in the air. Mathematical prediction models between COPD and air pollutants are helpful for early identification, individualized interventions to slow disease progression, and for reduction of medical expenditures. The aim was to build a regression prediction model for the occurrence of COPD acute exacerbation. We collected hospital admissions for COPD in 2015-2018 from ten hospitals in Chongqing, China, used the increment per week as response, and the local sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO) and particulate matter 2.5 (PM2.5) concentrations as predictor variables to build a multiple prediction model. The Mean Absolute Percentage Error (MAPE) was used to evaluate the efficiency. We found that PM2.5 and SO2 are the most important factors contributing to the improvement of prediction accuracy. Multiple locally weighted linear regression (LWLR) Model based on integrated kernel framework with the K-means algorithm demonstrated minimum prediction error of 9.03 %(k=11).",
        "The functional or regulatory processes within the cell are explicitly governed by the expression levels of a subset of its genes. Gene expression time series captures activities of individual genes over time and aids revealing underlying cellular dynamics. An important step in high-throughput gene expression time series experiment is clustering genes based on their temporal expression patterns and is conventionally achieved by unsupervised machine learning techniques. However, most of the clustering techniques either suffer from the short length of gene expression time series or ignore temporal structure of the data. In this work, we propose DeepTrust, a novel deep learning-based framework for gene expression time series clustering which can overcome these issues. DeepTrust initially transforms time series data into images to obtain richer data representations. Afterwards, a deep convolutional clustering algorithm is applied on the constructed images. Analyses on both simulated and biological data sets exhibit the efficiency of this new framework, compared to widely used clustering techniques. We also utilize enrichment analyses to illustrate the biological plausibility of the clusters detected by DeepTrust. Our code and data are available from http://github.com/tanlab/DeepTrust.",
        "Identifying the molecular modules that drive cancer progression can greatly deepen the understanding of cancer mechanisms and provide useful information for targeted therapies. Most methods currently addressing this issue primarily use mutual exclusivity without making full use of the extra layer of module property. In this paper, we propose MCLCluster to identity cancer driver modules, which use somatic mutation data, Cancer Cell Fraction (CCF) data, gene functional interaction network and protein-protein interaction (PPI) network to derive the module property on mutual exclusivity, connectivity in PPI network and functionally similarity of genes. We have taken three effective measures to ensure the effectiveness of our algorithm. First, we use CCF data to choose stronger signals and more confident mutations. Second, the weighted gene functional interaction network is used to quantify the gene functional similarity in PPI. The third, graph clustering method based on Markov is exploited to extract the candidate module. MCLCluster is tested in the two TCGA datasets (GBM and BRCA), and identifies several well-known oncogenes driver modules and some modules with functionally associated driver genes. Besides, we compare it with Multi-Dendrix, FSME Cluster and RME in simulated dataset with background noise and passenger rate, MCLCluster outperforming all of these methods.",
        "The clustering ensemble has emerged as an important extension of the classical clustering problem. It provides an elegant framework to integrate multiple weak base clusterings to generate a strong consensus result. Most existing clustering ensemble methods usually exploit all data to learn a consensus clustering result, which does not sufficiently consider the adverse effects caused by some difficult instances. To handle this problem, we propose a novel self-paced clustering ensemble (SPCE) method, which gradually involves instances from easy to difficult ones into the ensemble learning. In our method, we integrate the evaluation of the difficulty of instances and ensemble learning into a unified framework, which can automatically estimate the difficulty of instances and ensemble the base clusterings. To optimize the corresponding objective function, we propose a joint learning algorithm to obtain the final consensus clustering result. Experimental results on benchmark data sets demonstrate the effectiveness of our method.",
        "We consider the task of producing a useful clustering of healthcare providers from their clinical action signature- their drug, procedure, and billing codes. Because high-dimensional sparse count vectors are challenging to cluster, we develop a novel autoencoder framework to address this task. Our solution creates a low-dimensional embedded representation of the high-dimensional space that preserves angular relationships and assigns examples to clusters while optimizing the quality of this clustering. Our method is able to fi nd a better clustering than under a two-step alternative, e.g., projected K means/medoids, where a representation is learned and then clustering is applied to the representation. We demonstrate our method's characteristics through quantitative and qualitative analysis of real and simulated data, including in several real-world healthcare case studies. Finally, we develop a tool to enhance exploratory analysis of providers based on their clinical behaviors.",
        "OBJECTIVE: Seizure clusters are often encountered in people with poorly controlled epilepsy. Detection of seizure clusters is currently based on simple clinical rules, such as two seizures separated by four or fewer hours or multiple seizures in 24h. Current definitions fail to distinguish between statistically significant clusters and those that may result from natural variation in the person's seizures. Ability to systematically define when a seizure cluster is significant for the individual carries major implications for treatment. However, there is no uniform consensus on how to define seizure clusters. This study proposes a principled statistical approach to defining seizure clusters that addresses these issues. METHODS: A total of 533,968 clinical seizures from 1,748 people with epilepsy in the Seizure Tracker seizure diary database were used for algorithm development. We propose an algorithm for automated individualized seizure cluster identification combining cumulative sum change-point analysis with bootstrapping and aberration detection, which provides a new approach to personalized seizure cluster identification at user-specified levels of clinical significance. We develop a standalone user interface to make the proposed algorithm accessible for real-time seizure cluster identification (ClusterCalc). Clinical impact of systematizing cluster identification is demonstrated by comparing empirically-defined clusters to those identified by routine seizure cluster definitions. We also demonstrate use of the Hurst exponent as a standardized measure of seizure clustering for comparison of seizure clustering burden within or across patients. RESULTS: Seizure clustering was present in 26.7 % (95 % CI, 24.5-28.7 %) of people with epilepsy. Empirical tables were provided for standardizing inter- and intra-patient comparisons of seizure cluster tendency. Using the proposed algorithm, we found that 37.7-59.4 % of seizures identified as clusters based on routine definitions had high probability of occurring by chance. Several clusters identified by the algorithm were missed by conventional definitions. The utility of the ClusterCalc algorithm for individualized seizure cluster detection is demonstrated. SIGNIFICANCE: This study proposes a principled statistical approach to individualized seizure cluster identification and demonstrates potential for real-time clinical usage through ClusterCalc. Using this approach accounts for individual variations in baseline seizure frequency and evaluates statistical significance. This new definition has the potential to improve individualized epilepsy treatment by systematizing identification of unrecognized seizure clusters and preventing unnecessary intervention for random events previously considered clusters.",
        "BACKGROUND: Recent years have witnessed an increasing interest in multi-omics data, because these data allow for better understanding complex diseases such as cancer on a molecular system level. In addition, multi-omics data increase the chance to robustly identify molecular patient sub-groups and hence open the door towards a better personalized treatment of diseases. Several methods have been proposed for unsupervised clustering of multi-omics data. However, a number of challenges remain, such as the magnitude of features and the large difference in dimensionality across different omics data sources. RESULTS: We propose a multi-modal sparse denoising autoencoder framework coupled with sparse non-negative matrix factorization to robustly cluster patients based on multi-omics data. The proposed model specifically leverages pathway information to effectively reduce the dimensionality of omics data into a pathway and patient specific score profile. In consequence, our method allows us to understand, which pathway is a feature of which particular patient cluster. Moreover, recently proposed machine learning techniques allow us to disentangle the specific impact of each individual omics feature on a pathway score. We applied our method to cluster patients in several cancer datasets using gene expression, miRNA expression, DNA methylation and CNVs, demonstrating the possibility to obtain biologically plausible disease subtypes characterized by specific molecular features. Comparison against several competing methods showed a competitive clustering performance. In addition, post-hoc analysis of somatic mutations and clinical data provided supporting evidence and interpretation of the identified clusters. CONCLUSIONS: Our suggested multi-modal sparse denoising autoencoder approach allows for an effective and interpretable integration of multi-omics data on pathway level while addressing the high dimensional character of omics data. Patient specific pathway score profiles derived from our model allow for a robust identification of disease subgroups.",
        "BACKGROUND: Recent research in machine-learning techniques has led to signi fi cant progress in various research fi elds. In particular, knowledge discovery using this method has become a hot topic in traditional Chinese medicine. As the key clinical manifestations of patients, symptoms play a signi fi cant role in clinical diagnosis and treatment, which evidently have their underlying traditional Chinese medicine mechanisms. OBJECTIVE: We aimed to explore the core symptoms and potential regularity of symptoms for diagnosing insomnia to reveal the key symptoms, hidden relationships underlying the symptoms, and their corresponding syndromes. METHODS: An insomnia dataset with 807 samples was extracted from real-world electronic medical records. After cleaning and selecting the theme data referring to the syndromes and symptoms, the symptom network analysis model was constructed using complex network theory. We used four evaluation metrics of node centrality to discover the core symptom nodes from multiple aspects. To explore the hidden relationships among symptoms, we trained each symptom node in the network to obtain the symptom embedding representation using the Skip-Gram model and node embedding theory. After acquiring the symptom vocabulary in a digital vector format, we calculated the similarities between any two symptom embeddings, and clustered these symptom embeddings into five communities using the spectral clustering algorithm. RESULTS: The top five core symptoms of insomnia diagnosis, including difficulty falling asleep, easy to wake up at night, dysphoria and irascibility, forgetful, and spiritlessness and weakness, were identified using evaluation metrics of node centrality. The symptom embeddings with hidden relationships were constructed, which can be considered as the basic dataset for future insomnia research. The symptom network was divided into five communities, and these symptoms were accurately categorized into their corresponding syndromes. CONCLUSIONS: These results highlight that network and clustering analyses can objectively and effectively find the key symptoms and relationships among symptoms. Identification of the symptom distribution and symptom clusters of insomnia further provide valuable guidance for clinical diagnosis and treatment.",
        "In this article, we propose a multiview self-representation model for nonlinear subspaces clustering. By assuming that the heterogeneous features lie within the union of multiple linear subspaces, the recent multiview subspace learning methods aim to capture the complementary and consensus from multiple views to boost the performance. However, in real-world applications, data feature usually resides in multiple nonlinear subspaces, leading to undesirable results. To this end, we propose a kernelized version of tensor-based multiview subspace clustering, which is referred to as Kt-SVD-MSC, to jointly learn self-representation coefficients in mapped high-dimensional spaces and multiple views correlation in unified tensor space. In view-specific feature space, a kernel-induced mapping is introduced for each view to ensure the separability of self-representation coefficients. In unified tensor space, a new kind of tensor low-rank regularizer is employed on the rotated self-representation coefficient tensor to preserve the global consistency across different views. We also derive an algorithm to efficiently solve the optimization problem with all the subproblems having closed-form solutions. Furthermore, by incorporating the nonnegative and sparsity constraints, the proposed method can be easily extended to a useful variant, meaning that several useful variants can be easily constructed in a similar way. Extensive experiments of the proposed method are tested on eight challenging data sets, in which a significant (even a breakthrough) advance over state-of-the-art multiview clustering is achieved.",
        "Understanding and classifying electromyogram (EMG) signals is of significance for dexterous prosthetic hand control, sign languages, grasp recognition, human-machine interaction, etc.. The existing research of EMG-based hand gesture classification faces the challenges of unsatisfied classification accuracy, insufficient generalization ability, lack of training data and weak robustness. To address these problems, this paper combines unsupervised and supervised learning methods to classify an EMG dataset consisting of 10 classes of hand gestures. To lessen the difficulty of classification, clustering methods including subtractive clustering and fuzzy c-means (FCM) clustering algorithms are employed first to obtain the initial partition of the inputs. In particular, modified FCM algorithm is proposed to accustom the conventional FCM to the multi-class classification problem. Based on the grouping information obtained from clustering, a type of two-step supervised learning approach is proposed. Specifically, a top-classifier and three sub-classifiers integrated with windowing method and majority voting are employed to accomplish the two-step classification. The results demonstrate that the proposed method achieves 100% test accuracy and the strongest robustness compared to the conventional machine learning approaches, which shows the potential for industrial and healthcare applications, such as movement intention detection, grasp recognition and dexterous prostheses control.",
        "The current study seeks to compare 3 clustering algorithms that can be used in gene-based bioinformatics research to understand disease networks, protein-protein interaction networks, and gene expression data. Denclue, Fuzzy-C, and Balanced Iterative and Clustering using Hierarchies (BIRCH) were the 3 gene-based clustering algorithms selected. These algorithms were explored in relation to the subfield of bioinformatics that analyzes omics data, which include but are not limited to genomics, proteomics, metagenomics, transcriptomics, and metabolomics data. The objective was to compare the efficacy of the 3 algorithms and determine their strength and drawbacks. Result of the review showed that unlike Denclue and Fuzzy-C which are more efficient in handling noisy data, BIRCH can handle data set with outliers and have a better time complexity.",
        "In the screening of cervical cancer cells, accurate identification and segmentation of nucleus in cell images is a key part in the early diagnosis of cervical cancer. Overlapping, uneven staining, poor contrast, and other reasons present challenges to cervical nucleus segmentation. We propose a segmentation method for cervical nuclei based on a multi-scale fuzzy clustering algorithm, which segments cervical cell clump images at different scales. We adopt a novel interesting degree based on area prior to measure the interesting degree of the node. The application of these two methods not only solves the problem of selecting the categories number of the clustering algorithm but also greatly improves the nucleus recognition performance. The method is evaluated by the IBSI2014 and IBSI2015 public datasets. Experiments show that the proposed algorithm has greater advantages than the state-of-the-art cervical nucleus segmentation algorithms and accomplishes high accuracy nucleus segmentation results.",
        "Graph based multi-view learning is well known due to its effectiveness and good clustering performance. However, most existing methods directly construct graph from original high-dimensional data which always contain redundancy, noise and outlying entries in real applications, resulting in unreliable and inaccurate graph. Moreover, they do not effectively select some useful features which are important for graph learning and clustering. To solve these limits, we propose a novel model that combines dimensionality reduction, manifold structure learning and feature selection into a framework. We map high-dimensional data into low-dimensional space to reduce the complexity of the algorithm and reduce the effect of noise and redundance. Therefore, we can adaptively learn a more accurate graph. Further more, l21-norm regularization is adopted to adaptively select some important features which help improve clustering performance. Finally, an efficiently algorithm is proposed to solve the optimal solution. Extensive experimental results on some benchmark datasets demonstrate the superiority of the proposed method.",
        "Background and objectivesHealth professionals look for specific patterns by correlating multiple physiological data in the process of deciding treatments to remedy clinical abnormalities. Biomedical data exhibit some common patterns in the event of identical clinical illnesses. The primary interest of this work is automatic discovery of such patterns in vital sign data (e.g. heart rate, blood pressure) using unsupervised learning and utilising them to identify patients with similar clinical conditions. MethodsA patient clustering method is developed that efficiently isolates patients into multiple groups by discovering dynamic patterns in multi-dimensional vital sign data. A dynamic partitioning algorithm and a patient clustering approach is proposed by introducing a measure namely aggregated instance-wise uncertainty (AIU) computed from multi-dimensional physiological time-series data. ResultsThe developed model is evaluated qualitatively using principal component analysis and silhouette value; and quantitatively in terms of its ability of clustering patients associated with different clinical situations. Experiments are conducted using real-world biomedical data of patients having various clinical conditions. Thee observed accuracy was 82.85% and 91.17% on two experimental datasets comprised of 35 and 34 patients data respectively.The comparisons show that the proposed approached outperformed than other methods in state-of-the-art approach. ConclusionsThe experimental outcomes demonstrate the effectiveness of the proposed approach in discovering distinct patterns with predictive significance.",
        "Concepts have been adopted in concept-cognitive learning (CCL) and conceptual clustering for concept classification and concept discovery. However, the standard CCL algorithms are incapable of tackling continuous data directly, and some standard conceptual clustering methods mainly focus on the attribute information, ignoring the object information that is also important to improve clustering analysis and concept classification ability. Therefore, in this article, we present a novel concept learning method, called the fuzzy-based concept learning model (FCLM), to address these two issues by exploiting concept hierarchical relations in concept lattices. More specifically, we first show some new related notions for FCLM based on a regular fuzzy formal decision context; among these notions, the object-oriented and attribute-oriented fuzzy concept similarities are used to achieve the concept similarity measure in concept lattices. Moreover, a novel fuzzy concept learning framework is designed, and its corresponding learning algorithms are developed. Finally, we conduct some experiments on various real-world datasets to demonstrate that the proposed method can achieve the state-of-the-art classification performance among similarity-based learning methods. In addition, we further verify the effectiveness of our method in concept discovery on the MNIST dataset.",
        "OBJECTIVE: To analyze the rule of acupoint selection for cancer pain based on data mining. METHODS: The published literature regarding acupuncture for cancer pain in the recent 10 years was searched in PubMed, CNKI, VIP and Wanfang database. The acupoint selection was summarized and analyzed by TCMISS V2.5. RESULTS: Totally 68 literature was collected and 73 acupoint prescriptions were included, involving 117 acupoints. These acupoints were mainly in bladder meridian, stomach meridian, liver meridian and spleen meridian. Among them, 40 acupoints used more than 4 times were identified, and the top three acupoints were Zusanli (ST 36, 65 times), Neiguan (PC 6, 55 times) and Taichong (LR 3, 50 times). A total of 68 acupoint combinations used more than 19 times were identified, and the top three acupoint combinations were Zusanli (ST 36)-Neiguan (PC 6), Taichong (LR 3)-Zusanli (ST 36) and Zusanli (ST 36)-Sanyinjiao (SP 6). There were 103 acupoint combinations with strong association; based on the entropy clustering algorithm, 20 new acupoint combinations and 10 new acupoint prescriptions were obtained. CONCLUSION: The main meridians for cancer pain are bladder meridian, stomach meridian, liver meridian and spleen meridian, with Zusanli (ST 36), Neiguan (PC 6), Taichong (LR 3), Hegu (LI 4), Sanyinjiao (SP 6) and ashi points as core acupoints, and regulating spleen-stomach and treating qi-blood are the main principles.",
        "This work addresses the problem of constructing a unified, topologically optimal connectivity-based brain atlas. The proposed approach aggregates an ensemble partition from individual parcellations without label agreement, providing a balance between sufficiently flexible individual parcellations and intuitive representation of the average topological structure of the connectome. The methods exploit a previously proposed dense connectivity representation, first performing graph-based hierarchical parcellation of individual brains, and subsequently aggregating the individual parcellations into a consensus parcellation. The search for consensus-based on the hard ensemble (HE) algorithm-approximately minimizes the sum of cluster membership distances, effectively estimating a pseudo-Karcher mean of individual parcellations. Computational stability, graph structure preservation, and biological relevance of the simplified representation resulting from the proposed parcellation are assessed on the Human Connectome Project data set. These aspects are assessed using (1) edge weight distribution divergence with respect to the dense connectome representation, (2) interhemispheric symmetry, (3) network characteristics' stability and agreement with respect to individually and anatomically parcellated networks, and (4) performance of the simplified connectome in a biological sex classification task. Ensemble parcellation was found to be highly stable with respect to subject sampling, outperforming anatomical atlases and other connectome-based parcellations in classification as well as preserving global connectome properties. The HE-based parcellation also showed a degree of symmetry comparable with anatomical atlases and a high degree of spatial contiguity without using explicit priors.",
        "Fuzzy c-means (FCM) is one of the best-known clustering methods to organize the wide variety of datasets automatically and acquire accurate classification, but it has a tendency to fall into local minima. For overcoming these weaknesses, some methods that hybridize PSO and FCM for clustering have been proposed in the literature, and it is demonstrated that these hybrid methods have an improved accuracy over traditional partition clustering approaches, whereas PSO-based clustering methods have poor execution time in comparison to partitional clustering techniques, and the current PSO algorithms require tuning a range of parameters before they are able to find good solutions. Therefore, this paper introduces a hybrid method for fuzzy clustering, named FCM-ELPSO, which aim to deal with these shortcomings. It combines FCM with an improved version of PSO, called ELPSO, which adopts a new enhanced logarithmic inertia weight strategy to provide better balance between exploration and exploitation. This new hybrid method uses PBM(F) index and the objective function value as cluster validity indexes to evaluate the clustering effect. To verify the effectiveness of the algorithm, two types of experiments are performed, including PSO clustering and hybrid clustering. Experiments show that the proposed approach significantly improves convergence speed and the clustering effect.",
        "MOTIVATION: Single-cell RNA-sequencing (scRNA-seq) profiles transcriptome of individual cells, which enables the discovery of cell types or subtypes by using unsupervised clustering. Current algorithms perform dimension reduction before cell clustering because of noises, high-dimensionality and linear inseparability of scRNA-seq data. However, independence of dimension reduction and clustering fails to fully characterize patterns in data, resulting in an undesirable performance. RESULTS: In this study, we propose a flexible and accurate algorithm for scRNA-seq data by jointly learning dimension reduction and cell clustering (aka DRjCC), where dimension reduction is performed by projected matrix decomposition and cell type clustering by non-negative matrix factorization. We first formulate joint learning of dimension reduction and cell clustering into a constrained optimization problem and then derive the optimization rules. The advantage of DRjCC is that feature selection in dimension reduction is guided by cell clustering, significantly improving the performance of cell type discovery. Eleven scRNA-seq datasets are adopted to validate the performance of algorithms, where the number of single cells varies from 49 to 68 579 with the number of cell types ranging from 3 to 14. The experimental results demonstrate that DRjCC significantly outperforms 13 state-of-the-art methods in terms of various measurements on cell type clustering (on average 17.44% by improvement). Furthermore, DRjCC is efficient and robust across different scRNA-seq datasets from various tissues. The proposed model and methods provide an effective strategy to analyze scRNA-seq data. AVAILABILITY AND IMPLEMENTATION: The software is coded using matlab, and is free available for academic https://github.com/xkmaxidian/DRjCC. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "BACKGROUND: Pancreatic ductal adenocarcinoma (PDAC) is the most common pancreatic malignancy. Due to its wide heterogeneity, PDAC acts aggressively and responds poorly to most chemotherapies, causing an urgent need for the development of new therapeutic strategies. Cell lines have been used as the foundation for drug development and disease modeling. CRISPR-Cas9 plays a key role in every step-in drug discovery: from target identification and validation to preclinical cancer cell testing. Using cell-line models and CRISPR-Cas9 technology together make drug target prediction feasible. However, there is still a large gap between predicted results and actionable targets in real tumors. Biological network models provide great modus to mimic genetic interactions in real biological systems, which can benefit gene perturbation studies and potential target identification for treating PDAC. Nevertheless, building a network model that takes cell-line data and CRISPR-Cas9 data as input to accurately predict potential targets that will respond well on real tissue remains unsolved. METHODS: We developed a novel algorithm 'Spectral Clustering for Network-based target Ranking' (SCNrank) that systematically integrates three types of data: expression profiles from tumor tissue, normal tissue and cell-line PDAC; protein-protein interaction network (PPI); and CRISPR-Cas9 data to prioritize potential drug targets for PDAC. The whole algorithm can be classified into three steps: 1. using STRING PPI network skeleton, SCNrank constructs tissue-specific networks with PDAC tumor and normal pancreas tissues from expression profiles; 2. With the same network skeleton, SCNrank constructs cell-line-specific networks using the cell-line PDAC expression profiles and CRISPR-Cas 9 data from pancreatic cancer cell-lines; 3. SCNrank applies a novel spectral clustering approach to reduce data dimension and generate gene clusters that carry common features from both networks. Finally, SCNrank applies a scoring scheme called 'Target Influence score' (TI), which estimates a given target's influence towards the cluster it belongs to, for scoring and ranking each drug target. RESULTS: We applied SCNrank to analyze 263 expression profiles, CRPSPR-Cas9 data from 22 different pancreatic cancer cell-lines and the STRING protein-protein interaction (PPI) network. With SCNrank, we successfully constructed an integrated tissue PDAC network and an integrated cell-line PDAC network, both of which contain 4414 selected genes that are overexpressed in tumor tissue samples. After clustering, 4414 genes are distributed into 198 clusters, which include 367 targets of FDA approved drugs. These drug targets are all scored and ranked by their TI scores, which we defined to measure their influence towards the network. We validated top-ranked targets in three aspects: Firstly, mapping them onto the existing clinical drug targets of PDAC to measure the concordance. Secondly, we performed enrichment analysis to these drug targets and the clusters there are within, to reveal functional associations between clusters and PDAC; Thirdly, we performed survival analysis for the top-ranked targets to connect targets with clinical outcomes. Survival analysis reveals that overexpression of three top-ranked genes, PGK1, HMMR and POLE2, significantly increases the risk of death in PDAC patients. CONCLUSION: SCNrank is an unbiased algorithm that systematically integrates multiple types of omics data to do potential drug target selection and ranking. SCNrank shows great capability in predicting drug targets for PDAC. Pancreatic cancer-associated gene candidates predicted by our SCNrank approach have the potential to guide genetics-based anti-pancreatic drug discovery.",
        "With recent advances in single-cell RNA sequencing, enormous transcriptome datasets have been generated. These datasets have furthered our understanding of cellular heterogeneity and its underlying mechanisms in homogeneous populations. Single-cell RNA sequencing (scRNA-seq) data clustering can group cells belonging to the same cell type based on patterns embedded in gene expression. However, scRNA-seq data are high-dimensional, noisy, and sparse, owing to the limitation of existing scRNA-seq technologies. Traditional clustering methods are not effective and efficient for high-dimensional and sparse matrix computations. Therefore, several dimension reduction methods have been introduced. To validate a reliable and standard research routine, we conducted a comprehensive review and evaluation of four classical dimension reduction methods and five clustering models. Four experiments were progressively performed on two large scRNA-seq datasets using 20 models. Results showed that the feature selection method contributed positively to high-dimensional and sparse scRNA-seq data. Moreover, feature-extraction methods were able to promote clustering performance, although this was not eternally immutable. Independent component analysis (ICA) performed well in those small compressed feature spaces, whereas principal component analysis was steadier than all the other feature-extraction methods. In addition, ICA was not ideal for fuzzy C-means clustering in scRNA-seq data analysis. K-means clustering was combined with feature-extraction methods to achieve good results.",
        "The activity of post-marketing surveillance results in a collection of large amount of data. The analysis of data is very useful for raising early warnings on possible adverse reactions of drugs. Association rule mining techniques have been heavily explored by the research community for identifying binary association between drugs and their adverse effects. But these techniques perform poorly and miss out several interesting associations when it comes to analysis of multidimensional data which may include multiple patient attributes, drugs and adverse drug reactions. In the present work, a clustering-based hybrid approach has been presented for finding quantitative multidimensional association from the large amount of data. Firstly, it employs clustering technique for segmentation of data into semantically coherent clusters. Furthermore, disproportionality method called proportional reporting ratio is applied on clustered data for generating statistically strong associations. The performance of the proposed methodology has been examined on the data taken from the U.S. Food and Drug Administration Adverse Event Reporting System database corresponding to Aspirin and nine other drugs which are prescribed along with Aspirin. The experimental results show that the proposed approach discovered a number of association rules which are very comprehensive and informative regarding relationship of patient traits and drugs with adverse drug reactions. On comparing experimental results with LPMiner, it is observed that the quantitative association rules discovered by LPMiner are just 8.3% of what have been discovered by the proposed methodology.",
        "Recent advances in long fragment read (LFR, also known as linked-read technologies or read-cloud) technologies, such as single tube long fragment reads (stLFR), 10X Genomics Chromium reads, and TruSeq synthetic long-reads, have enabled efficient haplotyping and genome assembly. However, in the case of stLFR and 10X Genomics Chromium reads, the long fragments of a genome are covered sparsely by reads in each barcode and most barcodes are contained in multiple long fragments from different regions, which results in inefficient assembly when using long-range information. Thus, methods to address these shortcomings are vital for capitalizing on the additional information obtained using these technologies. We therefore designed IterCluster, a novel, alignment-free clustering algorithm that can cluster barcodes from the same target region of a genome, using -mer frequency-based features and a Markov Cluster (MCL) approach to identify enough reads in a target region of a genome to ensure sufficient target genome sequence depth. The IterCluster method was validated using BGI stLFR and 10X Genomics chromium reads datasets. IterCluster had a higher precision and recall rate on BGI stLFR data compared to 10X Genomics Chromium read data. In addition, we demonstrated how IterCluster improves the de novo assembly results when using a divide-and-conquer strategy on a human genome data set (scaffold/contig N50 = 13.2 kbp/7.1 kbp vs. 17.1 kbp/11.9 kbp before and after IterCluster, respectively). IterCluster provides a new way for determining LFR barcode enrichment and a novel approach for de novo assembly using LFR data. IterCluster is OpenSource and available on https://github.com/JianCong-WENG/IterCluster.",
        "PURPOSE: To use a novel segmentation methodology based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) to define tumour subregions of liver metastases from colorectal cancer (CRC), to compare these with histology, and to use these to compare extracted pharmacokinetic (PK) parameters between tumour subregions. MATERIALS AND METHODS: This ethically-approved prospective study recruited patients with CRC and >/=1 hepatic metastases scheduled for hepatic resection. Patients underwent DCE-MRI pre-metastasectomy. Histological sections of resection specimens were spatially matched to DCE-MRI acquisitions and used to define histological subregions of viable and non-viable tumour. A semi-automated voxel-wise image segmentation algorithm based on the DCE-MRI contrast-uptake curves was used to define imaging subregions of viable and non-viable tumour. Overlap of histologically-defined and imaging subregions was compared using the Dice similarity coefficient (DSC). DCE-MRI PK parameters were compared for the whole tumour and histology-defined and imaging-derived subregions. RESULTS: Fourteen patients were included in the analysis. Direct histological comparison with imaging was possible in nine patients. Mean DSC for viable tumour subregions defined by imaging and histology was 0.738 (range 0.540-0.930). There were significant differences between K(trans) and kep for viable and non-viable subregions (p < 0.001) and between whole lesions and viable subregions (p < 0.001). CONCLUSION: We demonstrate good concordance of viable tumour segmentation based on pre-operative DCE-MRI with a post-operative histological gold-standard. This can be used to extract viable tumour-specific values from quantitative image analysis, and could improve treatment response assessment in clinical practice.",
        "In clustering problems, to model the intrinsic structure of unlabeled data, the latent variable models are frequently used. These model-based clustering methods often provide a clustering rule minimizing the total false assignment error. However, in many clustering applications, it is desirable to treat false assignment errors for a certain cluster differently. In this paper, we introduce the false assignment rate for clustering and estimate it by using the extended likelihood approach. We propose VRclust, a novel clustering rule that controls various errors differently across clusters. Real data examples illustrate the usage of estimation of false assignment rate and a simulation study shows that error controls are consistent as the sample size increases.",
        "Chronic diseases represent a serious threat to public health across the world. It is estimated at about 60% of all deaths worldwide and approximately 43% of the global burden of chronic diseases. Thus, the analysis of the healthcare data has helped health officials, patients, and healthcare communities to perform early detection for those diseases. Extracting the patterns from healthcare data has helped the healthcare communities to obtain complete medical data for the purpose of diagnosis. The objective of the present research work is presented to improve the surveillance detection system for chronic diseases, which is used for the protection of people's lives. For this purpose, the proposed system has been developed to enhance the detection of chronic disease by using machine learning algorithms. The standard data related to chronic diseases have been collected from various worldwide resources. In healthcare data, special chronic diseases include ambiguous objects of the class. Therefore, the presence of ambiguous objects shows the availability of traits involving two or more classes, which reduces the accuracy of the machine learning algorithms. The novelty of the current research work lies in the assumption that demonstrates the noncrisp Rough K-means (RKM) clustering for figuring out the ambiguity in chronic disease dataset to improve the performance of the system. The RKM algorithm has clustered data into two sets, namely, the upper approximation and lower approximation. The objects belonging to the upper approximation are favourable objects, whereas the ones belonging to the lower approximation are excluded and identified as ambiguous. These ambiguous objects have been excluded to improve the machine learning algorithms. The machine learning algorithms, namely, naive Bayes (NB), support vector machine (SVM), K-nearest neighbors (KNN), and random forest tree, are presented and compared. The chronic disease data are obtained from the machine learning repository and Kaggle to test and evaluate the proposed model. The experimental results demonstrate that the proposed system is successfully employed for the diagnosis of chronic diseases. The proposed model achieved the best results with naive Bayes with RKM for the classification of diabetic disease (80.55%), whereas SVM with RKM for the classification of kidney disease achieved 100% and SVM with RKM for the classification of cancer disease achieved 97.53 with respect to accuracy metric. The performance measures, such as accuracy, sensitivity, specificity, precision, and F-score, are employed to evaluate the performance of the proposed system. Furthermore, evaluation and comparison of the proposed system with the existing machine learning algorithms are presented. Finally, the proposed system has enhanced the performance of machine learning algorithms.",
        "Introduction: The segmentation method has a number of approaches, one of which is clustering. The clustering method is widely used for segmenting retinal blood vessels, especially the k-mean algorithm and fuzzy c-means (FCM). Unfortunately, so far there have been no studies comparing the two methods for blood vessel segmentation. Many studies do not explain the reason for choosing the method. Aim: This study aims to analyze the performance of the algorithms of k-means and FCM for retinal blood vessel segmentation. Methods: This research method is divided into three stages, namely preprocessing, segmentation, and performance analysis. Preprocessing uses the green channel method, Contrast-limited adaptive histogram equalization (CLAHE) and median filter. Segmentation is divided into three processes, namely clustering, thresholding and determining the region of interest (ROI). In the thresholding process, the determination of the threshold value uses two methods, namely the mean and the median. The third stage performs performance analysis using the performance parameters of the area under the curve (AUC) and statistical tests. Results: The statistical test results comparing FCM with k-means based on AUC values resulted in p-values <0.05 with a confidence level of 95%. Conclusion: Retinal vascular segmentation with the FCM method is significantly better than k-means.",
        "MOTIVATION: The rapid proliferation of single-cell RNA-sequencing (scRNA-Seq) technologies has spurred the development of diverse computational approaches to detect transcriptionally coherent populations. While the complexity of the algorithms for detecting heterogeneity has increased, most require significant user-tuning, are heavily reliant on dimension reduction techniques and are not scalable to ultra-large datasets. We previously described a multi-step algorithm, Iterative Clustering and Guide-gene Selection (ICGS), which applies intra-gene correlation and hybrid clustering to uniquely resolve novel transcriptionally coherent cell populations from an intuitive graphical user interface. RESULTS: We describe a new iteration of ICGS that outperforms state-of-the-art scRNA-Seq detection workflows when applied to well-established benchmarks. This approach combines multiple complementary subtype detection methods (HOPACH, sparse non-negative matrix factorization, cluster 'fitness', support vector machine) to resolve rare and common cell-states, while minimizing differences due to donor or batch effects. Using data from multiple cell atlases, we show that the PageRank algorithm effectively downsamples ultra-large scRNA-Seq datasets, without losing extremely rare or transcriptionally similar yet distinct cell types and while recovering novel transcriptionally distinct cell populations. We believe this new approach holds tremendous promise in reproducibly resolving hidden cell populations in complex datasets. AVAILABILITY AND IMPLEMENTATION: ICGS2 is implemented in Python. The source code and documentation are available at http://altanalyze.org. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Sensor selection plays an essential and fundamental role in prognostics and health management technology, and it is closely related to fault diagnosis, life prediction, and health assessment. The existing methods of sensor selection do not have an evaluation standard, which leads to different selection results. It is not helpful for the selection and layout of sensors. This paper proposes a comprehensive evaluation method of sensor selection for prognostics and health management (PHM) based on grey clustering. The described approach divides sensors into three grey classes, and defines and quantifies three grey indexes based on a dependency matrix. After a brief introduction to the whitening weight function, we propose a combination weight considering the objective data and subjective tendency to improve the effectiveness of the selection result. Finally, the clustering result of sensors is obtained by analyzing the clustering coefficient, which is calculated based on the grey clustering theory. The proposed approach is illustrated by an electronic control system, in which the effectiveness of different methods of sensor selection is compared. The result shows that the technique can give a convincing analysis result by evaluating the selection results of different methods, and is also very helpful for adjusting sensors to provide a more precise result. This approach can be utilized in sensor selection and evaluation for prognostics and health management.",
        "BACKGROUND: In unsupervised learning and clustering, data integration from different sources and types is a difficult question discussed in several research areas. For instance in omics analysis, dozen of clustering methods have been developed in the past decade. When a single source of data is at play, hierarchical clustering (HC) is extremely popular, as a tree structure is highly interpretable and arguably more informative than just a partition of the data. However, applying blindly HC to multiple sources of data raises computational and interpretation issues. RESULTS: We propose mergeTrees, a method that aggregates a set of trees with the same leaves to create a consensus tree. In our consensus tree, a cluster at height h contains the individuals that are in the same cluster for all the trees at height h. The method is exact and proven to be [Formula: see text], n being the individuals and q being the number of trees to aggregate. Our implementation is extremely effective on simulations, allowing us to process many large trees at a time. We also rely on mergeTrees to perform the cluster analysis of two real -omics data sets, introducing a spectral variant as an efficient and robust by-product. CONCLUSIONS: Our tree aggregation method can be used in conjunction with hierarchical clustering to perform efficient cluster analysis. This approach was found to be robust to the absence of clustering information in some of the data sets as well as an increased variability within true clusters. The method is implemented in R/C++ and available as an R package named mergeTrees, which makes it easy to integrate in existing or new pipelines in several research areas.",
        "Clustering is a technique to analyze empirical data, with a major application for biomedical research. Essentially, clustering finds groups of related points in a dataset. However, results depend on both metrics for point-to-point similarity and rules for point-to-group association. Non-appropriate metrics and rules can lead to artifacts, especially in case of multiple groups with heterogeneous structure. In this work, we propose a clustering algorithm that evaluates the properties of paths between points (rather than point-to-point similarity) and solves a global optimization problem, finding solutions not obtainable by methods relying on local choices. Moreover, our algorithm is trainable. Hence, it can be adapted and adopted for specific datasets and applications by providing examples of valid and invalid paths to train a path classifier. We demonstrate its applicability to identify heterogeneous groups in challenging synthetic datasets, segment highly nonconvex immune cells in confocal microscopy images, and classify arrhythmic heartbeats in electrocardiographic signals.",
        "Within a systematic literature review (SLR), researchers are confronted with vast amounts of articles from scientific databases, which have to be manually evaluated regarding their relevance for a certain field of observation. The evaluation and filtering phase of prevalent SLR methodologies is therefore time consuming and hardly expressible to the intended audience. The proposed method applies natural language processing (NLP) on article meta data and a k-means clustering algorithm to automatically convert large article corpora into a distribution of focal topics. This allows efficient filtering as well as objectifying the process through the discussion of the clustering results. Beyond that, it allows to quickly identify scientific communities and therefore provides an iterative perspective for the so far linear SLR methodology.*NLP and k-means clustering to filter large article corpora during systematic literature reviews.*Automated clustering allows filtering very efficiently as well as effectively compared to manual selection.*Presentation and discussion of the clustering results helps to objectify the nontransparent filtering step in systematic literature reviews.",
        "Racquet sports can provide positive benefits for human healthcare. A reliable detection device that can effectively distinguish movement with similar sub-features is therefore needed. In this paper, a racquet sports recognition wristband system and a multilayer hybrid clustering model are proposed to achieve reliable activity recognition and perform number counting. Additionally, a Bluetooth mesh network enables communication between a phone and wristband, and sets-up the connection between multiple devices. This allows users to track their exercise through the phone and share information with other players and referees. Considering the complexity of the classification algorithm and the user-friendliness of the measurement system, the improved multi-layer hybrid clustering model applies three-level K-means clustering to optimize feature extraction and segmentation and then uses the density-based spatial clustering of applications with noise (DBSCAN) algorithm to determine the feature center of different movements. The model can identify unlabeled and noisy data without data calibration and is suitable for smartwatches to recognize multiple racquet sports. The proposed system shows better recognition results and is verified in practical experiments.",
        "Long-read sequencing of transcripts with Pacific Biosciences (PacBio) Iso-Seq and Oxford Nanopore Technologies has proven to be central to the study of complex isoform landscapes in many organisms. However, current de novo transcript reconstruction algorithms from long-read data are limited, leaving the potential of these technologies unfulfilled. A common bottleneck is the dearth of scalable and accurate algorithms for clustering long reads according to their gene family of origin. To address this challenge, we develop isONclust, a clustering algorithm that is greedy (to scale) and makes use of quality values (to handle variable error rates). We test isONclust on three simulated and five biological data sets, across a breadth of organisms, technologies, and read depths. Our results demonstrate that isONclust is a substantial improvement over previous approaches, both in terms of overall accuracy and/or scalability to large data sets.",
        "Pedestrian path prediction is a very challenging problem because scenes are often crowded or contain obstacles. Existing state-of-the-art long short-term memory (LSTM)-based prediction methods have been mainly focused on analyzing the influence of other people in the neighborhood of each pedestrian while neglecting the role of potential destinations in determining a walking path. In this article, we propose classifying pedestrian trajectories into a number of route classes (RCs) and using them to describe the pedestrian movement patterns. Based on the RCs obtained from trajectory clustering, our algorithm, which we name the prediction of pedestrian paths by LSTM (PoPPL), predicts the destination regions through a bidirectional LSTM classification network in the first stage and then generates trajectories corresponding to the predicted destination regions through one of the three proposed LSTM-based architectures in the second stage. Our algorithm also outputs probabilities of multiple predicted trajectories that head toward the destination regions. We have evaluated PoPPL against other state-of-the-art methods on two public data sets. The results show that our algorithm outperforms other methods and incorporating potential destination prediction improves the trajectory prediction accuracy.",
        "Semi-supervised clustering is one of important research topics in cluster analysis, which uses pre-given knowledge as constraints to improve the clustering performance. While clustering a data set, people often get prior constraints from different information sources, which may have different representations and contents, to guide clustering process. However, most of existing semi-supervised clustering algorithms are based on single-source constraints and rarely consider to integrate multi-source constraints to enhance the clustering quality. To solve the problem, we analyze the relations among different types of constraints and propose an uniform representation for them. Based it, we propose a new semi-supervised clustering algorithm to find out a clustering that has good cluster structure and high consensus of all the sources of constraints. In the algorithm, we construct an optimization objective model and its solution method to achieve the aim. This algorithm can integrate multi-source constraints well to reduce the effect of incorrect constraints from single sources and find out a high-quality clustering. By the experimental studies on several benchmark data sets, we illustrate the effectiveness of the proposed algorithm, compared to other semi-supervised clustering algorithms.",
        "MOTIVATION: Dimension reduction techniques are widely used to interpret high-dimensional biological data. Features learned from these methods are used to discover both technical artifacts and novel biological phenomena. Such feature discovery is critically importent in analysis of large single-cell datasets, where lack of a ground truth limits validation and interpretation. Transfer learning (TL) can be used to relate the features learned from one source dataset to a new target dataset to perform biologically driven validation by evaluating their use in or association with additional sample annotations in that independent target dataset. RESULTS: We developed an R/Bioconductor package, projectR, to perform TL for analyses of genomics data via TL of clustering, correlation and factorization methods. We then demonstrate the utility TL for integrated data analysis with an example for spatial single-cell analysis. AVAILABILITY AND IMPLEMENTATION: projectR is available on Bioconductor and at https://github.com/genesofeve/projectR. CONTACT: gsteinobrien@jhmi.edu or ejfertig@jhmi.edu. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Over the recent era, Wireless Sensor Network (WSN) has attracted much attention among industrialists and researchers owing to its contribution to numerous applications including military, environmental monitoring and so on. However, reducing the network delay and improving the network lifetime are always big issues in the domain of WSN. To resolve these downsides, we propose an Energy-Efficient Scheduling using the Deep Reinforcement Learning (DRL) (E(2)S-DRL) algorithm in WSN. E(2)S-DRL contributes three phases to prolong network lifetime and to reduce network delay that is: the clustering phase, duty-cycling phase and routing phase. E(2)S-DRL starts with the clustering phase where we reduce the energy consumption incurred during data aggregation. It is achieved through the Zone-based Clustering (ZbC) scheme. In the ZbC scheme, hybrid Particle Swarm Optimization (PSO) and Affinity Propagation (AP) algorithms are utilized. Duty cycling is adopted in the second phase by executing the DRL algorithm, from which, E(2)S-DRL reduces the energy consumption of individual sensor nodes effectually. The transmission delay is mitigated in the third (routing) phase using Ant Colony Optimization (ACO) and the Firefly Algorithm (FFA). Our work is modeled in Network Simulator 3.26 (NS3). The results are valuable in provisions of upcoming metrics including network lifetime, energy consumption, throughput and delay. From this evaluation, it is proved that our E(2)S-DRL reduces energy consumption, reduces delays by up to 40% and enhances throughput and network lifetime up to 35% compared to the existing cTDMA, DRA, LDC and iABC methods.",
        "With the development of population aging, the recognition of elderly activity in smart homes has received increasing attention. In recent years, single-resident activity recognition based on smart homes has made great progress. However, few researchers have focused on multi-resident activity recognition. In this paper, we propose a method to recognize two-resident activities based on time clustering. First, to use a de-noising method to extract the feature of the dataset. Second, to cluster the dataset based on the begin time and end time. Finally, to complete activity recognition using a similarity matching method. To test the performance of the method, we used two two-resident datasets provided by Center for Advanced Studies in Adaptive Systems (CASAS). We evaluated our method by comparing it with some common classifiers. The results show that our method has certain improvements in the accuracy, recall, precision, and F-Measure. At the end of the paper, we explain the parameter selection and summarize our method.",
        "This paper considers the clustering problem of physical step count data recorded on wearable devices. Clustering step data give an insight into an individual's activity status and further provide the groundwork for health-related policies. However, classical methods, such as K-means clustering and hierarchical clustering, are not suitable for step count data that are typically high-dimensional and zero-inflated. This paper presents a new clustering method for step data based on a novel combination of ensemble clustering and binning. We first construct multiple sets of binned data by changing the size and starting position of the bin, and then merge the clustering results from the binned data using a voting method. The advantage of binning, as a critical component, is that it substantially reduces the dimension of the original data while preserving the essential characteristics of the data. As a result, combining clustering results from multiple binned data can provide an improved clustering result that reflects both local and global structures of the data. Simulation studies and real data analysis were carried out to evaluate the empirical performance of the proposed method and demonstrate its general utility.",
        "In machine learning, the nature of the dataset itself such as convexity of the data point sets affects the right choice of clustering algorithm to give good performance. This brief paper first focuses on how data convexity influences the clustering performance on biomedical datasets. Then it addresses the main challenges of two well-known clustering groups which are centroid-based and density-based clustering. These techniques typically require a set of parameters to be provided by the user before the algorithms can perform well in terms of good clustering and give the optimal number of clusters. Two parameter independent clustering techniques utilizing unique neighborhood sets (UNSs) called Parameter Independent Convex Centroid-based Clustering (ConvexClust) for convex-dominated datasets and Parameter Independent Non-Convex Density-based Clustering (NonConvexClust) for nonconvex-dominated datasets are introduced. The ConvexClust and NonConvex Clust algorithms are extensively evaluated on real-world biomedical datasets. Their performances are also compared with other clustering algorithms using evaluation criteria such as SSE, entropy and purity. The results have revealed the good performance of the proposed parameter-independent clustering techniques and also shown that most of the biomedical datasets in the experiments demonstrated their tendency towards convex-dominated data point sets.",
        "Brain electroencephalography (EEG), the complex, weak, multivariate, nonlinear, and nonstationary time series, has been recently widely applied in neurocognitive disorder diagnoses and brain-machine interface developments. With its specific features, unlabeled EEG is not well addressed by conventional unsupervised time-series learning methods. In this article, we handle the problem of unlabeled EEG time-series clustering and propose a novel EEG clustering algorithm, that we call mwcEEGc. The idea is to map the EEG clustering to the maximum-weight clique (MWC) searching in an improved Frechet similarity-weighted EEG graph. The mwcEEGc considers the weights of both vertices and edges in the constructed EEG graph and clusters EEG based on their similarity weights instead of calculating the cluster centroids. To the best of our knowledge, it is the first attempt to cluster unlabeled EEG trials using MWC searching. The mwcEEGc achieves high-quality clusters with respect to intracluster compactness as well as intercluster scatter. We demonstrate the superiority of mwcEEGc over ten state-of-the-art unsupervised learning/clustering approaches by conducting detailed experimentations with the standard clustering validity criteria on 14 real-world brain EEG datasets. We also present that mwcEEGc satisfies the theoretical properties of clustering, such as richness, consistency, and order independence.",
        "We share our experiences teaching university students about clustering algorithms using EduClust, an online visualization we developed. EduClust supports professors in preparing teaching material and students in visually and interactively exploring cluster steps and the effects of changing clustering parameters. We used EduClust for two years in our computer science lectures on clustering algorithms and share our experience integrating the online application in a data science curriculum. We also point to opportunities for future development.",
        "Responses of sensory neurons are often modeled using a weighted combination of rectified linear subunits. Since these subunits often cannot be measured directly, a flexible method is needed to infer their properties from the responses of downstream neurons. We present a method for maximum likelihood estimation of subunits by soft-clustering spike-triggered stimuli, and demonstrate its effectiveness in visual neurons. For parasol retinal ganglion cells in macaque retina, estimated subunits partitioned the receptive field into compact regions, likely representing aggregated bipolar cell inputs. Joint clustering revealed shared subunits between neighboring cells, producing a parsimonious population model. Closed-loop validation, using stimuli lying in the null space of the linear receptive field, revealed stronger nonlinearities in OFF cells than ON cells. Responses to natural images, jittered to emulate fixational eye movements, were accurately predicted by the subunit model. Finally, the generality of the approach was demonstrated in macaque V1 neurons.",
        "Emission of N2O represents an increasing concern in wastewater treatment, in particular for its large contribution to the plant's carbon footprint (CFP). In view of the potential introduction of more stringent regulations regarding wastewater treatment plants' CFP, there is a growing need for advanced monitoring with online implementation of mitigation strategies for N2O emissions. Mechanistic kinetic modelling in full-scale applications, are often represented by a very detailed representation of the biological mechanisms resulting in an elevated uncertainty on the many parameters used while limited by a poor representation of hydrodynamics. This is particularly true for current N2O kinetic models. In this paper, a possible full-scale implementation of a data mining approach linking plant-specific dynamics to N2O production is proposed. A data mining approach was tested on full-scale data along with different clustering techniques to identify process criticalities. The algorithm was designed to provide an applicable solution for full-scale plants' control logics aimed at online N2O emission mitigation. Results show the ability of the algorithm to isolate specific N2O emission pathways, and highlight possible solutions towards emission control.",
        "In this paper, we propose a novel hyper-Laplacian regularized multiview subspace clustering with low-rank tensor constraint method, which is referred as HLR-MSCLRT. In the HLR-MSCLRT model, the subspace representation matrices of different views are stacked as a tensor, and then the high order correlations among data can be captured. To reduce the redundancy information of the learned subspace representations, a low-rank constraint is adopted to the constructed tensor. Since data in the real world often reside in multiple nonlinear subspaces, the HLR-MSCLRT model utilizes the hyper-Laplacian graph regularization to preserve the local geometry structure embedded in a high-dimensional ambient space. An efficient algorithm is also presented to solve the optimization problem of the HLR-MSCLRT model. The experimental results on some data sets show that the proposed HLR-MSCLRT model outperforms many state-of-the-art multi-view clustering approaches.",
        "STUDY DESIGN: Method development. OBJECTIVES: To develop a reliable protocol for automatic segmentation of Thoracolumbar spinal cord using MRI based on K-means clustering algorithm in 3D images. SETTING: University-based laboratory, Tehran, Iran. METHODS: T2 structural volumes acquired from the spinal cord of 20 uninjured volunteers on a 3T MR scanner. We proposed an automatic method for spinal cord segmentation based on the K-means clustering algorithm in 3D images and compare our results with two available segmentation methods (PropSeg, DeepSeg) implemented in the Spinal Cord Toolbox. Dice and Hausdorff were used to compare the results of our method (K-Seg) with the manual segmentation, PropSeg, and DeepSeg. RESULTS: The accuracy of our automatic segmentation method for T2-weighted images was significantly better or similar to the SCT methods, in terms of 3D DC (p < 0.001). The 3D DCs were respectively (0.81 +/- 0.04) and Hausdorff Distance (12.3 +/- 2.48) by the K-Seg method in contrary to other SCT methods for T2-weighted images. CONCLUSIONS: The output with similar protocols showed that K-Seg results match the manual segmentation better than the other methods especially on the thoracolumbar levels in the spinal cord due to the low image contrast as a result of poor SNR in these areas.",
        "Outcome regressed on class labels identified by unsupervised clustering is custom in many applications. However, it is common to ignore the misclassification of class labels caused by the learning algorithm, which potentially leads to serious bias of the estimated effect parameters. Due to their generality we suggest to address the problem by use of regression calibration or the misclassification simulation and extrapolation method. Performance is illustrated by simulated data from Gaussian mixture models, documenting a reduced bias and improved coverage of confidence intervals when adjusting for misclassification with either method. Finally, we apply our method to data from a previous study, which regressed overall survival on class labels derived from unsupervised clustering of gene expression data from bone marrow samples of multiple myeloma patients.",
        "Clustering is the task of identifying groups of similar subjects according to certain criteria. The AJCC staging system can be thought as a clustering mechanism that groups patients based on their disease stage. This grouping drives prognosis and influences treatment. The goal of this work is to evaluate the efficacy of machine learning algorithms to cluster the patients into discriminative groups to improve prognosis for overall survival (OS) and relapse free survival (RFS) outcomes. We apply clustering over a retrospectively collected data from 644 head and neck cancer patients including both clinical and radiomic features. In order to incorporate outcome information into the clustering process and deal with the large proportion of censored samples, the feature space was scaled using the regression coefficients fitted using a proxy dependent variable, martingale residuals, instead of follow-up time. Two clusters were identified and evaluated using cross validation. The Kaplan Meier (KM) curves between the two clusters differ significantly for OS and RFS (p-value < 0.0001). Moreover, there was a relative predictive improvement when using the cluster label in addition to the clinical features compared to using only clinical features where AUC increased by 5.7% and 13.0% for OS and RFS, respectively.",
        "Single-cell RNA sequencing (scRNA-seq) technologies allow numerous opportunities for revealing novel and potentially unexpected biological discoveries. scRNA-seq clustering helps elucidate cell-to-cell heterogeneity and uncover cell subgroups and cell dynamics at the group level. Two important aspects of scRNA-seq data analysis were introduced and discussed in the present review: relevant datasets and analytical tools. In particular, we reviewed popular scRNA-seq datasets and discussed scRNA-seq clustering models including K-means clustering, hierarchical clustering, consensus clustering, and so on. Seven state-of-the-art scRNA clustering methods were compared on five public available datasets. Two primary evaluation metrics, the Adjusted Rand Index (ARI) and the Normalized Mutual Information (NMI), were used to evaluate these methods. Although unsupervised models can effectively cluster scRNA-seq data, these methods also have challenges. Some suggestions were provided for future research directions.",
        "Moving object detection in video streams plays a key role in many computer vision applications. In particular, separation between background and foreground items represents a main prerequisite to carry out more complex tasks, such as object classification, vehicle tracking, and person re-identification. Despite the progress made in recent years, a main challenge of moving object detection still regards the management of dynamic aspects, including bootstrapping and illumination changes. In addition, the recent widespread of Pan-Tilt-Zoom (PTZ) cameras has made the management of these aspects even more complex in terms of performance due to their mixed movements (i.e. pan, tilt, and zoom). In this paper, a combined keypoint clustering and neural background subtraction method, based on Self-Organized Neural Network (SONN), for real-time moving object detection in video sequences acquired by PTZ cameras is proposed. Initially, the method performs a spatio-temporal tracking of the sets of moving keypoints to recognize the foreground areas and to establish the background. Then, it adopts a neural background subtraction, localized in these areas, to accomplish a foreground detection able to manage bootstrapping and gradual illumination changes. Experimental results on three well-known public datasets, and comparisons with different key works of the current literature, show the efficiency of the proposed method in terms of modeling and background subtraction.",
        "In this paper, a new algorithm denoted as FilterK is proposed for improving the purity of k-means derived physical activity clusters by reducing outlier influence. We applied it to physical activity data obtained with body-worn accelerometers and clustered using k-means. We compared its performance with three existing outlier detection methods: Local Outlier Factor, Isolation Forests and KNN using the ground truth (class labels), average cluster and event purity (ACEP). FilterK provided comparable gains in ACEP (0.581 --> 0.596 compared to 0.580-0.617) whilst removing a lower number of outliers than the other methods (4% total dataset size vs 10% to achieve this ACEP). The main focus of our new outlier detection method is to improve the cluster purities of physical activity accelerometer data, but we also suggest it may be potentially applied to other types of dataset captured by k-means clustering. We demonstrate our method using a k-means model trained on two independent accelerometer datasets (training n = 90) and re-applied to an independent dataset (test n = 41). Labelled physical activities include lying down, sitting, standing, household chores, walking (laboratory and non-laboratory based), stairs and running. This type of clustering algorithm could be used to assist with identifying optimal physical activity patterns for health.",
        "In this article, we propose a two-stage time-series clustering approach to cluster time series with different shapes. The first step is to represent the time series by a suite of information granules following the principle of justifiable granularity to perform dimensionality reduction, while the second step is to realize the fuzzy clustering of the time series in the transformed representation space (viz., the space of information granules). In the dimensionality reduction process, the numerical data are granulated using a collection of information granules forming a new sequence that can well describe the original time series. Then, when clustering the time series, dynamic time warping (DTW) is employed to measure the similarity between time series and DTW barycenter averaging (DBA) is generalized to weighted DBA to be involved in the fuzzy C-means (FCMs) algorithm. Finally, the experiments are conducted on the datasets coming from UCR time-series database and Chinese stocks to demonstrate the effectiveness and advantages of the proposed fuzzy clustering approach.",
        "The bedding materials used in dairy cow housing systems are extremely important for animal welfare and performance. A wide range of materials can be used as bedding for dairy cattle, but their physical properties must be analysed to evaluate their potential. In the present study, the physical properties of various bedding materials for dairy cattle were investigated, and different fuzzy clustering algorithms were employed to cluster these materials based on their physical properties. A total of 51 different bedding materials from various places in Europe were collected and tested. Physical analyses were carried out for the following parameters: bulk density (BD), water holding capacity (WHC), air-filled porosity (AFP), global density (GD), container capacity (CC), total effective porosity (TEP), saturated humidity (SH), humidity (H), and average particle size (APS). These data were analysed by principal components analysis (PCA) to reduce the amount of data and, subsequently, by fuzzy clustering analysis. Three clustering algorithms were tested: k-means (KM), fuzzy c-means (FCM) and Gustafson-Kessel (GK) algorithms. Furthermore, different numbers of clusters (2-8) were evaluated and subsequently compared using five validation indexes. The GK clustering algorithm with eight clusters fit better regarding the division of materials according to their properties. From this clustering analysis, it was possible to understand how the physical properties of the bedding materials may influence their behaviour. Among the materials that fit better as bedding materials for dairy cows, Posidonia oceanica (Cluster 6) can be considered an alternative material.",
        "Clustering of single-cell RNA sequencing (scRNA-seq) data enables discovering cell subtypes, which is helpful for understanding and analyzing the processes of diseases. Determining the weight of edges is an essential component in graph-based clustering methods. While several graph-based clustering algorithms for scRNA-seq data have been proposed, they are generally based on k-nearest neighbor (KNN) and shared nearest neighbor (SNN) without considering the structure information of graph. Here, to improve the clustering accuracy, we present a novel method for single-cell clustering, called structural shared nearest neighbor-Louvain (SSNN-Louvain), which integrates the structure information of graph and module detection. In SSNN-Louvain, based on the distance between a node and its shared nearest neighbors, the weight of edge is defined by introducing the ratio of the number of the shared nearest neighbors to that of nearest neighbors, thus integrating structure information of the graph. Then, a modified Louvain community detection algorithm is proposed and applied to identify modules in the graph. Essentially, each community represents a subtype of cells. It is worth mentioning that our proposed method integrates the advantages of both SNN graph and community detection without the need for tuning any additional parameter other than the number of neighbors. To test the performance of SSNN-Louvain, we compare it to five existing methods on 16 real datasets, including nonnegative matrix factorization, single-cell interpretation via multi-kernel learning, SNN-Cliq, Seurat and PhenoGraph. The experimental results show that our approach achieves the best average performance in these datasets.",
        "Incomplete multi-view clustering (IMVC) optimally combines multiple pre-specified incomplete views to improve clustering performance. Among various excellent solutions, the recently proposed multiple kernel k-means with incomplete kernels (MKKM-IK) forms a benchmark, which redefines IMVC as a joint optimization problem where the clustering and kernel matrix imputation tasks are alternately performed until convergence. Though demonstrating promising performance in various applications, we observe that the manner of kernel matrix imputation in MKKM-IK would incur intensive computational and storage complexities, over-complicated optimization and limitedly improved clustering performance. In this paper, we first propose an Efficient and Effective Incomplete Multi-view Clustering (EE-IMVC) algorithm to address these issues. Instead of completing the incomplete kernel matrices, EE-IMVC proposes to impute each incomplete base matrix generated by incomplete views with a learned consensus clustering matrix. Moreover, we further improve this algorithm by incorporating prior knowledge to regularize the learned consensus clustering matrix. Two three-step iterative algorithms are carefully developed to solve the resultant optimization problems with linear computational complexity, and their convergence is theoretically proven. After that, we theoretically study the generalization bound of the proposed algorithms. Furthermore, we conduct comprehensive experiments to study the proposed algorithms in terms of clustering accuracy, evolution of the learned consensus clustering matrix and the convergence. As indicated, our algorithms deliver their effectiveness by significantly and consistently outperforming some state-of-the-art ones.",
        "Age-associated changes in walking parameters are relevant to recognize functional capacity and physical performance. However, the sensible nuances of slightly different gait patterns are hardly noticeable by inexperienced observers. Due to the complexity of this evaluation, we aimed at verifying the efficiency of applied hybrid-adaptive algorithms to cluster groups with similar gait patterns. Based on self-organizing maps (SOM), k-means clustering (KM), and fuzzy c-means (FCM), we compared the hybrid algorithms to a conventional FCM approach to cluster accordingly age-related groups. Additionally, we performed a relevance analysis to identify the principal gait characteristics. Our experiments, based on inertial-sensors data, comprised a sample of 180 healthy subjects, divided into age-related groups. The outcomes suggest that our methods outperformed the FCM algorithm, demonstrating a high accuracy (88%) and consistent sensitivity also to distinguish groups that presented a significant difference (p<.05) only in one of the six observed gait features. The applied algorithms showed a compatible performance, but the SOM + KM required less computation cost and, therefore, was more efficient. Furthermore, the results indicate the overall importance of cadence, as a measurement of physical performance, especially when clustering subjects by their age. Such output provides valuable information to healthcare professionals, concerning the subject's physical performance related to his age, supporting and guiding the physical evaluation.",
        "Background: Advances in molecular biology have resulted in big and complicated data sets, therefore a clustering approach that able to capture the actual structure and the hidden patterns of the data is required. Moreover, the geometric space may not reflects the actual similarity between the different objects. As a result, in this research we use clustering-based space that convert the geometric space of the molecular to a categorical space based on clustering results. Then we use this space for developing a new classification algorithm. Results: In this study, we propose a new classification method named GrpClassifierEC that replaces the given data space with categorical space based on ensemble clustering (EC). The EC space is defined by tracking the membership of the points over multiple runs of clustering algorithms. Different points that were included in the same clusters will be represented as a single point. Our algorithm classifies all these points as a single class. The similarity between two objects is defined as the number of times that these objects were not belong to the same cluster. In order to evaluate our suggested method, we compare its results to the k nearest neighbors, Decision tree and Random forest classification algorithms on several benchmark datasets. The results confirm that the suggested new algorithm GrpClassifierEC outperforms the other algorithms. Conclusions: Our algorithm can be integrated with many other algorithms. In this research, we use only the k-means clustering algorithm with different k values. In future research, we propose several directions: (1) checking the effect of the clustering algorithm to build an ensemble clustering space. (2) Finding poor clustering results based on the training data, (3) reducing the volume of the data by combining similar points based on the EC. Availability and implementation: The KNIME workflow, implementing GrpClassifierEC, is available at https://malikyousef.com.",
        "Brain network analysis using functional magnetic resonance imaging (fMRI) is a widely used technique. The first step of brain network analysis in fMRI is to detect regions of interest (ROIs). The signals from these ROIs are then used to evaluate neural networks and quantify neuronal dynamics. The two main methods to identify ROIs are based on brain atlas registration and clustering. This work proposes a bioinspired method that combines both paradigms. The method, dubbed HAnt, consists of an anatomical clustering of the signal followed by an ant clustering step. The method is evaluated empirically in both in silico and in vivo experiments. The results show a significantly better performance of the proposed approach compared to other brain parcellations obtained using purely clustering-based strategies or atlas-based parcellations.",
        "The detection and delineation of QRS-complexes and T-waves in Electrocardiogram (ECG) is an important task because these features are associated with the cardiac abnormalities including ventricular arrhythmias that may lead to sudden cardiac death. In this paper, we propose a novel method for the R-peak and the T-peak detection using hierarchical clustering and Discrete Wavelet Transform (DWT) from the ECG signal. In the first step, a template of the single ECG beat is identified. Secondly, all R-peaks are detected by using hierarchical clustering. Then, each corresponding T-wave boundary is delineated based on the template morphology. Finally, the determination of T wave peaks is achieved based on the Modulus-Maxima Analysis (MMA) of the DWT coefficients. We evaluated the algorithm by using all records from the MIT-BIH arrhythmia database and QT database. The R-peak detector achieved a sensitivity of 99.89%, a positive predictivity of 99.97% and 99.83% accuracy over the validation MIT-BIH database. In addition, it shows a sensitivity of 100%, a positive predictivity of 99.83% in manually annotated QT database. It also shows 99.92% sensitivity and 99.96% positive predictivity over the automatic annotated QT database. In terms of the T-peak detection, our algorithm is verified with 99.91% sensitivity and 99.38% positive predictivity in manually annotated QT database.",
        "Microbial network inference and analysis have become successful approaches to extract biological hypotheses from microbial sequencing data. Network clustering is a crucial step in this analysis. Here, we present a novel heuristic network clustering algorithm, manta, which clusters nodes in weighted networks. In contrast to existing algorithms, manta exploits negative edges while differentiating between weak and strong cluster assignments. For this reason, manta can tackle gradients and is able to avoid clustering problematic nodes. In addition, manta assesses the robustness of cluster assignment, which makes it more robust to noisy data than most existing tools. On noise-free synthetic data, manta equals or outperforms existing algorithms, while it identifies biologically relevant subcompositions in real-world data sets. On a cheese rind data set, manta identifies groups of taxa that correspond to intermediate moisture content in the rinds, while on an ocean data set, the algorithm identifies a cluster of organisms that were reduced in abundance during a transition period but did not correlate strongly to biochemical parameters that changed during the transition period. These case studies demonstrate the power of manta as a tool that identifies biologically informative groups within microbial networks.IMPORTANCE manta comes with unique strengths, such as the abilities to identify nodes that represent an intermediate between clusters, to exploit negative edges, and to assess the robustness of cluster membership. manta does not require parameter tuning, is straightforward to install and run, and can be easily combined with existing microbial network inference tools.",
        "Alzheimer's disease (AD) has become a severe medical challenge. Advances in technologies produced high-dimensional data of different modalities including functional magnetic resonance imaging (fMRI) and single nucleotide polymorphism (SNP). Understanding the complex association patterns among these heterogeneous and complementary data is of benefit to the diagnosis and prevention of AD. In this paper, we apply the appropriate correlation analysis method to detect the relationships between brain regions and genes, and propose \"brain region-gene pairs\" as the multimodal features of the sample. In addition, we put forward a novel data analysis method from technology aspect, cluster evolutionary random forest (CERF), which is suitable for \"brain region-gene pairs\". The idea of clustering evolution is introduced to improve the generalization performance of random forest which is constructed by randomly selecting samples and sample features. Through hierarchical clustering of decision trees in random forest, the decision trees with higher similarity are clustered into one class, and the decision trees with the best performance are retained to enhance the diversity between decision trees. Furthermore, based on CERF, we integrate feature construction, feature selection and sample classification to find the optimal combination of different methods, and design a comprehensive diagnostic framework for AD. The framework is validated by the samples with both fMRI and SNP data from ADNI. The results show that we can effectively identify AD patients and discover some brain regions and genes associated with AD significantly based on this framework. These findings are conducive to the clinical treatment and prevention of AD.",
        "Model-based clustering with finite mixture models has become a widely used clustering method. One of the recent implementations is MCLUST. When objects to be clustered are summary statistics, such as regression coefficient estimates, they are naturally associated with estimation errors, whose covariance matrices can often be calculated exactly or approximated using asymptotic theory. This article proposes an extension to Gaussian finite mixture modeling-called MCLUST-ME-that properly accounts for the estimation errors. More specifically, we assume that the distribution of each observation consists of an underlying true component distribution and an independent measurement error distribution. Under this assumption, each unique value of estimation error covariance corresponds to its own classification boundary, which consequently results in a different grouping from MCLUST. Through simulation and application to an RNA-Seq data set, we discovered that under certain circumstances, explicitly, modeling estimation errors, improves clustering performance or provides new insights into the data, compared with when errors are simply ignored, whereas the degree of improvement depends on factors such as the distribution of error covariance matrices.",
        "BACKGROUND: RNA-Seq is the preferred method to explore transcriptomes and to estimate differential gene expression. When an organism has a well-characterized and annotated genome, reads obtained from RNA-Seq experiments can be directly mapped to that genome to estimate the number of transcripts present and relative expression levels of these transcripts. However, for unknown genomes, de novo assembly of RNA-Seq reads must be performed to generate a set of contigs that represents the transcriptome. These contig sets contain multiple transcripts, including immature mRNAs, spliced transcripts and allele variants, as well as products of close paralogs or gene families that can be difficult to distinguish. Thus, tools are needed to select a set of less redundant contigs to represent the transcriptome for downstream analyses. Here we describe the development of Compacta to produce contig sets from de novo assemblies. RESULTS: Compacta is a fast and flexible computational tool that allows selection of a representative set of contigs from de novo assemblies. Using a graph-based algorithm, Compacta groups contigs into clusters based on the proportion of shared reads. The user can determine the minimum coverage of the contigs to be clustered, as well as a threshold for the proportion of shared reads in the clustered contigs, thus providing a dynamic range of transcriptome compression that can be adapted according to experimental aims. We compared the performance of Compacta against state of the art clustering algorithms on assemblies from Arabidopsis, mouse and mango, and found that Compacta yielded more rapid results and had competitive precision and recall ratios. We describe and demonstrate a pipeline to tailor Compacta parameters to specific experimental aims. CONCLUSIONS: Compacta is a fast and flexible algorithm for the determination of optimum contig sets that represent the transcriptome for downstream analyses.",
        "Limited energy resources of sensor nodes in Wireless Sensor Networks (WSNs) make energy consumption the most significant problem in practice. This paper proposes a novel, dynamic, self-organizing Hesitant Fuzzy Entropy-based Opportunistic Clustering and data fusion Scheme (HFECS) in order to overcome the energy consumption and network lifetime bottlenecks. The asynchronous working-sleeping cycle of sensor nodes could be exploited to make an opportunistic connection between sensor nodes in heterogeneous clustering. HFECS incorporates two levels of hierarchy in the network and energy heterogeneity is characterized using three levels of energy in sensor nodes. HFECS gathers local sensory data from sensor nodes and utilizes multi-attribute decision modeling and the entropy weight coefficient method for cluster formation and the cluster head election procedure. After cluster formation, HFECS uses the same techniques for performing data fusion at the first hierarchical level to reduce the redundant information flow from the first-second hierarchical levels, which can lead to an improvement in energy consumption, better utilization of bandwidth and extension of network lifetime. Our simulation results reveal that HFECS outperforms the existing benchmark schemes of heterogeneous clustering for larger network sizes in terms of half-life period, stability period, average residual energy, network lifetime, and packet delivery ratio.",
        "BACKGROUND: Large numbers of fibers produced by fiber tractography are often grouped into bundles with anatomical interpretations. Traditional clustering methods usually generate bundles with spatial anatomic coherences only. To associate bundles with function, some studies incorporate functional connectivity of grey matter to guide clustering on the premise that fibers provide the basis of information transmission for cortex. However, functional properties along fiber tracts were ignored by these methods. Considering several recent studies showing that BOLD (Blood-Oxygen-Level Dependent) signals of white matter contain functional information of axonal fibers, this work is motivated to demonstrate that whole brain white matter fibers can be clustered with integration of functional and structural information they contain. NEW METHODS: We proposed a novel algorithm based on Gaussian mixture model and expectation maximization to achieve optimal bundling with both structural and functional coherences. The functional coherence between two fibers is defined as the average correlation in BOLD signal between corresponding points. Whole brain fibers under resting state and sensory stimulation conditions were used to demonstrate the effectiveness of the proposed technique. RESULTS: Our in vivo experiments show the robustness of proposed algorithm and influences of weights between structure and function, and repeatability of reconstructed major bundles across individuals. COMPARISON WITH EXISTING METHODS: In contrast to traditional methods, the proposed clustering method can achieve structurally more compact bundles, which are specifically related to evoking function. CONCLUSION: The proposed concept and framework can be used to identify functional pathways and their structural features under specific function loading.",
        "Cerebral Blood Flow Velocity waveforms acquired via Transcranial Doppler (TCD) can provide evidence for cerebrovascular occlusion and stenosis. Thrombolysis in Brain Ischemia (TIBI) flow grades are widely used for this purpose, but require subjective assessment by expert evaluators to be reliable. In this work we seek to determine whether TCD morphology can be objectively assessed using an unsupervised machine learning approach to waveform categorization. TCD beat waveforms were recorded at multiple depths from the Middle Cerebral Arteries of 106 subjects; 33 with Large Vessel Occlusion (LVO). From each waveform, three morphological features were extracted, quantifying onset of maximal velocity, systolic canopy length, and the number/prominence of peaks/troughs. Spectral clustering identified groups implicit in the resultant three-dimensional feature space, with gap statistic criteria establishing the optimal cluster number. We found that gap statistic disparity was maximized at four clusters, referred to as flow types I, II, III, and IV. Types I and II were primarily composed of control subject waveforms, whereas types III and IV derived mainly from LVO patients. Cluster morphologies for types I and IV aligned clearly with Normal and Blunted TIBI flows, respectively. Types II and III represented commonly observed flow-types not delineated by TIBI, which nonetheless deviate from normal and blunted flows. We conclude that important morphological variability exists beyond that currently quantified by TIBI in populations experiencing or at-risk for acute ischemic stroke, and posit that the observed flow-types provide the foundation for objective methods of real-time automated flow type classification.",
        "Genome-wide data is used to stratify patients into classes for precision medicine using clustering algorithms. A common problem in this area is selection of the number of clusters (K). The Monti consensus clustering algorithm is a widely used method which uses stability selection to estimate K. However, the method has bias towards higher values of K and yields high numbers of false positives. As a solution, we developed Monte Carlo reference-based consensus clustering (M3C), which is based on this algorithm. M3C simulates null distributions of stability scores for a range of K values thus enabling a comparison with real data to remove bias and statistically test for the presence of structure. M3C corrects the inherent bias of consensus clustering as demonstrated on simulated and real expression data from The Cancer Genome Atlas (TCGA). For testing M3C, we developed clusterlab, a new method for simulating multivariate Gaussian clusters.",
        "Machine learning (ML) techniques can be utilized by physicians, clinicians, as well as other users, to discover Autism Spectrum Disorder (ASD) symptoms based on historical cases and controls to enhance autism screening efficiency and accuracy. The aim of this study is to improve the performance of detecting ASD traits by reducing data dimensionality and eliminating redundancy in the autism dataset. To achieve this, a new semi-supervised ML framework approach called Clustering-based Autistic Trait Classification (CATC) is proposed that uses a clustering technique and that validates classifiers using classification techniques. The proposed method identifies potential autism cases based on their similarity traits as opposed to a scoring function used by many ASD screening tools. Empirical results on different datasets involving children, adolescents, and adults were verified and compared to other common machine learning classification techniques. The results showed that CATC offers classifiers with higher predictive accuracy, sensitivity, and specificity rates than those of other intelligent classification approaches such as Artificial Neural Network (ANN), Random Forest, Random Trees, and Rule Induction. These classifiers are useful as they are exploited by diagnosticians and other stakeholders involved in ASD screening.",
        "Epigenetic alteration is a fundamental characteristic of nearly all human cancers. Tumor cells not only harbor genetic alterations, but also are regulated by diverse epigenetic modifications. Identification of epigenetic similarities across different cancer types is beneficial for the discovery of treatments that can be extended to different cancers. Nowadays, abundant epigenetic modification profiles have provided a great opportunity to achieve this goal. Here, we proposed a new approach TriPCE, introducing tri-clustering strategy to integrative pan-cancer epigenomic analysis. The method is able to identify coherent patterns of various epigenetic modifications across different cancer types. To validate its capability, we applied the proposed TriPCE to analyze six important epigenetic marks among seven cancer types, and identified significant cross-cancer epigenetic similarities. These results suggest that specific epigenetic patterns indeed exist among these investigated cancers. Furthermore, the gene functional analysis performed on the associated gene sets demonstrates strong relevance with cancer development and reveals consistent risk tendency among these investigated cancer types.",
        "BACKGROUND: Agriculture is one of the most essential industry that fullfills people's need and also plays an important role in economic evolution of the nation. However, there is a gap between the agriculture sector and the technological industry and the agriculture plants are mostly affected by diseases, such as the bacterial, fungus and viral diseases that lead to loss in crop yield. The affected parts of the plants need to be identified at the beginning stage to eliminate the huge loss in productivity. METHODS: In the present scenario, crop cultivation system depend on the farmers experience and the man power, but it consumes more time and increases error rate. To overcome this issue, the proposed system introduces the Double Line Clustering technique based disease identification system using the image processing and data mining methods. The introduced method analyze the Anthracnose, blight disease in grapes, tomato and cucumber. The leaf images are captured and the noise has been removed by non-local median filter and the segmentation is done by double line clustering method. The segmented part compared with diseased leaf using pattern matching algorithm. RESULTS: The efficiency of the system is implemented in tomato, grape, cucumber plants leaf images and the results are analyzed in terms of the error rate, sensitivity, specificity, accuracy and time. CONCLUSION: The result of the clustering algorithm achieved high accuracy, sensitivity, and specificity. The feature extraction is applied after the clustering process which produces minimum error rate.",
        "Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.",
        "BACKGROUND: Subjective classification of gait pattern in children with cerebral palsy depends on the assessor's experience, while mathematical methods produce virtual groups with no clinical interpretation. METHODS: In a retrospective study, gait data from 66 children (132 limbs) with a mean age of 9.6 (SD 3.7) years with cerebral palsy and no history of surgery or botulinum toxin injection were reviewed. The gait pattern of each limb was classified in four groups according to Rodda using three methods: 1) a team of experts subjectively assigning a gait pattern, 2) using the plantarflexor-knee extension couple index introduced by Sangeux et al., and 3) employing a fuzzy algorithm to translate the experiences of experts into objective rules and execute a clustering tool. To define fuzzy repeated-measures, 75% of the members in each group were used, and the remaining were used for validation. Eight parameters were objectively extracted from kinematic data for each group and compared using repeated measure ANOVA and post-hoc analysis was performed. Finally, the results of the clustering of the latter two methods were compared to the subjective method. FINDINGS: The plantarflexor-knee extension couple index achieved 86% accuracy while the fuzzy system yielded a 98% accuracy. The most substantial errors occurred between jump and apparent in both methods. INTERPRETATION: The presented method is a fast, reliable, and objective fuzzy clustering system to classify gait patterns in cerebral palsy, which produces clinically-relevant results. It can provide a universal common language for researchers.",
        "Identification of novel molecular subtypes of disease using multi-source 'omics data is an active area of on-going research. Integrative clustering is a powerful approach to identify latent subtype structure inherent in the data sets accounting for both between and within data correlations. We propose a new integrative network-based clustering method using the non-negative matrix factorization, nNMF, for clustering multiple types of interrelated datasets assayed on same tumor-samples. nNMF utilizes the consensus matrices generated using the non-negative matrix factorization (NMF) algorithm on each type of data as networks among the patient samples. The multiple networks are then combined, and a comprehensive network structure is created optimizing the strengths of the relationships. A spectral clustering algorithm is then used on the final network data to determine the cluster groups. nNMF is a non-parametric method and therefore prior assumptions on the statistical distribution of data is not required. The application of the proposed nNMF method has been provided with simulated and the real-life datasets obtained from The Cancer Genome Atlas studies on glioblastoma, lower grade glioma and head and neck cancer. nNMF was found to be working competitively with previous methods and sometimes better as compared to previous NMF or model-based method especially when the signal to noise ratio is small. The novel nNMF method allows researchers to utilize such relationships to identify the latent subtype structure inherent in the data so that further association studies can be carried out. The R program for the nNMF will be available upon request.",
        "Gene regulatory network Inference with high accuracy based on gene expression data sets is one of the most challenging problems in computational biology. To improve the accuracy of gene regulatory network inference and find hub genes, we proposed a novel model integration network inference method with clustering and hub genes finding called MINICHG. The method is divided into three main steps: (1) using single models inference results based on three machine learning algorithms to construct feature matrix; (2) using k-means to cluster gene pairs according to feature matrix; (3) hub genes finding. MINICHG integrates RF(Random Forest), GBDT (Gradient Boosting Decision Tree) and Pearson Correlation results with a novel weighted strategy in a semi-unsupervised way. The designed optimization scheme in MINICHG considering sparse gold standard data characteristics is suitable for most gene regulatory network reconstruction. We evaluated the proposed method on simulated data sets from five Dream4 multifactorial data sets and Dream5 in silico data set and real data set from E.coli. The performance was better than other network inference methods with high accuracy and robustness.",
        "BACKGROUND AND OBJECTIVES: Brain MR images consist of three major regions: gray matter, white matter and cerebrospinal fluid. Medical experts make decisions on different serious diseases by evaluating the developments in these areas. One of the significant approaches used in analyzing the MR images were segmenting the regions. However, their segmentation suffers from two major problems as: (a) the boundaries of their gray matter and white matter regions are ambiguous in nature, and (b) their regions are formed with unclear inhomogeneous gray structures. For these reasons, diagnosis of critical diseases is often very difficult. METHODS: This study presented a new method for MR image segmentation, which consisted of two main parts as: (a) neutrosophic-entropy based clustering algorithm (NEBCA), and (b) HSV color system. The NEBCA's role in this study was to perform segmentation of MR regions, while HSV color system was used to provide better visual representation of features in segmented regions. RESULTS: Application of the proposed method was demonstrated in 30 different MR images of Parkinson's disease (PD). Experimental results were presented individually for the NEBCA and HSV color system. The performance of the proposed method was evaluated in terms of statistical metrics used in an image segmentation domain. Experimental results, including statistical analysis reflected the efficiency of the proposed method over the existing well-known image segmentation methods available in literature. For the proposed method and existing methods, the average CPU time (in nanosecond) was computed and it was found that the proposed method consumed less time to segment MR images. CONCLUSION: The proposed method can effectively segment different regions of MR images and can very clearly represent those segmented regions.",
        "Information theory is a branch of mathematics that overlaps with communications, biology, and medical engineering. Entropy is a measure of uncertainty in the set of information. In this study, for each gene and its exons sets, the entropy was calculated in orders one to four. Based on the relative entropy of genes and exons, Kullback-Leibler divergence was calculated. After obtaining the Kullback-Leibler distance for genes and exons sets, the results were entered as input into 7 clustering algorithms: single, complete, average, weighted, centroid, median, and K-means. To aggregate the results of clustering, the AdaBoost algorithm was used. Finally, the results of the AdaBoost algorithm were investigated by GeneMANIA prediction server to explore the results from gene annotation point of view. All calculations were performed using the MATLAB Engineering Software (2015). Following our findings on investigating the results of genes metabolic pathways based on the gene annotations, it was revealed that our proposed clustering method yielded correct, logical, and fast results. This method at the same that had not had the disadvantages of aligning allowed the genes with actual length and content to be considered and also did not require high memory for large-length sequences. We believe that the performance of the proposed method could be used with other competitive gene clustering methods to group biologically relevant set of genes. Also, the proposed method can be seen as a predictive method for those genes bearing up weak genomic annotations.",
        "MOTIVATION: New single-cell technologies continue to fuel the explosive growth in the scale of heterogeneous single-cell data. However, existing computational methods are inadequately scalable to large datasets and therefore cannot uncover the complex cellular heterogeneity. RESULTS: We introduce a highly scalable graph-based clustering algorithm PARC-Phenotyping by Accelerated Refined Community-partitioning-for large-scale, high-dimensional single-cell data (>1 million cells). Using large single-cell flow and mass cytometry, RNA-seq and imaging-based biophysical data, we demonstrate that PARC consistently outperforms state-of-the-art clustering algorithms without subsampling of cells, including Phenograph, FlowSOM and Flock, in terms of both speed and ability to robustly detect rare cell populations. For example, PARC can cluster a single-cell dataset of 1.1 million cells within 13 min, compared with >2 h for the next fastest graph-clustering algorithm. Our work presents a scalable algorithm to cope with increasingly large-scale single-cell analysis. AVAILABILITY AND IMPLEMENTATION: https://github.com/ShobiStassen/PARC. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Algorithm benchmarking and characterization are an important part of algorithm development and validation prior to clinical implementation. However, benchmarking may be limited to a small collection of test cases due to the resource-intensive nature of establishing 'ground-truth' references. This study proposes a framework for selecting test cases to assess algorithm and workflow equivalence. Effective test case selection may minimize the number of ground-truth comparisons required to establish robust and clinically relevant benchmarking and characterization results. To demonstrate the proposed framework, we clustered differences between two independent workflows estimating during-treatment dose objective violations for 15 head and neck cancer patients (15 planning CTs, 105 on-unit CBCTs). Each workflow used a different deformable image registration algorithm to estimate inter-fractional anatomy and contour changes. The Hopkins statistic tested whether workflow output was inherently clustered and k-medoid clustering formalized cluster assignment. Further statistical analyses verified the relevance of clusters to algorithm output. Data at cluster centers ('medoids') were considered as candidate test cases representative of workflow-relevant algorithm differences. The framework indicated that differences in estimated dose objective violations were naturally grouped (Hopkins = 0.75, providing 90% confidence). K-medoid clustering identified five clusters which stratified workflow differences (MANOVA: p < 0.001) in estimated parotid gland D50%, spinal cord/brainstem Dmax, and high dose CTV coverage dose violations (Kendall's tau: p < 0.05). Systematic algorithm differences resulting in workflow discrepancies were: parotid gland volumes (ANOVA: p < 0.001), external contour deformations (t-test: p = 0.022), and CTV-to-PTV margins (t-test: 0.009), respectively. Five candidate test cases were verified as representative of the five clusters. The framework successfully clustered workflow outputs and identified five test cases representative of clinically relevant algorithm discrepancies. This approach may improve the allocation of resources during the benchmarking and characterization process and the applicability of results to clinical data.",
        "Cancer subtype analysis, as an extension of cancer diagnosis, can be regarded as a consensus clustering problem. This analysis is beneficial for providing patients with more accurate treatment. Consensus clustering refers to a situation in which several different clusters have been obtained for a particular data set, and it is desired to aggregate those clustering results to get a better clustering solution. In this paper, we propose to generalize the traditional consensus clustering methods in three manners: (1) We provide Bregmannian consensus clustering (BCC), where the loss between the consensus clustering result and all the input clusterings are generalized from a traditional Euclidean distance to a general Bregman loss; (2) we generalize the BCC to a weighted case, where each input clustering has different weights, providing a better solution for the final clustering result; and (3) we propose a novel semi-supervised consensus clustering, which adds some must-link and cannot-link constraints compared with the first two methods. Then, we obtain three cancer (breast, lung, colorectal cancer) data sets from The Cancer Genome Atlas (TCGA). Each data set has three data types (mRNA, mircoRNA, methylation), and each is respectively used to test the accuracy of the proposed algorithms for clusterings. The experimental results demonstrate that the highest aggregation accuracy of the weighted BCC (WBCC) on cancer data sets is 90.2%. Moreover, although the lowest accuracy is 62.3%, it is higher than other methods on the same data set. Therefore, we conclude that as compared with the competition, our method is more effective.",
        "In the field of computational bioinformatics, identifying a set of genes which are responsible for a particular cellular mechanism, is very much essential for tasks such as medical diagnosis or disease gene identification. Accurately grouping (clustering) the genes is one of the important tasks in understanding the functionalities of the disease genes. In this regard, ensemble clustering becomes a promising approach to combine different clustering solutions to generate almost accurate gene partitioning. Recently, researchers have used generative model as a smart ensemble method to produce the right consensus solution. In the current paper, we develop a protein-protein interaction-based generative model that can efficiently perform a gene clustering. Utilizing protein interaction information as the generative model's latent variable enables enhance the generative model's efficiency in inferring final probabilistic labels. The proposed generative model utilizes different weak supervision sources rather utilizing any ground truth information. For weak supervision sources, we use a multi-objective optimization based clustering technique together with the world's largest gene ontology based knowledge-base named Gene Ontology Consortium(GOC). These weakly supervised labels are supplied to a generative model that eventually assigns all genes to probabilistic labels. The comparative study with respect to silhouette score, Biological Homogeneity Index (BHI) and Biological Stability Index (BSI) proves that the proposed generative model outperforms than other state-of-the-art techniques.",
        "BACKGROUND: We re-evaluate our RNA-As-Graphs clustering approach, using our expanded graph library and new RNA structures, to identify potential RNA-like topologies for design. Our coarse-grained approach represents RNA secondary structures as tree and dual graphs, with vertices and edges corresponding to RNA helices and loops. The graph theoretical framework facilitates graph enumeration, partitioning, and clustering approaches to study RNA structure and its applications. METHODS: Clustering graph topologies based on features derived from graph Laplacian matrices and known RNA structures allows us to classify topologies into 'existing' or hypothetical, and the latter into, 'RNA-like' or 'non RNA-like' topologies. Here we update our list of existing tree graph topologies and RAG-3D database of atomic fragments to include newly determined RNA structures. We then use linear and quadratic regression, optionally with dimensionality reduction, to derive graph features and apply several clustering algorithms on our tree-graph library and recently expanded dual-graph library to classify them into the three groups. RESULTS: The unsupervised PAM and K-means clustering approaches correctly classify 72-77% of all existing graph topologies and 75-82% of newly added ones as RNA-like. For supervised k-NN clustering, the cross-validation accuracy ranges from 57 to 81%. CONCLUSIONS: Using linear regression with unsupervised clustering, or quadratic regression with supervised clustering, provides better accuracies than supervised/linear clustering. All accuracies are better than random, especially for newly added existing topologies, thus lending credibility to our approach. GENERAL SIGNIFICANCE: Our updated RAG-3D database and motif classification by clustering present new RNA substructures and RNA-like motifs as novel design candidates.",
        "BACKGROUND: The similarity or distance measure used for clustering can generate intuitive and interpretable clusters when it is tailored to the unique characteristics of the data. In time series datasets generated with high-throughput biological assays, measurements such as gene expression levels or protein phosphorylation intensities are collected sequentially over time, and the similarity score should capture this special temporal structure. RESULTS: We propose a clustering similarity measure called Lag Penalized Weighted Correlation (LPWC) to group pairs of time series that exhibit closely-related behaviors over time, even if the timing is not perfectly synchronized. LPWC aligns time series profiles to identify common temporal patterns. It down-weights aligned profiles based on the length of the temporal lags that are introduced. We demonstrate the advantages of LPWC versus existing time series and general clustering algorithms. In a simulated dataset based on the biologically-motivated impulse model, LPWC is the only method to recover the true clusters for almost all simulated genes. LPWC also identifies clusters with distinct temporal patterns in our yeast osmotic stress response and axolotl limb regeneration case studies. CONCLUSIONS: LPWC achieves both of its time series clustering goals. It groups time series with correlated changes over time, even if those patterns occur earlier or later in some of the time series. In addition, it refrains from introducing large shifts in time when searching for temporal patterns by applying a lag penalty. The LPWC R package is available at https://github.com/gitter-lab/LPWC and CRAN under a MIT license.",
        "Internet of Things (IoT) facilitates a wide range of applications through sensor-based connected devices that require bandwidth and other network resources. Enhancement of efficient utilization of a heterogeneous IoT network is an open optimization problem that is mostly suffered by network flooding. Redundant, unwanted, and flooded queries are major causes of inefficient utilization of resources. Several query control mechanisms in the literature claimed to cater to the issues related to bandwidth, cost, and Quality of Service (QoS). This research article presented a statistical performance evaluation of different query control mechanisms that addressed minimization of energy consumption, energy cost and network flooding. Specifically, it evaluated the performance measure of Query Control Mechanism (QCM) for QoS-enabled layered-based clustering for reactive flooding in the Internet of Things. By statistical means, this study inferred the significant achievement of the QCM algorithm that outperformed the prevailing algorithms, i.e., Divide-and-Conquer (DnC), Service Level Agreements (SLA), and Hybrid Energy-aware Clustering Protocol for IoT (Hy-IoT) for identification and elimination of redundant flooding queries. The inferential analysis for performance evaluation of algorithms was measured in terms of three scenarios, i.e., energy consumption, delays and throughput with different intervals of traffic, malicious mote and malicious mote with realistic condition. It is evident from the results that the QCM algorithm outperforms the existing algorithms and the statistical probability value \"P\" < 0.05 indicates the performance of QCM is significant at the 95% confidence interval. Hence, it could be inferred from findings that the performance of the QCM algorithm was substantial as compared to that of other algorithms.",
        "Brain Tissue Segmentation (BTS) in young children and neonates is not a trivial task due to peculiar characteristics of the developing brain. The aim of this study is to present the preliminary results of new atlas-free BTS (afBTS) algorithm of MR images for pediatric applications, based on clustering. The algorithm works on axial T1, T2 and FLAIR sequences. First, the Cerebrospinal Fluid (CSF) is identified using the Region Growing algorithm. The remaining voxels are processed with the k-means algorithm in order to separate White Matter (WM) and Grey Matter (GM). The afBTS algorithm was applied to a population of 13 neonates; the segmentations were evaluated by two expert pediatric neuroradiologists and compared with an atlas-based algorithm. The results were promising: afBTS allowed reconstruction of WM and CSF with an image quality comparable to the reference of standard while lower segmentation quality was obtained for the GM segmentation.",
        "The analysis and interpretation of physiological signals acquired non-invasively are increasingly important in Smart Health, precision medicine, and medical research. However, this analysis is hampered due to the length, complexity, and inter-subject variation of these signals, and, consequently, dimensionality reduction and clustering offer substantial benefits. Machine learning, used widely in biomedicine, is increasingly being applied to physiological time series. Among the applications of unsupervised learning, clustering is one of the most important. In this paper, an unsupervised autoen-coder architecture, deep convolutional embedded clustering, is presented as a data-driven approach to study time-frequency characteristics of heart rate variability records. An autoen-coder network is trained on continuous wavelet transforms of heart rate variability signals calculated from publicly-available annotated ECG records with a wide variety of conditions. The latent variables learned by the clustering autoencoder are low-dimensional representations of wavelet transform characteristics that can be visualized and further analyzed. The results indicate that the learned clusters correspond to beat morphologies in the electrocardiogram in many cases, but also that the reduced dimensions of the time-frequency features can potentially provide additional insights into cardiac activity and the autonomic nervous system.",
        "In this paper, we propose a clustering method with temporal ordering information for endoscopic image sequences. It is difficult to collect a sufficient amount of endoscopic image datasets to train machine learning techniques by manual labeling. The clustering of endoscopic images leads to group-based labeling, which is useful for reducing the cost of dataset construction. Therefore, in this paper, we propose a clustering method where the property of endoscopic image sequences is fully utilized. For the proposed method, a deep neural network was used to extract features from endoscopic images, and clustering with temporal ordering information was solved by dynamic programming. In the experiments, we clustered the esophagogastroduodenoscopy images. From the results, we confirmed that the performance was improved by using the sequential property.",
        "Biomedical signal analysis often depends on methods to detect and distinguish abnormal or high noise/artifact signal from normal signal. A novel unsupervised clustering method suitable for resource constrained embedded computing contexts, classifies arterial blood pressure (ABP) beat cycles as normal or abnormal. A cycle detection algorithm delineates beat cycles, so that each cycle can be modeled by a continuous time Fourier series decomposition. The Fourier series parameters are a discrete vector representation for the cycle along with the cycle period. The sequence of cycle parameter vectors is a non-uniform discrete time representation for the ABP signal that provides feature input for a clustering algorithm. Clustering uses a weighted distance function of normalized cycle parameters to ignore cycle differences due to natural and expected physiological modulations, such as respiratory modulation, while accounting for differences due to other causes, such as patient movement artifact. Challenging cardiac surgery patient signal examples indicate effectiveness.",
        "Autism spectrum disorder (ASD) is a lifelong condition characterized by social and communication impairments. This study attempts to apply unsupervised Machine Learning to discover clusters in ASD. The key idea is to learn clusters based on the visual representation of eye-tracking scanpaths. The clustering model was trained using compressed representations learned by a deep autoencoder. Our experimental results demonstrate a promising tendency of clustering structure. Further, the clusters are explored to provide interesting insights into the characteristics of the gaze behavior involved in autism.",
        "To minimize the occurrence of ulcers in high-risk mobile individuals such as wheelchair users, it is necessary to detect all typical distribution patterns and to indentify the patterns that may be associated with pressure ulcers. However, pattern detection is difficult because the pressure distribution during motion includes a variety of patterns compared to those of static postures. Thus, the establishment of a method to detect typical patterns based on distribution patterns is important. We utilized deep embedded clustering for identification purposes. This clustering technique extracts features using auto-encoder and simultaneously optimizes data points into the clusters, which might realize good clustering performance due to the detected optimal features. We used a pressure distribution dataset that was pre-labeled by nursing experts. The dataset consisted of a total of 26944 distribution images with ten class annotations. The clustering method including traditional approaches (k-means and principal component analysis plus k-means) were compared with deep embedded clustering while the threshold to noise reduction was changing. The deep embedded clustering with 80 mmHg threshold achieved the best performance. This approach also tended to be less dependent on the threshold values.",
        "Clustering analysis is employed in brain dynamic functional connectivity to cluster the data into a set of dynamic states. These states correspond to different patterns of functional connectivity that iterate through time. Although several methods to determine the best clustering partition exists, the appropriateness of methods to apply in the case of dynamic connectivity analysis has not been determined. In this work we examine the use of the Davies-Bouldin clustering validity index via simulation and real data analysis. Currently employed indexes, such as the Silhouette index, do not provide an effective estimation requiring the use of an elbow criterion. All elbow criteria rely on users experience and introduce uncertainty into the estimation. We demonstrate the feasibility of using the Davies-Bouldin index as a method delivering a unique discrete response to provide automated selection of the number of clusters.",
        "This paper presents the combination of clustering-based independent component analysis (ICASSO) and power spectral density (PSD) as a features extractor of mental fatigue from spinal cord injury (SCI) patients. Initially, the results show that SCI and abled-bodied groups have no differences in EEG for alert and mental fatigue states. Further, the coefficient determination (R(2)) is calculated for testing the variation of data alert vs. fatigue on the SCI group, resulting in a lower R(2) for proposed combination of ICASSO and PSD method compared to the PSD method only. With the lower R(2) values, this shows that the proposed method ICASSO and PSD is able to provide superior distinction for separating fatigue vs. alert data variation. The statistical significance is found across four EEG bands and EEG channels.",
        "Time-series clustering is the process of grouping time series with respect to their similarity or characteristics. Previous approaches usually combine a specific distance measure for time series and a standard clustering method. However, these approaches do not take the similarity of the different subsequences of each time series into account, which can be used to better compare the time-series objects of the dataset. In this article, we propose a novel technique of time-series clustering consisting of two clustering stages. In a first step, a least-squares polynomial segmentation procedure is applied to each time series, which is based on a growing window technique that returns different-length segments. Then, all of the segments are projected into the same dimensional space, based on the coefficients of the model that approximates the segment and a set of statistical features. After mapping, a first hierarchical clustering phase is applied to all mapped segments, returning groups of segments for each time series. These clusters are used to represent all time series in the same dimensional space, after defining another specific mapping process. In a second and final clustering stage, all the time-series objects are grouped. We consider internal clustering quality to automatically adjust the main parameter of the algorithm, which is an error threshold for the segmentation. The results obtained on 84 datasets from the UCR Time Series Classification Archive have been compared against three state-of-the-art methods, showing that the performance of this methodology is very promising, especially on larger datasets.",
        "The task-based approach is a parallelization paradigm in which an algorithm is transformed into a direct acyclic graph of tasks: the vertices are computational elements extracted from the original algorithm and the edges are dependencies between those. During the execution, the management of the dependencies adds an overhead that can become significant when the computational cost of the tasks is low. A possibility to reduce the makespan is to aggregate the tasks to make them heavier, while having fewer of them, with the objective of mitigating the importance of the overhead. In this paper, we study an existing clustering/partitioning strategy to speed up the parallel execution of a task-based application. We provide two additional heuristics to this algorithm and perform an in-depth study on a large graph set. In addition, we propose a new model to estimate the execution duration and use it to choose the proper granularity. We show that this strategy allows speeding up a real numerical application by a factor of 7 on a multi-core system.",
        "Single-cell RNA-seq (scRNAseq) is a powerful tool to study heterogeneity of cells. Recently, several clustering based methods have been proposed to identify distinct cell populations. These methods are based on different statistical models and usually require to perform several additional steps, such as preprocessing or dimension reduction, before applying the clustering algorithm. Individual steps are often controlled by method-specific parameters, permitting the method to be used in different modes on the same datasets, depending on the user choices. The large number of possibilities that these methods provide can intimidate non-expert users, since the available choices are not always clearly documented. In addition, to date, no large studies have invistigated the role and the impact that these choices can have in different experimental contexts. This work aims to provide new insights into the advantages and drawbacks of scRNAseq clustering methods and describe the ranges of possibilities that are offered to users. In particular, we provide an extensive evaluation of several methods with respect to different modes of usage and parameter settings by applying them to real and simulated datasets that vary in terms of dimensionality, number of cell populations or levels of noise. Remarkably, the results presented here show that great variability in the performance of the models is strongly attributed to the choice of the user-specific parameter settings. We describe several tendencies in the performance attributed to their modes of usage and different types of datasets, and identify which methods are strongly affected by data dimensionality in terms of computational time. Finally, we highlight some open challenges in scRNAseq data clustering, such as those related to the identification of the number of clusters.",
        "The accelerating growth of the public microbial genomic data imposes substantial burden on the research community that uses such resources. Building databases for non-redundant reference sequences from massive microbial genomic data based on clustering analysis is essential. However, existing clustering algorithms perform poorly on long genomic sequences. In this article, we present Gclust, a parallel program for clustering complete or draft genomic sequences, where clustering is accelerated with a novel parallelization strategy and a fast sequence comparison algorithm using sparse suffix arrays (SSAs). Moreover, genome identity measures between two sequences are calculated based on their maximal exact matches (MEMs). In this paper, we demonstrate the high speed and clustering quality of Gclust by examining four genome sequence datasets. Gclust is freely available for non-commercial use at https://github.com/niu-lab/gclust. We also introduce a web server for clustering user-uploaded genomes at http://niulab.scgrid.cn/gclust.",
        "Face clustering is the task of grouping unlabeled face images according to individual identities. Several applications require this type of clustering, for instance, social media, law enforcement, and surveillance applications. In this paper, we propose an effective graph-based method for clustering faces in the wild. The proposed algorithm does not require prior knowledge of the data. This fact increases the pertinence of the proposed method near to market solutions. The experiments conducted on four well-known datasets showed that our proposal achieves state-of-the-art results, regarding the clustering performance, also showing stability for different values of the input parameter. Moreover, in these experiments, it is shown that our proposal discovers a number of identities closer to the real number existing in the data.",
        "A common trait of the more established clustering algorithms such as K-Means and HCA is their tendency to focus mainly on the bulk features of the data which causes minor features to be attributed to larger clusters. For hyperspectral imaging this has the consequence that substances which are covered by only a few pixels tend to be overlooked and thus cannot be separated. If small lateral features such as particles are the research objective this might be the reason why cluster analysis fails. Therefore we propose a novel graph-based clustering algorithm dubbed GBCC which is sensitive to small variations in data density and scales its clusters according to the underlying structures. The analysis of the proposed method covers a comparison to K-Means, DBSCAN and KNSC using a 2D artificial dataset. Further the method is evaluated on a multisensor image of atmospheric particulate matter composed of Raman and EDX data as well as an FTIR image of microplastics.",
        "Objective: To define the SLE phenotype associated with pulmonary hypertension using multiple autoantibodies. Methods: 207 (8%) patients with SLE with pulmonary hypertension, defined as a right ventricular systolic pressure greater than 40 mm Hg on transthoracic echocardiogram or as pulmonary artery dilatation on CT of the chest, were identified from the Hopkins Lupus Cohort (94.2% female; 56.5% African-American, 39% Caucasian; mean age 45.6 years). 53 patients were excluded from the clustering analysis due to incomplete autoantibody profiles. Agglomerative hierarchical clustering algorithm with Ward's method was used to cluster the patients with pulmonary hypertension, based on their autoantibodies. Autoantibodies used in the clustering analysis included lupus anticoagulant, anticardiolipin, anti-beta 2 glycoprotein I, antidouble-stranded DNA, anti-Sm (anti-Smith), antiribonucleoprotein, false positive-rapid plasma reagin, anti-Ro, anti-La and hypocomplementaemia (C3 ever low or C4 ever low). The Dunn index was used to internally validate the clusters. Bootstrap resampling derived the mean Jaccard coefficient for each cluster. All analyses were performed in R V.3.6.1 using the packages cluster, fpc and gplots. Results: A significantly higher prevalence of pulmonary hypertension in African-American patients with SLE, compared with Caucasian patients with SLE (11.5% vs 5.9%, p<0.0001), was found. Based on equivalent Dunn indices, the 154 patients with SLE-associated pulmonary hypertension with complete autoantibody data were divided into five clusters, three of which had mean Jaccard coefficients greater than 0.6. Hypocomplementaemia, renal disorder and age at diagnosis significantly differed across clusters. One cluster was defined by antiphospholipid antibodies. One cluster was defined by anti-Ro and anti-La. One cluster had low frequencies of all antibodies. Conclusion: SLE-associated pulmonary hypertension disproportionately affects African-American patients. Pulmonary hypertension in SLE is defined by five autoantibody clusters. Antiphospholipid antibodies, anti-Ro and anti-La positivity, serological activity, and age at pulmonary hypertension diagnosis significantly differed across clusters, possibly indicating different pathophysiological mechanisms.",
        "In this paper, a novel multi-sensor clustering algorithm, based on the density peaks clustering (DPC) algorithm, is proposed to address the multi-sensor data fusion (MSDF) problem. The MSDF problem is raised in the multi-sensor target detection (MSTD) context and corresponds to clustering observations of multiple sensors, without prior information on clutter. During the clustering process, the data points from the same sensor cannot be grouped into the same cluster, which is called the cannot link (CL) constraint; the size of each cluster should be within a certain range; and overlapping clusters (if any) must be divided into multiple clusters to satisfy the CL constraint. The simulation results confirm the validity and reliability of the proposed algorithm.",
        "Traditional clustering algorithms for medical image segmentation can only achieve satisfactory clustering performance under relatively ideal conditions, in which there is adequate data from the same distribution, and the data is rarely disturbed by noise or outliers. However, a sufficient amount of medical images with representative manual labels are often not available, because medical images are frequently acquired with different scanners (or different scan protocols) or polluted by various noises. Transfer learning improves learning in the target domain by leveraging knowledge from related domains. Given some target data, the performance of transfer learning is determined by the degree of relevance between the source and target domains. To achieve positive transfer and avoid negative transfer, a negative-transfer-resistant mechanism is proposed by computing the weight of transferred knowledge. Extracting a negative-transfer-resistant fuzzy clustering model with a shared cross-domain transfer latent space (called NTR-FC-SCT) is proposed by integrating negative-transfer-resistant and maximum mean discrepancy (MMD) into the framework of fuzzy c-means clustering. Experimental results show that the proposed NTR-FC-SCT model outperformed several traditional non-transfer and related transfer clustering algorithms.",
        "In the context of recent deep clustering studies, discriminative models dominate the literature and report the most competitive performances. These models learn a deep discriminative neural network classifier in which the labels are latent. Typically, they use multinomial logistic regression posteriors and parameter regularization, as is very common in supervised learning. It is generally acknowledged that discriminative objective functions (e.g., those based on the mutual information or the KL divergence) are more flexible than generative approaches (e.g., K-means) in the sense that they make fewer assumptions about the data distributions and, typically, yield much better unsupervised deep learning results. On the surface, several recent discriminative models may seem unrelated to K-means. This study shows that these models are, in fact, equivalent to K-means under mild conditions and common posterior models and parameter regularization. We prove that, for the commonly used logistic regression posteriors, maximizing the L2 regularized mutual information via an approximate alternating direction method (ADM) is equivalent to minimizing a soft and regularized K-means loss. Our theoretical analysis not only connects directly several recent state-of-the-art discriminative models to K-means, but also leads to a new soft and regularized deep K-means algorithm, which yields competitive performance on several image clustering benchmarks.",
        "Super-resolution imaging techniques have largely improved our capabilities to visualize nanometric structures in biological systems. Their application further permits the quantitation relevant parameters to determine the molecular organization and stoichiometry in cells. However, the inherently stochastic nature of fluorescence emission and labeling strategies imposes the use of dedicated methods to accurately estimate these parameters. Here, we describe a Bayesian approach to precisely quantitate the relative abundance of molecular aggregates of different stoichiometry from segmented images. The distribution of proxies for the number of molecules in a cluster, such as the number of localizations or the fluorescence intensity, is fitted via a nested sampling algorithm to compare mixture models of increasing complexity and thus determine the optimum number of mixture components and their weights. We test the performance of the algorithm on in silico data as a function of the number of data points, threshold, and distribution shape. We compare these results to those obtained with other statistical methods, showing the improved performance of our approach. Our method provides a robust tool for model selection in fitting data extracted from fluorescence imaging, thus improving the precision of parameter determination. Importantly, the largest benefit of this method occurs for small-statistics or incomplete datasets, enabling an accurate analysis at the single image level. We further present the results of its application to experimental data obtained from the super-resolution imaging of dynein in HeLa cells, confirming the presence of a mixed population of cytoplasmic single motors and higher-order structures.",
        "Taxonomic profiling, using hyper-variable regions of 16S rRNA, is one of the important goals in metagenomics analysis. Operational taxonomic unit (OTU) clustering algorithms are the important tools to perform taxonomic profiling by grouping 16S rRNA sequence reads into OTU clusters. Presently various OTU clustering algorithms are available within different pipelines, even some pipelines have implemented more than one clustering algorithms, but there is less literature available for the relative performance and features of these algorithms. This makes the choice of using these methods unclear. In this study five current state-of-the-art OTU clustering algorithms (CDHIT, Mothur's Average Neighbour, SUMACLUST, Swarm, and UCLUST) have been comprehensively evaluated on the metagenomics sequencing data. It was found that in all the datasets, Mothur's average neighbour and Swarm created more number of OTU clusters. Based on normalized mutual information (NMI) and normalized information difference (NID), Swarm and Mothur's average neighbour showed better clustering qualities than others. But in terms of time complexity the greedy algorithms (SUMACLUST, CDHIT, and UCLUST) performed well. So there is a trade-off between quality and time, and it is necessary while analysing large size of 16S rRNA gene sequencing data.",
        "Cluster analysis plays a significant role regarding automating such a knowledge discovery process in spatial data mining. A good clustering algorithm supports two essential conditions, namely high intra-cluster similarity and low inter-cluster similarity. Maximized intra-cluster/within-cluster similarity produces low distances between data points inside the same cluster. However, minimized inter-cluster/between-cluster similarity increases the distance between data points in different clusters by furthering them apart from each other. We previously presented a spatial clustering algorithm, abbreviated CutESC (Cut-Edge for Spatial Clustering) with a graph-based approach. The data presented in this article is related to and supportive to the research paper entitled \"CutESC: Cutting edge spatial clustering technique based on proximity graphs\" (Aksac et al., 2019) [1], where interpretation research data presented here is available. In this article, we share the parametric version of our algorithm named CutESC-P, the best parameter settings for the experiments, the additional analyses and some additional information related to the proposed algorithm (CutESC) in [1].",
        "In many research areas scientists are interested in clustering objects within small datasets while making use of prior knowledge from large reference datasets. We propose a method to apply the machine learning concept of transfer learning to unsupervised clustering problems and show its effectiveness in the field of single-cell RNA sequencing (scRNA-Seq). The goal of scRNA-Seq experiments is often the definition and cataloguing of cell types from the transcriptional output of individual cells. To improve the clustering of small disease- or tissue-specific datasets, for which the identification of rare cell types is often problematic, we propose a transfer learning method to utilize large and well-annotated reference datasets, such as those produced by the Human Cell Atlas. Our approach modifies the dataset of interest while incorporating key information from the larger reference dataset via Non-negative Matrix Factorization (NMF). The modified dataset is subsequently provided to a clustering algorithm. We empirically evaluate the benefits of our approach on simulated scRNA-Seq data as well as on publicly available datasets. Finally, we present results for the analysis of a recently published small dataset and find improved clustering when transferring knowledge from a large reference dataset. Implementations of the method are available at https://github.com/nicococo/scRNA.",
        "BACKGROUND: Patient stratification is a critical task in clinical decision making since it can allow physicians to choose treatments in a personalized way. Given the increasing availability of electronic medical records (EMRs) with longitudinal data, one crucial problem is how to efficiently cluster the patients based on the temporal information from medical appointments. In this work, we propose applying the Temporal Needleman-Wunsch (TNW) algorithm to align discrete sequences with the transition time information between symbols. These symbols may correspond to a patient's current therapy, their overall health status, or any other discrete state. The transition time information represents the duration of each of those states. The obtained TNW pairwise scores are then used to perform hierarchical clustering. To find the best number of clusters and assess their stability, a resampling technique is applied. RESULTS: We propose the AliClu, a novel tool for clustering temporal clinical data based on the TNW algorithm coupled with clustering validity assessments through bootstrapping. The AliClu was applied for the analysis of the rheumatoid arthritis EMRs obtained from the Portuguese database of rheumatologic patient visits (Reuma.pt). In particular, the AliClu was used for the analysis of therapy switches, which were coded as letters corresponding to biologic drugs and included their durations before each change occurred. The obtained optimized clusters allow one to stratify the patients based on their temporal therapy profiles and to support the identification of common features for those groups. CONCLUSIONS: The AliClu is a promising computational strategy to analyse longitudinal patient data by providing validated clusters and by unravelling the patterns that exist in clinical outcomes. Patient stratification is performed in an automatic or semi-automatic way, allowing one to tune the alignment, clustering, and validation parameters. The AliClu is freely available at https://github.com/sysbiomed/AliClu.",
        "BACKGROUND: Advanced non-invasive neuroimaging techniques offer new approaches to study functions and structures of human brains. Whole-brain functional networks obtained from resting state functional magnetic resonance imaging has been widely used to study brain diseases like autism spectrum disorder (ASD). Auto-classification of ASD has become an important issue. Existing classification methods for ASD are based on features extracted from the whole-brain functional networks, which may be not discriminant enough for good performance. METHODS: In this study, we propose a network clustering based feature selection strategy for classifying ASD. In our proposed method, we first apply symmetric non-negative matrix factorization to divide brain networks into four modules. Then we extract features from one of four modules called default mode network (DMN) and use them to train several classifiers for ASD classification. RESULTS: The computational experiments show that our proposed method achieves better performances than those trained with features extracted from the whole brain network. CONCLUSION: It is a good strategy to train the classifiers for ASD based on features from the default mode subnetwork.",
        "BACKGROUND: The rapid development of Next-Generation Sequencing technologies enables sequencing genomes with low cost. The dramatically increasing amount of sequencing data raised crucial needs for efficient compression algorithms. Reference-based compression algorithms have exhibited outstanding performance on compressing single genomes. However, for the more challenging and more useful problem of compressing a large collection of n genomes, straightforward application of these reference-based algorithms suffers a series of issues such as difficult reference selection and remarkable performance variation. RESULTS: We propose an efficient clustering-based reference selection algorithm for reference-based compression within separate clusters of the n genomes. This method clusters the genomes into subsets of highly similar genomes using MinHash sketch distance, and uses the centroid sequence of each cluster as the reference genome for an outstanding reference-based compression of the remaining genomes in each cluster. A final reference is then selected from these reference genomes for the compression of the remaining reference genomes. Our method significantly improved the performance of the-state-of-art compression algorithms on large-scale human and rice genome databases containing thousands of genome sequences. The compression ratio gain can reach up to 20-30% in most cases for the datasets from NCBI, the 1000 Human Genomes Project and the 3000 Rice Genomes Project. The best improvement boosts the performance from 351.74 compression folds to 443.51 folds. CONCLUSIONS: The compression ratio of reference-based compression on large scale genome datasets can be improved via reference selection by applying appropriate data preprocessing and clustering methods. Our algorithm provides an efficient way to compress large genome database.",
        "BACKGROUND: Identifying different types of cancer based on gene expression data has become hotspot in bioinformatics research. Clustering cancer gene expression data from multiple cancers to their own class is a significance solution. However, the characteristics of high-dimensional and small samples of gene expression data and the noise of the data make data mining and research difficult. Although there are many effective and feasible methods to deal with this problem, the possibility remains that these methods are flawed. RESULTS: In this paper, we propose the graph regularized low-rank representation under symmetric and sparse constraints (sgLRR) method in which we introduce graph regularization based on manifold learning and symmetric sparse constraints into the traditional low-rank representation (LRR). For the sgLRR method, by means of symmetric constraint and sparse constraint, the effect of raw data noise on low-rank representation is alleviated. Further, sgLRR method preserves the important intrinsic local geometrical structures of the raw data by introducing graph regularization. We apply this method to cluster multi-cancer samples based on gene expression data, which improves the clustering quality. First, the gene expression data are decomposed by sgLRR method. And, a lowest rank representation matrix is obtained, which is symmetric and sparse. Then, an affinity matrix is constructed to perform the multi-cancer sample clustering by using a spectral clustering algorithm, i.e., normalized cuts (Ncuts). Finally, the multi-cancer samples clustering is completed. CONCLUSIONS: A series of comparative experiments demonstrate that the sgLRR method based on low rank representation has a great advantage and remarkable performance in the clustering of multi-cancer samples.",
        "BACKGROUND: In recent years, identification of differentially expressed genes and sample clustering have become hot topics in bioinformatics. Principal Component Analysis (PCA) is a widely used method in gene expression data. However, it has two limitations: first, the geometric structure hidden in data, e.g., pair-wise distance between data points, have not been explored. This information can facilitate sample clustering; second, the Principal Components (PCs) determined by PCA are dense, leading to hard interpretation. However, only a few of genes are related to the cancer. It is of great significance for the early diagnosis and treatment of cancer to identify a handful of the differentially expressed genes and find new cancer biomarkers. RESULTS: In this study, a new method gLSPCA is proposed to integrate both graph Laplacian and sparse constraint into PCA. gLSPCA on the one hand improves the clustering accuracy by exploring the internal geometric structure of the data, on the other hand identifies differentially expressed genes by imposing a sparsity constraint on the PCs. CONCLUSIONS: Experiments of gLSPCA and its comparison with existing methods, including Z-SPCA, GPower, PathSPCA, SPCArt, gLPCA, are performed on real datasets of both pancreatic cancer (PAAD) and head & neck squamous carcinoma (HNSC). The results demonstrate that gLSPCA is effective in identifying differentially expressed genes and sample clustering. In addition, the applications of gLSPCA on these datasets provide several new clues for the exploration of causative factors of PAAD and HNSC.",
        "BACKGROUND: Modern flow cytometry technology has enabled the simultaneous analysis of multiple cell markers at the single-cell level, and it is widely used in a broad field of research. The detection of cell populations in flow cytometry data has long been dependent on \"manual gating\" by visual inspection. Recently, numerous software have been developed for automatic, computationally guided detection of cell populations; however, they are not designed for time-series flow cytometry data. Time-series flow cytometry data are indispensable for investigating the dynamics of cell populations that could not be elucidated by static time-point analysis. Therefore, there is a great need for tools to systematically analyze time-series flow cytometry data. RESULTS: We propose a simple and efficient statistical framework, named CYBERTRACK (CYtometry-Based Estimation and Reasoning for TRACKing cell populations), to perform clustering and cell population tracking for time-series flow cytometry data. CYBERTRACK assumes that flow cytometry data are generated from a multivariate Gaussian mixture distribution with its mixture proportion at the current time dependent on that at a previous timepoint. Using simulation data, we evaluate the performance of CYBERTRACK when estimating parameters for a multivariate Gaussian mixture distribution, tracking time-dependent transitions of mixture proportions, and detecting change-points in the overall mixture proportion. The CYBERTRACK performance is validated using two real flow cytometry datasets, which demonstrate that the population dynamics detected by CYBERTRACK are consistent with our prior knowledge of lymphocyte behavior. CONCLUSIONS: Our results indicate that CYBERTRACK offers better understandings of time-dependent cell population dynamics to cytometry users by systematically analyzing time-series flow cytometry data.",
        "A novel weighted K-means scheme for a probabilistic-shaped (PS) 64 quadrature amplitude modulation (QAM) signal is proposed in order to locate the decision points more accurately and enhance the robustness of clustering algorithm. By using a weighting factor following the reciprocal of Maxwell-Boltzmann distribution, the proposed algorithm can combine the advantages of PS and K-means robustly while reducing the overall computational complexity of the clustering process. Experimental verification of the proposed clustering technique was demonstrated in a 120-Gb/s probabilistic-shaped 64QAM coherent optical communication system. The results show that the proposed algorithm has outperformed K-means with respect to bit error rate (BER), clustering robustness and iteration times in both back-to-back and 375km transmission scenarios. For the back-to-back situation, the proposed algorithm is capable of achieving about 0.6dB and 1.8dB OSNR gain over K-means clustered signals and unclustered signals. For the case of transmission, the proposed clustering procedure can robustly locate the optimal decision points with launched signal power ranging from -5dBm to 5dBm, while the working range for K-means procedure is only -4dBm to 2dBm. In addition, the proposed weighted algorithm takes less iteration times than K-means to converge, especially when the signal impairments caused by fiber Kerr nonlinearity is severe.",
        "Accurate segmentation of pulmonary nodules is an important basis for doctors to determine lung cancer. Aiming at the problem of incorrect segmentation of pulmonary nodules, especially the problem that it is difficult to separate adhesive pulmonary nodules connected with chest wall or blood vessels, an improved random walk method is proposed to segment difficult pulmonary nodules accurately in this paper. The innovation of this paper is to introduce geodesic distance to redefine the weights in random walk combining the coordinates of the nodes and seed points in the image with the space distance. The improved algorithm is used to achieve the accurate segmentation of pulmonary nodules. The computed tomography (CT) images of 17 patients with different types of pulmonary nodules were selected for segmentation experiments. The experimental results are compared with the traditional random walk method and those of several literatures. Experiments show that the proposed method has good accuracy in the segmentation of pulmonary nodule, and the accuracy can reach more than 88% with segmentation time is less than 4 seconds. The results could be used to assist doctors in the diagnosis of benign and malignant pulmonary nodules and improve clinical efficiency.",
        "BACKGROUND: Recent high throughput technologies have been applied for collecting heterogeneous biomedical omics datasets. Computational analysis of the multi-omics datasets could potentially reveal deep insights for a given disease. Most existing clustering methods by multi-omics data assume strong consistency among different sources of datasets, and thus may lose efficacy when the consistency is relatively weak. Furthermore, they could not identify the conflicting parts for each view, which might be important in applications such as cancer subtype identification. METHODS: In this work, we propose an integrative subspace clustering method (ISC) by common and specific decomposition to identify clustering structures with multi-omics datasets. The main idea of our ISC method is that the original representations for the samples in each view could be reconstructed by the concatenation of a common part and a view-specific part in orthogonal subspaces. The problem can be formulated as a matrix decomposition problem and solved efficiently by our proposed algorithm. RESULTS: The experiments on simulation and text datasets show that our method outperforms other state-of-art methods. Our method is further evaluated by identifying cancer types using a colorectal dataset. We finally apply our method to cancer subtype identification for five cancers using TCGA datasets, and the survival analysis shows that the subtypes we found are significantly better than other compared methods. CONCLUSION: We conclude that our ISC model could not only discover the weak common information across views but also identify the view-specific information.",
        "BACKGROUND: Cluster analysis is a core task in modern data-centric computation. Algorithmic choice is driven by factors such as data size and heterogeneity, the similarity measures employed, and the type of clusters sought. Familiarity and mere preference often play a significant role as well. Comparisons between clustering algorithms tend to focus on cluster quality. Such comparisons are complicated by the fact that algorithms often have multiple settings that can affect the clusters produced. Such a setting may represent, for example, a preset variable, a parameter of interest, or various sorts of initial assignments. A question of interest then is this: to what degree do the clusters produced vary as setting values change? RESULTS: This work introduces a new metric, termed simply \"robustness\", designed to answer that question. Robustness is an easily-interpretable measure of the propensity of a clustering algorithm to maintain output coherence over a range of settings. The robustness of eleven popular clustering algorithms is evaluated over some two dozen publicly available mRNA expression microarray datasets. Given their straightforwardness and predictability, hierarchical methods generally exhibited the highest robustness on most datasets. Of the more complex strategies, the paraclique algorithm yielded consistently higher robustness than other algorithms tested, approaching and even surpassing hierarchical methods on several datasets. Other techniques exhibited mixed robustness, with no clear distinction between them. CONCLUSIONS: Robustness provides a simple and intuitive measure of the stability and predictability of a clustering algorithm. It can be a useful tool to aid both in algorithm selection and in deciding how much effort to devote to parameter tuning.",
        "It is important to enhance the contrast and remove the speckle noise for optical coherence tomography (OCT) images. In this paper, we propose a selective retinex enhancement method based on the fuzzy c-means (FCM) clustering algorithm to enhance only the structure part in OCT images and combines with the block-matching 3D (BM3D) algorithm for filtering. In the proposed selective retinex enhancement method, we first calculate the feature image of the original image, which includes the mean value and standard deviation of each pixel in the original image and its correlation image. Second, by applying the FCM clustering algorithm to the feature image, a mask is generated that can distinguish the structure part from the background part in the OCT image. Then, the mask is applied to the multi-scale retinex algorithm, and only the structure part in the OCT image is enhanced. Moreover, the BM3D method is applied to filter the enhanced image. Experimental results demonstrate that the proposed method performs impressively in improving the contrast and removing the speckle noise of OCT images, and it provides better quantitative performance in terms of signal-to-noise ratio, contrast-to-noise ratio, equivalent number of looks, and the edge preservation parameter $ \\beta $beta.",
        "A decision technique using mixture-of-Gaussian (MoG) clustering algorithms is proposed in the context of a coherent optical communication system. For an 80-Gb/s single-carrier polarization-division multiplexed 16 quadrature amplitude modulation (QAM) transmission system at 975 km, an improvement in $Q$Q factors up to 0.41 dB is observed for the entire range of the considered optical signal-to-noise SNR. Compared with the traditional minimum Euclidean distance-based decision, the MoG clustering-based decision achieves a transmission distance increase of 175 km at a $Q$Q factor of 9.96 dB. Experiment results show that the proposed decision technique is insensitive to the system's nonlinear impairments and can effectively improve nonlinear tolerance of the system. We also propose a majorization method for decision-directed least mean square (LMS) using the MoG clustering-based decision algorithm, called MoG-LMS. The performance improvement of the MoG-LMS algorithm is verified by experiments.",
        "Cluster analysis plays vital role in pattern recognition in several fields of science. Silhouette width is a widely used index for assessing the fit of individual objects in the classification, as well as the quality of clusters and the entire classification. Silhouette combines two clustering criteria, compactness and separation, which imply that spherical cluster shapes are preferred over others-a property that can be seen as a disadvantage in the presence of complex, nonspherical clusters, which is common in real situations. We suggest a generalization of the silhouette width using the generalized mean. By changing the p parameter of the generalized mean between -infinity and +infinity, several specific summary statistics, including the minimum, maximum, the arithmetic, harmonic, and geometric means, can be reproduced. Implementing the generalized mean in the calculation of silhouette width allows for changing the sensitivity of the index to compactness versus connectedness. With higher sensitivity to connectedness, the preference of silhouette width toward spherical clusters should reduce. We test the performance of the generalized silhouette width on artificial data sets and on the Iris data set. We examine how classifications with different numbers of clusters prepared by different algorithms are evaluated, if p is set to different values. When p was negative, well-separated clusters achieved high silhouette widths despite their elongated or circular shapes. Positive values of p increased the importance of compactness; hence, the preference toward spherical clusters became even more detectable. With low p, single linkage clustering was deemed the most efficient clustering method, while with higher parameter values the performance of group average, complete linkage, and beta flexible with beta = -0.25 seemed better. The generalized silhouette allows for adjusting the contribution of compactness and connectedness criteria, thus avoiding underestimation of clustering efficiency in the presence of clusters with high internal heterogeneity.",
        "In cluster-based wireless sensor networks, cluster heads (CHs) gather and fuse data packets from sensor nodes; then, they forward fused packets to the sink node (SN). This helps wireless sensor networks balance energy effectively and efficiently to prolong their lifetime. However, cluster-based WSNs are vulnerable to selective forwarding attacks. Compromised CHs would become malicious and launch selective forwarding attacks in which they drop part of or all the packets from other nodes. In this paper, a data clustering algorithm (DCA) for detecting a selective forwarding attack (DCA-SF) is proposed. It can capture and isolate malicious CHs that have launched selective forwarding attacks by clustering their cumulative forwarding rates (CFRs). The DCA-SF algorithm has been strengthened by changing the DCA parameters (Eps, Minpts) adaptively. The simulation results show that the DCA-SF has a low missed detection rate of 1.04% and a false detection rate of 0.42% respectively with low energy consumption.",
        "Early detection of potential hazards in the fetal physiological state during pregnancy and childbirth is very important. Noninvasive fetal electrocardiogram (FECG) can be extracted from the maternal abdominal signal. However, due to the interference of maternal electrocardiogram and other noises, the task of extraction is challenging. This paper introduces a novel single-lead noninvasive fetal electrocardiogram extraction method based on the technique of clustering and PCA. The method is divided into four steps: (1) pre-preprocessing; (2) fetal QRS complexes and maternal QRS complexes detection based on k-means clustering algorithm with the feature of max-min pairs; (3) FQRS correction step is to improve the performance of step two; (4) template subtraction based on PCA is introduced to extract FECG waveform. To verify the performance of the proposed algorithm, two clinical open-access databases are used to check the performance of FQRS detection. As a result, the method proposed shows the average PPV of 95.35%, Se of 96.23%, and F1-measure of 95.78%. Furthermore, the robustness test is carried out on an artificial database which proves that the algorithm has certain robustness in various noise environments. Therefore, this method is feasible and reliable to detect fetal heart rate and extract FECG. Graphical abstract Early detection of potential hazards in the fetal physiological state during pregnancy and childbirth is very important. Noninvasive fetal electrocardiogram (FECG) can be extracted from maternal abdominal signal. However, due to the interference of maternal electrocardiogram and other noises, the task of extraction is challenging. This paper introduces a novel single-lead noninvasive fetal electrocardiogram extraction method based on the technique of clustering and PCA. The method is divided into four steps: (1) pre-preprocessing; (2) fetal QRS complexes and maternal QRS complexes detection based on k-means clustering algorithm with the feature of max-min pairs; (3) FQRS correction step is to improve the performance of step two; (4) template subtraction based on PCA is introduced to extract FECG waveform. To verify the performance of algorithm, two clinical open-access databases are used to check the performance of FQRS detection. As a result, the method proposed shows the average PPV of 95.35%, Se of 96.23%, and F1-measure of 95.78%. Furthermore, the robustness test is carried out on an artificial database which proves that the algorithm has certain robustness in various noise environments. Therefore, this method is feasible and reliable to detect fetal heart rate and extract FECG.",
        "BACKGROUND: Traditional Chinese medicine (TCM) is a highly important complement to modern medicine and is widely practiced in China and in many other countries. The work of Chinese medicine is subject to the two factors of the inheritance and development of clinical experience of famous Chinese medicine practitioners and the difficulty in improving the service capacity of basic Chinese medicine practitioners. Heterogeneous information networks (HINs) are a kind of graphical model for integrating and modeling real-world information. Through HINs, we can integrate and model the large-scale heterogeneous TCM data into structured graph data and use this as a basis for analysis. METHODS: Mining categorizations from TCM data is an important task for precision medicine. In this paper, we propose a novel structured learning model to solve the problem of formula regularity, a pivotal task in prescription optimization. We integrate clustering with ranking in a heterogeneous information network. RESULTS: The results from experiments on the Pharmacopoeia of the People's Republic of China (ChP) demonstrate the effectiveness and accuracy of the proposed model for discovering useful categorizations of formulas. CONCLUSIONS: We use heterogeneous information networks to model TCM data and propose a TCM-HIN. Combining the heterogeneous graph with the probability graph, we proposed the TCM-Clus algorithm, which combines clustering with ranking and classifies traditional Chinese medicine prescriptions. The results of the categorizations can help Chinese medicine practitioners to make clinical decision.",
        "Spontaneous resting-state neural activity or hemodynamics has been used to reveal functional connectivity in the brain. However, most of the commonly used clustering algorithms for functional parcellation are time-consuming, especially for high-resolution imaging data. We propose a density center-based fast clustering (DCBFC) method that can rapidly perform the functional parcellation of isocortex. DCBFC was validated using both simulation data and the spontaneous calcium signals from widefield fluorescence imaging of excitatory neuron-expressing transgenic mice (Vglut2-GCaMP6s). Compared to commonly used clustering methods such as k-means, hierarchical, and spectral, DCBFC showed a higher adjusted Rand index when the signal-to-noise ratio was greater than - 8 dB for simulated data and higher silhouette coefficient for in vivo mouse data. The resting-state functional connectivity (RSFC) patterns obtained by DCBFC were compared with the anatomic axonal projection density (PDs) maps derived from the voxel-scale model. The results showed a high spatial correlation between RSFC patterns and PDs.",
        "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry.",
        "Tomographic synthetic aperture radar (TomoSAR) produces 3-D point clouds with unavoidable noise or false targets that seriously deteriorate the quality of 3-D images and the building reconstruction over urban areas. In this paper, a Hough transform was adopted to detect the outline of a building; however, on one hand, the obtained outline of a building with Hough transform is broken, and on the other hand, some of these broken lines belong to the same segment of a building outline, but the parameters of these lines are slightly different. These problems will lead to that segment of a building outline being represented by multiple different parameters in the Hough transform. Therefore, an unsupervised clustering method was employed for clustering these line parameters. The lines gathered in the same cluster were considered to correspond to a same segment of a building outline. In this way, different line parameters corresponding to a segment of a building outline were integrated into one and then the continuous outline of the building in cloud points was obtained. Steps of the proposed data processing method were as follows. First, the Hough transform was made use of to detect the lines on the tomography plane in TomoSAR point clouds. These detected lines lay on the outline of the building, but they were broken due to the density variation of point clouds. Second, the lines detected using the Hough transform were grouped as a date set for training the building outline. Unsupervised clustering was utilized to classify the lines in several clusters. The cluster number was automatically determined via the unsupervised clustering algorithm, which meant the number of straight segments of the building edge was obtained. The lines in each cluster were considered to belong to the same straight segment of the building outline. Then, within each cluster, which represents a part or a segment of the building edge, a repaired straight line was constructed. Third, between each two clusters or each two segments of the building outline, the joint point was estimated by extending the two segments. Therefore, the building outline was obtained as completely as possible. Finally, taking the estimated building outline as the clustering center, supervised learning algorithm was used to classify the building cloud point and the noise (or false targets), then the building cloud point was refined. Then, our refined and unrefined data were fed into the neural network for building the 3-D construction. The comparison results show the correctness and the effectiveness of our improved method.",
        "BACKGROUND: Flow cytometry (FCM) is a powerful single-cell based measurement method to ascertain multidimensional optical properties of millions of cells. FCM is widely used in medical diagnostics and health research. There is also a broad range of applications in the analysis of complex microbial communities. The main concern in microbial community analyses is to track the dynamics of microbial subcommunities. So far, this can be achieved with the help of time-consuming manual clustering procedures that require extensive user-dependent input. In addition, several tools have recently been developed by using different approaches which, however, focus mainly on the clustering of medical FCM data or of microbial samples with a well-known background, while much less work has been done on high-throughput, online algorithms for two-channel FCM. RESULTS: We bridge this gap with flowEMMi, a model-based clustering tool based on multivariate Gaussian mixture models with subsampling and foreground/background separation. These extensions provide a fast and accurate identification of cell clusters in FCM data, in particular for microbial community FCM data that are often affected by irrelevant information like technical noise, beads or cell debris. flowEMMi outperforms other available tools with regard to running time and information content of the clustering results and provides near-online results and optional heuristics to reduce the running-time further. CONCLUSIONS: flowEMMi is a useful tool for the automated cluster analysis of microbial FCM data. It overcomes the user-dependent and time-consuming manual clustering procedure and provides consistent results with ancillary information and statistical proof.",
        "Traditional Chinese medicine (TCM) has become popular and been viewed as an effective clinical treatment across the world. Accordingly, there is an ever-increasing interest in performing data analysis over TCM data. Aiming to cope with the problem of excessively depending on empirical values when selecting cluster centers by traditional clustering algorithms, an improved artificial bee colony algorithm is proposed by which to automatically select cluster centers and apply it to aggregate Chinese herbal medicines. The proposed method integrates the following new techniques: (1) improving the artificial bee colony algorithm by applying a new searching strategy of neighbour nectar, (2) employing the improved artificial bee colony algorithm to optimize the parameters of the cutoff distance dc, the local density rhoi and the minimum distance deltai between the element i and any other element with higher density in the cluster algorithm by fast search and finding of density peaks (called DP algorithm) to find the optimal cluster centers, in order to clustering herbal medicines in an accurate fashion with the guarantee of runtime performance. Extensive experiments were conducted on the UCI benchmark datasets and the TCM datasets and the results verify the effectiveness of the proposed method by comparing it with classical clustering algorithms including K-means, K-mediods and DBSCAN in multiple evaluation metrics, that is, Silhouette Coefficient, Entropy, Purity, Precision, Recall and F1-Measure. The results show that the IABC-DP algorithm outperforms other approaches with good clustering quality and accuracy on the UCI and the TCM datasets as well. In addition, it can be found that the improved artificial bee colony algorithm can effectively reduce the number of iterations when compared to the traditional bee colony algorithm. In particular, the IABC-DP algorithm is applied to cluster multi-dimensional Chinese herbal medicines and the result shows that it outperforms other clustering algorithms in clustering Chinese herbal medicines, which can contribute to a larger effort targeted at advancing the study of discovering composition rules of traditional Chinese prescriptions.",
        "Automatic decision support systems have gained importance in health sector in recent years. In parallel with recent developments in the fields of artificial intelligence and image processing, embedded systems are also used in decision support systems for tumor diagnosis. Extreme learning machine (ELM), is a recently developed, quick and efficient algorithm which can quickly and flawlessly diagnose tumors using machine learning techniques. Similarly, significantly fast and robust fuzzy C-means clustering algorithm (FRFCM) is a novel and fast algorithm which can display a high performance. In the present study, a brain tumor segmentation approach is proposed based on extreme learning machine and significantly fast and robust fuzzy C-means clustering algorithms (BTS-ELM-FRFCM) running on Raspberry Pi (PRI) hardware. The present study mainly aims to introduce a new segmentation system hardware containing new algorithms and offering a high level of accuracy the health sector. PRI's are useful mobile devices due to their cost-effectiveness and satisfying hardware. 3200 training images were used to train ELM in the present study. 20 pieces of MRI images were used for testing process. Figure of merid (FOM), Jaccard similarity coefficient (JSC) and Dice indexes were used in order to evaluate the performance of the proposed approach. In addition, the proposed method was compared with brain tumor segmentation based on support vector machine (BTS-SVM), brain tumor segmentation based on fuzzy C-means (BTS-FCM) and brain tumor segmentation based on self-organizing maps and k-means (BTS-SOM). The statistical analysis on FOM, JSC and Dice results obtained using four different approaches indicated that BTS-ELM-FRFCM displayed the highest performance. Thus, it can be concluded that the embedded system designed in the present study can perform brain tumor segmentation with a high accuracy rate.",
        "Although only 2 % of crashes are head-on crashes in the United States, they account for over 10 % of all crash-related fatalities. This study aims to investigate the contributing factors that affect the injury severity of head-on crashes and develop appropriate countermeasures. Due to the unobserved heterogeneity inherent in the crash data, a latent class clustering analysis is firstly conducted to segment the head-on crashes into relatively homogeneous clusters. Then, mixed logit models are developed to further explore the unobserved heterogeneity within each cluster. Analyses are performed based on the data collected from the Highway Safety Information System (HSIS) from 2005 to 2013 in North Carolina. The estimated parameters and associated marginal effects are combined to interpret significant variables of the developed models. The proposed method is able to uncover the heterogeneity within the whole dataset and the homogeneous clusters. Results of this research can provide more reliable and insightful information to engineers and policy makers regarding the contributing factors to head-on crashes.",
        "Papillary thyroid carcinomas (PTC) are the most common type of thyroid malignant tumors. Existing methods for clustering high-noise ultrasound images tend to degrade the clustering performance. In order to realize accurate segmentation of thyroid nodule in noisy environment, this paper proposes an improved segmentation algorithm based on adaptive fast generalized clustering. Firstly, the parameter balance factor is adaptively determined according to the noise probability of non-local pixels so as to reflect the spatial structure information in the image more accurately. Then, the balance factor is used to effectively combine the linear weighted filtered image in the AFGC algorithm so as to create the adaptive filtered image. Since the filtering degree depends on the probability whether the pixel is noise in the image, the dynamic noise suppression performance of the proposed method can be greatly improved. A large number of qualitative and quantitative experimental results show that the proposed generalized clustering algorithm can obtain more accurate results when clustering images with high noise. It is suitable for intelligent diagnosis of papillary thyroid convolution in clinical examination.",
        "Background: Genomic data analyses such as Genome-Wide Association Studies (GWAS) or Hi-C studies are often faced with the problem of partitioning chromosomes into successive regions based on a similarity matrix of high-resolution, locus-level measurements. An intuitive way of doing this is to perform a modified Hierarchical Agglomerative Clustering (HAC), where only adjacent clusters (according to the ordering of positions within a chromosome) are allowed to be merged. But a major practical drawback of this method is its quadratic time and space complexity in the number of loci, which is typically of the order of 10 4 to 10 5 for each chromosome. Results: By assuming that the similarity between physically distant objects is negligible, we are able to propose an implementation of adjacency-constrained HAC with quasi-linear complexity. This is achieved by pre-calculating specific sums of similarities, and storing candidate fusions in a min-heap. Our illustrations on GWAS and Hi-C datasets demonstrate the relevance of this assumption, and show that this method highlights biologically meaningful signals. Thanks to its small time and memory footprint, the method can be run on a standard laptop in minutes or even seconds. Availability and implementation: Software and sample data are available as an R package, adjclust, that can be downloaded from the Comprehensive R Archive Network (CRAN).",
        "BACKGROUND: As societies become more complex, larger populations suffer from insomnia. In 2014, the US Centers for Disease Control and Prevention declared that sleep disorders should be dealt with as a public health epidemic. However, it is hard to provide adequate treatment for each insomnia sufferer, since various behavioral characteristics influence symptoms of insomnia collectively. OBJECTIVE: We aim to develop a neural-net based unsupervised user clustering method towards insomnia sufferers in order to clarify the unique traits for each derived groups. Unlike the current diagnosis of insomnia that requires qualitative analysis from interview results, the classification of individuals with insomnia by using various information modalities from smart bands and neural-nets can provide better insight into insomnia treatments. METHODS: This study, as part of the precision psychiatry initiative, is based on a smart band experiment conducted over 6 weeks on individuals with insomnia. During the experiment period, a total of 42 participants (19 male; average age 22.00 [SD 2.79]) from a large university wore smart bands 24/7, and 3 modalities were collected and examined: sleep patterns, daily activities, and personal demographics. We considered the consecutive daily information as a form of images, learned the latent variables of the images via a convolutional autoencoder (CAE), and clustered and labeled the input images based on the derived features. We then converted consecutive daily information into a sequence of the labels for each subject and finally clustered the people with insomnia based on their predominant labels. RESULTS: Our method identified 5 new insomnia-activity clusters of participants that conventional methods have not recognized, and significant differences in sleep and behavioral characteristics were shown among groups (analysis of variance on rank: F4,37=2.36, P=.07 for the sleep_min feature; F4,37=9.05, P<.001 for sleep_efficiency; F4,37=8.16, P<.001 for active_calorie; F4,37=6.53, P<.001 for walks; and F4,37=3.51, P=.02 for stairs). Analyzing the consecutive data through a CAE and clustering could reveal intricate connections between insomnia and various everyday activity markers. CONCLUSIONS: Our research suggests that unsupervised learning allows health practitioners to devise precise and tailored interventions at the level of data-guided user clusters (ie, precision psychiatry), which could be a novel solution to treating insomnia and other mental disorders.",
        "Identification of causal noncoding single nucleotide polymorphisms (SNPs) is important for maximizing the knowledge dividend from human genome-wide association studies (GWAS). Recently, diverse machine learning-based methods have been used for functional SNP identification; however, this task remains a fundamental challenge in computational biology. We report CERENKOV3, a machine learning pipeline that leverages clustering-derived and molecular network-derived features to improve prediction accuracy of regulatory SNPs (rSNPs) in the context of post-GWAS analysis. The clustering-derived feature, locus size (number of SNPs in the locus), derives from our locus partitioning procedure and represents the sizes of clusters based on SNP locations. We generated two molecular network-derived features from representation learning on a network representing SNP-gene and gene-gene relations. Based on empirical studies using a ground-truth SNP dataset, CERENKOV3 significantly improves rSNP recognition performance in AUPRC, AUROC, and AVGRANK (a locus-wise rank-based measure of classification accuracy we previously proposed).",
        "High costs in health care and everlasting need for quality improvement in care delivery is increasingly becoming the motivating factor for novel predictive studies in health care informatics. Surgical services impact both the operating theatre costs and revenues and play critical role in care quality. Efficiency of such units relies extremely on effective operational planning and inventory management. A key ingredient to such planning activities is the structured and unstructured data available prior to the surgery day from the electronic health records and other information systems. Unstructured data, such as textual features of procedure description and notes, provide additional information while structured data alone is not sufficient. To effectively utilize textual information using text mining, textual features should be easily identifiable, i.e., without typographical errors and ad hoc abbreviations. While there exists numerous spelling correction and abbreviation identification tools, they are not suitable for the surgical medical text as they require a dictionary and cannot accommodate ad hoc words such as abbreviations. This study proposes a novel preprocessing framework for surgical text data to detect misspellings and abbreviations prior to the application of any text mining and predictive modeling. The proposed approach helps extract the most salient text features from the unstructured principal procedure and additional notes by effectively reducing the raw feature set dimension. The transformed (text) feature set thus improves subsequent prediction tasks in surgery units. We test and validate the proposed approach using datasets from multiple hospitals' surgical departments and benchmark feature sets.",
        "One of the foremost and challenging tasks in hematoxylin and eosin stained histological image analysis is to reduce color variation present among images, which may significantly affect the performance of computer-aided histological image analysis. In this regard, the paper introduces a new rough-fuzzy circular clustering algorithm for stain color normalization. It judiciously integrates the merits of both fuzzy and rough sets. While the theory of rough sets deals with uncertainty, vagueness, and incompleteness in stain class definition, fuzzy set handles the overlapping nature of histochemical stains. The proposed circular clustering algorithm works on a weighted hue histogram, which considers both saturation and local neighborhood information of the given image. A new dissimilarity measure is introduced to deal with the circular nature of hue values. Some new quantitative measures are also proposed to evaluate the color constancy after normalization. The performance of the proposed method, along with a comparison with other state-of-the-art methods, is demonstrated on several publicly available standard data sets consisting of hematoxylin and eosin stained histological images.",
        "BACKGROUND AND OBJECTIVE: Early identification and diagnosis of tumors are of great significance to improve the survival rate of patients. Amongst other techniques, contrast-enhanced ultrasound is an important means to help doctors diagnose tumors. Due to the advantages of high efficiency, accuracy and objectivity, more and more computer-aided methods are used in medical diagnosis. Here we propose, a color-coded diagram based on quantitative blood perfusion parameters for contrast-enhanced ultrasound video. The method realizes the static description of the dynamic blood perfusion process in contrast-enhanced ultrasound videos and reveal the blood perfusion characteristics of all regions of the tissue providing assistance to the doctors in their clinical diagnosis. METHODS: For effective illustration of the blood perfusion through tissues, we propose (a) an improved block matching algorithm to eliminate the image distortions caused by breathing; (b) compute the time-grayscale intensity curve for each pixel to obtain four different quantitative blood perfusion parameters; and finally (c) employ the fuzzy C-means clustering algorithm to cluster the blood perfusion parameters, where each parameter is associated with a particular color. Thus based on the correspondence between the pixel and the blood perfusion parameters, all the pixels are color-coded to obtain the color-coded diagram. RESULTS: To the best of our knowledge, the proposed technique is one-of-its-kind to color code the contrast-enhanced ultrasound videos using blood perfusion parameters in order to understand the hemodynamic characteristics of the benign and malignant lesion. In our experiments, various contrast-enhanced ultrasound videos corresponding to several real-world cases were color-coded and the results of the experiments illustrated that the proposed color-coded diagrams are consistent with the diagnosis presented by the physicians. CONCLUSIONS: The experimental results suggested that the proposed method can comprehensively describe the blood perfusion characteristics of tissues during the angiography process thereby effectively assisting the doctors in diagnosis.",
        "Multiview subspace clustering, which aims to cluster the given data points with information from multiple sources or features into their underlying subspaces, has a wide range of applications in the communities of data mining and pattern recognition. Compared with the single-view subspace clustering, it is challenging to efficiently learn the structure of the representation matrix from each view and make use of the extra information embedded in multiple views. To address the two problems, a novel correntropy-based multiview subspace clustering (CMVSC) method is proposed in this article. The objective function of our model mainly includes two parts. The first part utilizes the Frobenius norm to efficiently estimate the dense connections between the points lying in the same subspace instead of following the standard compressive sensing approach. In the second part, the correntropy-induced metric (CIM) is introduced to characterize the noise in each view and utilize the information embedded in different views from an information-theoretic perspective. Furthermore, an efficient iterative algorithm based on the half-quadratic technique (HQ) and the alternating direction method of multipliers (ADMM) is developed to optimize the proposed joint learning problem, and extensive experimental results on six real-world multiview benchmarks demonstrate that the proposed methods can outperform several state-of-the-art multiview subspace clustering methods.",
        "Recent advances in NGS sequencing, microarrays and mass spectrometry for omics data production have enabled the generation and collection of different modalities of high-dimensional molecular data. The integration of multiple omics datasets is a statistical challenge, due to the limited number of individuals, the high number of variables and the heterogeneity of the datasets to integrate. Recently, a lot of tools have been developed to solve the problem of integrating omics data including canonical correlation analysis, matrix factorization and SM. These commonly used techniques aim to analyze simultaneously two or more types of omics. In this article, we compare a panel of 13 unsupervised methods based on these different approaches to integrate various types of multi-omics datasets: iClusterPlus, regularized generalized canonical correlation analysis, sparse generalized canonical correlation analysis, multiple co-inertia analysis (MCIA), integrative-NMF (intNMF), SNF, MoCluster, mixKernel, CIMLR, LRAcluster, ConsensusClustering, PINSPlus and multi-omics factor analysis (MOFA). We evaluate the ability of the methods to recover the subgroups and the variables that drive the clustering on eight benchmarks of simulation. MOFA does not provide any results on these benchmarks. For clustering, SNF, MoCluster, CIMLR, LRAcluster, ConsensusClustering and intNMF provide the best results. For variable selection, MoCluster outperforms the others. However, the performance of the methods seems to depend on the heterogeneity of the datasets (especially for MCIA, intNMF and iClusterPlus). Finally, we apply the methods on three real studies with heterogeneous data and various phenotypes. We conclude that MoCluster is the best method to analyze these omics data. Availability: An R package named CrIMMix is available on GitHub at https://github.com/CNRGH/crimmix to reproduce all the results of this article.",
        "Metabolic profiling of breath analysis involves processing, alignment, scaling, and clustering of thousands of features extracted from gas chromatography/mass spectrometry (GC/MS) data from hundreds of participants. The multistep data processing is complicated, operator error-prone, and time-consuming. Automated algorithmic clustering methods that are able to cluster features in a fast and reliable way are necessary. These accelerate metabolic profiling and discovery platforms for next-generation medical diagnostic tools. Our unsupervised clustering technique, VOCCluster, prototyped in Python, handles features of deconvolved GC/MS breath data. VOCCluster was created from a heuristic ontology based on the observation of experts undertaking data processing with a suite of software packages. VOCCluster identifies and clusters groups of volatile organic compounds (VOCs) from deconvolved GC/MS breath with similar mass spectra and retention index profiles. VOCCluster was used to cluster more than 15 000 features extracted from 74 GC/MS clinical breath samples obtained from participants with cancer before and after a radiation therapy. Results were evaluated against a panel of ground truth compounds and compared to other clustering methods (DBSCAN and OPTICS) that were used in previous metabolomics studies. VOCCluster was able to cluster those features into 1081 groups (including endogenous and exogenous compounds and instrumental artifacts) with an accuracy rate of 96% (+/-0.04 at 95% confidence interval).",
        "Clustering is an essential step in the analysis of single cell RNA-seq (scRNA-seq) data to shed light on tissue complexity including the number of cell types and transcriptomic signatures of each cell type. Due to its importance, novel methods have been developed recently for this purpose. However, different approaches generate varying estimates regarding the number of clusters and the single-cell level cluster assignments. This type of unsupervised clustering is challenging and it is often times hard to gauge which method to use because none of the existing methods outperform others across all scenarios. We present SAME-clustering, a mixture model-based approach that takes clustering solutions from multiple methods and selects a maximally diverse subset to produce an improved ensemble solution. We tested SAME-clustering across 15 scRNA-seq datasets generated by different platforms, with number of clusters varying from 3 to 15, and number of single cells from 49 to 32 695. Results show that our SAME-clustering ensemble method yields enhanced clustering, in terms of both cluster assignments and number of clusters. The mixture model ensemble clustering is not limited to clustering scRNA-seq data and may be useful to a wide range of clustering applications.",
        "BACKGROUND: Maximum parsimony reconciliation in the duplication-transfer-loss model is a widely-used method for analyzing the evolutionary histories of pairs of entities such as hosts and parasites, symbiont species, and species and genes. While efficient algorithms are known for finding maximum parsimony reconciliations, the number of such reconciliations can be exponential in the size of the trees. Since these reconciliations can differ substantially from one another, making inferences from any one reconciliation may lead to conclusions that are not supported, or may even be contradicted, by other maximum parsimony reconciliations. Therefore, there is a need to find small sets of best representative reconciliations when the space of solutions is large and diverse. RESULTS: We provide a general framework for hierarchical clustering the space of maximum parsimony reconciliations. We demonstrate this framework for two specific linkage criteria, one that seeks to maximize the average support of the events found in the reconciliations in each cluster and the other that seeks to minimize the distance between reconciliations in each cluster. We analyze the asymptotic worst-case running times and provide experimental results that demonstrate the viability and utility of this approach. CONCLUSIONS: The hierarchical clustering algorithm method proposed here provides a new approach to find a set of representative reconciliations in the potentially vast and diverse space of maximum parsimony reconciliations.",
        "To improve the performance of nonlinear system modeling, this study proposes a feature clustering-based adaptive modular neural network (FC-AMNN) by simulating information processing mechanism of human brains in the way that different information is processed by different modules in parallel. Firstly, features are clustered using an adaptive feature clustering algorithm, and the number of modules in FC-AMNN is determined by the number of feature clusters automatically. The features in each cluster are then allocated to the corresponding module in FC-AMNN. Then, a self-constructive RBF neural network based on Error Correction algorithm is adopted as the subnetwork to study the allocated features. All modules work in parallel and are finally integrated using a Bayesian method to obtain the output. To demonstrate the effectiveness of the proposed model, FC-AMNN is tested on several UCI benchmark problems as well as a practical problem in wastewater treatment process. The experimental results show that the FC-AMNN can achieve a better generalization performance and an accurate result for nonlinear system modeling compared with other modular neural networks.",
        "High-resolution range profile (HRRP) has attracted intensive attention from radar community because it is easy to acquire and analyze. However, most of the conventional algorithms require the prior information of targets, and they cannot process a large number of samples in real time. In this paper, a novel HRRP recognition method is proposed to classify unlabeled samples automatically where the number of categories is unknown. Firstly, with the preprocessing of HRRPs, we adopt principal component analysis (PCA) for dimensionality reduction of data. Afterwards, t-distributed stochastic neighbor embedding (t-SNE) with Barnes-Hut approximation is conducted for the visualization of high-dimensional data. It proves to reduce the dimensionality, which has significantly improved the computation speed. Finally, it is exhibited that the recognition performance with density-based clustering is superior to conventional algorithms under the condition of large azimuth angle ranges and low signal-to-noise ratio (SNR).",
        "MOTIVATION: Sequencing technologies allow the sequencing of microbial communities directly from the environment without prior culturing. Because assembly typically produces only genome fragments, also known as contigs, it is crucial to group them into putative species for further taxonomic profiling and down-streaming functional analysis. Taxonomic analysis of microbial communities requires contig clustering, a process referred to as binning, that is still one of the most challenging tasks when analyzing metagenomic data. The major problems are the lack of taxonomically related genomes in existing reference databases, the uneven abundance ratio of species, sequencing errors, and the limitations due to binning contig of different lengths. RESULTS: In this context we present MetaCon a novel tool for unsupervised metagenomic contig binning based on probabilistic k-mers statistics and coverage. MetaCon uses a signature based on k-mers statistics that accounts for the different probability of appearance of a k-mer in different species, also contigs of different length are clustered in two separate phases. The effectiveness of MetaCon is demonstrated in both simulated and real datasets in comparison with state-of-art binning approaches such as CONCOCT, MaxBin and MetaBAT.",
        "The prevailing paradigm for the analysis of biological data involves comparing groups of replicates from different conditions (e.g. control and treatment) to statistically infer features that discriminate them (e.g. differentially expressed genes). However, many situations in modern genomics such as single-cell omics experiments do not fit well into this paradigm because they lack true replicates. In such instances, spectral techniques could be used to rank features according to their degree of consistency with an underlying metric structure without the need to cluster samples. Here, we extend spectral methods for feature selection to abstract simplicial complexes and present a general framework for clustering-independent analysis. Combinatorial Laplacian scores take into account the topology spanned by the data and reduce to the ordinary Laplacian score when restricted to graphs. We demonstrate the utility of this framework with several applications to the analysis of gene expression and multi-modal genomic data. Specifically, we perform differential expression analysis in situations where samples cannot be grouped into distinct classes, and we disaggregate differentially expressed genes according to the topology of the expression space (e.g. alternative paths of differentiation). We also apply this formalism to identify genes with spatial patterns of expression using fluorescence in-situ hybridization data and to establish associations between genetic alterations and global expression patterns in large cross-sectional studies. Our results provide a unifying perspective on topological data analysis and manifold learning approaches to the analysis of large-scale biological datasets.",
        "Multiview clustering explores complementary information among distinct views to enhance clustering performance under the assumption that all samples have complete information in all available views. However, this assumption does not hold in many real applications, where the information of some samples in one or more views may be missing, leading to partial multiview clustering problems. In this case, significant performance degeneration is usually observed. A collection of partial multiview clustering algorithms has been proposed to address this issue and most treat all different views equally during clustering. In fact, because different views provide features collected from different angles/feature spaces, they might play different roles in the clustering process. With the diversity of different views considered, in this study, a novel adaptive method is proposed for partial multiview clustering by automatically adjusting the contributions of different views. The samples are divided into complete and incomplete sets, while a joint learning mechanism is established to facilitate the connection between them and thereby improve clustering performance. More specifically, the method is characterized by a joint optimization model comprising two terms. The first term mines the underlying cluster structure from both complete and incomplete samples by adaptively updating their importance in all available views. The second term is designed to group all data with the aid of the cluster structure modeled in the first term. These two terms seamlessly integrate the complementary information among multiple views and enhance the performance of partial multiview clustering. Experimental results on real-world datasets illustrate the effectiveness and efficiency of our proposed method.",
        "BACKGROUND: Aging is an organized biological process that is regulated by highly interconnected pathways between different cells and tissues in the living organism. Identification of similar genes between tissues in different ages may also help to discover the general mechanism of aging or to discover more effective therapeutic decisions. OBJECTIVE: According to the wide application of model-based clustering techniques, the aim is to evaluate the performance of the Mixture of Multivariate Normal Distributions (MMNDs) as a valid method for clustering time series gene expression data with the Mixture of Matrix-Variate Normal Distributions (MMVNDs). METHODS: In this study, the expression of aging data from NCBI's Gene Expression Omnibus was elaborated to utilize proper data. A set of common genes which were differentially expressed between different tissues were selected and then clustered together through two methods. Finally, the biological significance of clusters was evaluated, using their ability to find genes in the cell using Enricher. RESULTS: The MMVNDs is more efficient to find co-express genes. Six clusters of genes were observed using the MMVNDs. According to the functional analysis, most genes in clusters 1-6 are related to the B-cell receptors and IgG immunoglobulin complex, proliferating cell nuclear antigen complex, the metabolic pathways of iron, fat, and body mass control, the defense against bacteria, the cancer development incidence, and the chronic kidney failure, respectively. CONCLUSION: Results showed that most biological changes of aging between tissues are related to the specific components of immune cells. Also, the application of MMVNDs can increase the ability to find similar genes.",
        "Our proposed research technique intends to provide an effective liver magnetic resonance imaging (MRI) and computed tomography (CT) scan image classification which would play a significant role in medical dataset especially in feature selection and classification. There are a number of existing research works classifying the liver tumor disease. Early detection of liver tumor will help the patients to get cured rapidly. Our proposed research focuses on the classification of medical images with respect to the classification technique artificial neural network (ANN) to classify an image as normal or abnormal. In the pre-processing step, the input image is selected from the database and adaptive median filtering is used for noise removal. For better enhancement, histogram equalization (HE) is done in the noise-removed images. In the pre-processed images, the texture feature such as gray-level co-occurrence matrix (GLCM) and statistical features are extracted. From the extensive feature set, optimal features are selected using the optimal kernel K-means (OKK-means) clustering algorithm along with the oppositional firefly algorithm (OFA). The proposed method obtained 97.5% accuracy in the classification when compared to the existing method.",
        "Recently, Yang et al. (2019) proposed a fuzzy model-based Gaussian (F-MB-Gauss) clustering that combines a model-based Gaussian with fuzzy membership functions for clustering. In this paper, we further consider the F-MB-Gauss clustering with the least absolute shrinkage and selection operator (Lasso) for feature (variable) selection, termed a fuzzy Gaussian Lasso (FG-Lasso) clustering algorithm. We demonstrate that the proposed FG-Lasso is a good clustering algorithm with better choice for feature subset selection. Experimental results and comparisons actually present these good aspects of the proposed FG-Lasso clustering algorithm. Cancer is a disease with growth of abnormal cells in a body. WHO reported that it is the first or second main leading cause of death. It spreads and affects the other parts of body if there is not properly diagnosed. In the paper, we apply the proposed FG-Lasso to cancer data with good feature selection and clustering results.",
        "Multiview clustering has gained increasing attention recently due to its ability to deal with multiple sources (views) data and explore complementary information between different views. Among various methods, multiview subspace clustering methods provide encouraging performance. They mainly integrate the multiview information in the space where the data points lie. Hence, their performance may be deteriorated because of noises existing in each individual view or inconsistent between heterogeneous features. For multiview clustering, the basic premise is that there exists a shared partition among all views. Therefore, the natural space for multiview clustering should be all partitions. Orthogonal to existing methods, we propose to fuse multiview information in partition level following two intuitive assumptions: (i) each partition is a perturbation of the consensus clustering; (ii) the partition that is close to the consensus clustering should be assigned a large weight. Finally, we propose a unified multiview subspace clustering model which incorporates the graph learning from each view, the generation of basic partitions, and the fusion of consensus partition. These three components are seamlessly integrated and can be iteratively boosted by each other towards an overall optimal solution. Experiments on four benchmark datasets demonstrate the efficacy of our approach against the state-of-the-art techniques.",
        "BACKGROUND: Precision medicine requires a stratification of patients by disease presentation that is sufficiently informative to allow for selecting treatments on a per-patient basis. For many diseases, such as neurological disorders, this stratification problem translates into a complex problem of clustering multivariate and relatively short time series because (i) these diseases are multifactorial and not well described by single clinical outcome variables and (ii) disease progression needs to be monitored over time. Additionally, clinical data often additionally are hindered by the presence of many missing values, further complicating any clustering attempts. FINDINGS: The problem of clustering multivariate short time series with many missing values is generally not well addressed in the literature. In this work, we propose a deep learning-based method to address this issue, variational deep embedding with recurrence (VaDER). VaDER relies on a Gaussian mixture variational autoencoder framework, which is further extended to (i) model multivariate time series and (ii) directly deal with missing values. We validated VaDER by accurately recovering clusters from simulated and benchmark data with known ground truth clustering, while varying the degree of missingness. We then used VaDER to successfully stratify patients with Alzheimer disease and patients with Parkinson disease into subgroups characterized by clinically divergent disease progression profiles. Additional analyses demonstrated that these clinical differences reflected known underlying aspects of Alzheimer disease and Parkinson disease. CONCLUSIONS: We believe our results show that VaDER can be of great value for future efforts in patient stratification, and multivariate time-series clustering in general.",
        "The presence of missing entries in data often creates challenges for pattern recognition algorithms. Traditional algorithms for clustering data assume that all the feature values are known for every data point. We propose a method to cluster data in the presence of missing information. Unlike conventional clustering techniques where every feature is known for each point, our algorithm can handle cases where a few feature values are unknown for every point. For this more challenging problem, we provide theoretical guarantees for clustering using a l 0 fusion penalty based optimization problem. Furthermore, we propose an algorithm to solve a relaxation of this problem using saturating non-convex fusion penalties. It is observed that this algorithm produces solutions that degrade gradually with an increase in the fraction of missing feature values. We demonstrate the utility of the proposed method using a simulated dataset, the Wine dataset and also an under-sampled cardiac MRI dataset. It is shown that the proposed method is a promising clustering technique for datasets with large fractions of missing entries.",
        "Semicontinuous data, characterized by a sizable number of zeros and observations from a continuous distribution, are frequently encountered in health research concerning food consumptions, physical activities, medical and pharmacy claims expenditures, and many others. In analyzing such semicontinuous data, it is imperative that the excessive zeros be adequately accounted for to obtain unbiased and efficient inference. Although many methods have been proposed in the literature for the modeling and analysis of semicontinuous data, little attention has been given to clustering of semicontinuous data to identify important patterns that could be indicative of certain health outcomes or intervention effects. We propose a Bernoulli-normal mixture model for clustering of multivariate semicontinuous data and demonstrate its accuracy as compared to the well-known clustering method with the conventional normal mixture model. The proposed method is illustrated with data from a dietary intervention trial to promote healthy eating behavior among children with type 1 diabetes. In the trial, certain diabetes friendly foods (eg, total fruit, whole fruit, dark green and orange vegetables and legumes, whole grain) were only consumed by a proportion of study participants, yielding excessive zero values due to nonconsumption of the foods. Baseline foods consumptions data in the trial are used to explore preintervention dietary patterns among study participants. While the conventional normal mixture model approach fails to do so, the proposed Bernoulli-normal mixture model approach has shown to be able to identify a dietary profile that significantly differentiates the intervention effects from others, as measured by the popular healthy eating index at the end of the trial.",
        "In smart environments based on the Internet of Things (IoT), almost all of the object information that is collected by various sensors is time series data, which records the behavior of the objects. Analyzing the correlation between different time series data, other than those in the same time series, is more helpful to discovering their behavioral relations. This has become one of the important current issues in the IoT. To analyze the correlation, a clustering algorithm named the CPCCM (clustering algorithm based on precise correlation coefficient matching) is presented. First, each initial sequence is split into a set of subsequences by adopting a preset sliding window. Then, the correlation coefficients between any pair of subsequence sets from two sequences are resolved. Those pairs that pass some preset Pearson correlation coefficient threshold are clustered. In the CPCCM, a cross-traversal strategy is introduced to improve the search efficiency. The cross-traversal strategy alternatively searches the subsequences in two subsequence sets. To improve the clustering efficiency, in each initial sequence, adjacent subsequences are merged into longer subsequences and replaced by it if they appear in the same subsequence set. Finally, by analyzing practical electric power consumption data, the CPCCM is shown to be promising and able to be applied in similar scenarios. By comparison with the agglomerative hierarchical clustering algorithm, the major contributions of this work is that the clustering quality is improved by using the strategy of precise matching and cross-traversal, and complexity of the algorithm is reduced by merging adjacent subsequences. Therefore, CPCCM can be applied to analyze behavior between different objects in smart environments.",
        "In this paper we describe and validate a new coordinate-based method for meta-analysis of neuroimaging data based on an optimized hierarchical clustering algorithm: CluB (Clustering the Brain). The CluB toolbox permits both to extract a set of spatially coherent clusters of activations from a database of stereotactic coordinates, and to explore each single cluster of activation for its composition according to the cognitive dimensions of interest. This last step, called \"cluster composition analysis,\" permits to explore neurocognitive effects by adopting a factorial-design logic and by testing the working hypotheses using either asymptotic tests, or exact tests either in a classic inference, or in a Bayesian-like context. To perform our validation study, we selected the fMRI data from 24 normal controls involved in a reading task. We run a standard random-effects second level group analysis to obtain a \"Gold Standard\" of reference. In a second step, the subject-specific reading effects (i.e., the linear t-contrast \"reading > baseline\") were extracted to obtain a coordinates-based database that was used to run a meta-analysis using both CluB and the popular Activation Likelihood Estimation method implemented in the software GingerALE. The results of the two meta-analyses were compared against the \"Gold Standard\" to compute performance measures, i.e., sensitivity, specificity, and accuracy. The GingerALE method obtained a high level of accuracy (0.967) associated with a high sensitivity (0.728) and specificity (0.971). The CluB method obtained a similar level of accuracy (0.956) and specificity (0.969), notwithstanding a lower level of sensitivity (0.14) due to the lack of prior Gaussian transformation of the data. Finally, the two methods obtained a good-level of concordance (AC1 = 0.93). These results suggested that methods based on hierarchical clustering (and post-hoc statistics) and methods requiring prior Gaussian transformation of the data can be used as complementary tools, with the GingerALE method being optimal for neurofunctional mapping of pooled data according to simpler designs, and the CluB method being preferable to test more specific, and localized, neurocognitive hypotheses according to factorial designs.",
        "Despite the fact that many important problems (including clustering) can be described using hypergraphs, theoretical foundations as well as practical algorithms using hypergraphs are not well developed yet. In this paper, we propose a hypergraph modularity function that generalizes its well established and widely used graph counterpart measure of how clustered a network is. In order to define it properly, we generalize the Chung-Lu model for graphs to hypergraphs. We then provide the theoretical foundations to search for an optimal solution with respect to our hypergraph modularity function. A simple heuristic algorithm is described and applied to a few illustrative examples. We show that using a strict version of our proposed modularity function often leads to a solution where a smaller number of hyperedges get cut as compared to optimizing modularity of 2-section graph of a hypergraph.",
        "Maintaining high medication adherence is essential for achieving desired efficacy in clinical trials, especially prevention trials. However, adherence is traditionally measured by self-reports that are subject to reporting biases and measurement error. Recently, electronic medication dispenser devices have been adopted in several HIV pre-exposure prophylaxis prevention studies. These devices are capable of collecting objective, frequent, and timely drug adherence data. The device opening signals generated by such devices are often represented as regularly or irregularly spaced discrete functional data, which are challenging for statistical analysis. In this paper we focus on clustering the adherence monitoring data from such devices. We first pre-process the raw discrete functional data into smoothed functional data. Parametric mixture models with change-points, as well as several non-parametric and semi-parametric functional clustering approaches are adapted and applied to the smoothed adherence data. Simulation studies were conducted to evaluate finite sample performances, on the choices of tuning parameters in the pre-processing step as well as the relative performance of different clustering algorithms. We applied these methods to the HIV Prevention Trials Network(HPTN) 069 study for identifying subgroups with distinct adherence behavior over the study period.",
        "MOTIVATION: Recently, it has become feasible to generate large-scale, multi-tissue gene expression data, where expression profiles are obtained from multiple tissues or organs sampled from dozens to hundreds of individuals. When traditional clustering methods are applied to this type of data, important information is lost, because they either require all tissues to be analyzed independently, ignoring dependencies and similarities between tissues, or to merge tissues in a single, monolithic dataset, ignoring individual characteristics of tissues. RESULTS: We developed a Bayesian model-based multi-tissue clustering algorithm, revamp, which can incorporate prior information on physiological tissue similarity, and which results in a set of clusters, each consisting of a core set of genes conserved across tissues as well as differential sets of genes specific to one or more subsets of tissues. Using data from seven vascular and metabolic tissues from over 100 individuals in the STockholm Atherosclerosis Gene Expression (STAGE) study, we demonstrate that multi-tissue clusters inferred by revamp are more enriched for tissue-dependent protein-protein interactions compared to alternative approaches. We further demonstrate that revamp results in easily interpretable multi-tissue gene expression associations to key coronary artery disease processes and clinical phenotypes in the STAGE individuals. AVAILABILITY AND IMPLEMENTATION: Revamp is implemented in the Lemon-Tree software, available at https://github.com/eb00/lemon-tree. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "The internal structure of the human hippocampus is challenging to map using histology or neuroimaging due to its complex archicortical folding. Here, we aimed to overcome this challenge using a unique combination of three methods. First, we leveraged a histological dataset with unprecedented 3D coverage, BigBrain. Second, we imposed a computational unfolding framework that respects the topological continuity of hippocampal subfields, which are traditionally defined by laminar composition. Third, we adapted neocortical parcellation techniques to map the hippocampus with respect to not only laminar but also morphological features. Unsupervised clustering of these features revealed subdivisions that closely resemble gold standard manual subfield segmentations. Critically, we also show that morphological features alone are sufficient to derive most hippocampal subfield boundaries. Moreover, some features showed differences within subfields along the hippocampal longitudinal axis. Our findings highlight new characteristics of internal hippocampal structure, and offer new avenues for its characterization with in-vivo neuroimaging.",
        "This article studies a new problem of data stream clustering, namely, multiview data stream (MVStream) clustering. Although many data stream clustering algorithms have been developed, they are restricted to the single-view streaming data, and clustering MVStreams still remains largely unsolved. In addition to the many issues encountered by the conventional single-view data stream clustering, such as capturing cluster evolution and discovering clusters of arbitrary shapes under the limited computational resources, the main challenge of MVStream clustering lies in integrating information from multiple views in a streaming manner and abstracting summary statistics from the integrated features simultaneously. In this article, we propose a novel MVStream clustering algorithm for the first time. The main idea is to design a multiview support vector domain description (MVSVDD) model, by which the information from multiple insufficient views can be integrated, and the outputting support vectors (SVs) are utilized to abstract the summary statistics of the historical multiview data objects. Based on the MVSVDD model, a new multiview cluster labeling method is designed, whereby clusters of arbitrary shapes can be discovered for each view. By tracking the cluster labels of SVs in each view, the cluster evolution associated with concept drift can be captured. Since the SVs occupy only a small portion of data objects, the proposed MVStream algorithm is quite efficient with the limited computational resources. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposed method.",
        "OBJECTIVE: With its increasingly widespread adoption, electronic health records (EHR) have enabled phenotypic information extraction at an unprecedented granularity and scale. However, often a medical concept (e.g. diagnosis, prescription, symptom) is described in various synonyms across different EHR systems, hindering data integration for signal enhancement and complicating dimensionality reduction for knowledge discovery. Despite existing ontologies and hierarchies, tremendous human effort is needed for curation and maintenance - a process that is both unscalable and susceptible to subjective biases. This paper aims to develop a data-driven approach to automate grouping medical terms into clinically relevant concepts by combining multiple up-to-date data sources in an unbiased manner. METHODS: We present a novel data-driven grouping approach - multi-view banded spectral clustering (mvBSC) combining summary data from multiple healthcare systems. The proposed method consists of a banding step that leverages the prior knowledge from the existing coding hierarchy, and a combining step that performs spectral clustering on an optimally weighted matrix. RESULTS: We apply the proposed method to group ICD-9 and ICD-10-CM codes together by integrating data from two healthcare systems. We show grouping results and hierarchies for 13 representative disease categories. Individual grouping qualities were evaluated using normalized mutual information, adjusted Rand index, and F1-measure, and were found to consistently exhibit great similarity to the existing manual grouping counterpart. The resulting ICD groupings also enjoy comparable interpretability and are well aligned with the current ICD hierarchy. CONCLUSION: The proposed approach, by systematically leveraging multiple data sources, is able to overcome bias while maximizing consensus to achieve generalizability. It has the advantage of being efficient, scalable, and adaptive to the evolving human knowledge reflected in the data, showing a significant step toward automating medical knowledge integration.",
        "This method article aims to use group technology to classify engineering students at classroom level into clusters according to their learning style preferences. The Felder and Silverman's Index Learning Style (ILS) was used to evaluate students' learning style preferences. Students were then grouped into clusters based on the similarities of their learning styles preferences by using clustering algorithms, such as complete clustering. *Prior research on Learning Styles preferences in engineering education is limited in Saudi Arabia.*Students' learning style preferences allows instructors to adopt suitable teaching approach. Students having same learning styles can work together in group assignments.*Grouping students into clusters, we find that outlier students who having different learning styles than the rest may allow instructors to deal with them accordingly.",
        "This paper presents a fault detection and diagnosis (FDD) method, which uses one-dimensional convolutional neural network (1-D CNN) and WaveCluster clustering analysis to detect and diagnose sensor faults in the supply air temperature (T sup) control loop of the air handling unit. In this approach, 1-D CNN is employed to extract man-guided features from raw data, and the extracted features are analyzed by WaveCluster clustering. The suspicious sensor faults are indicated and categorized by denoting clusters. Moreover, the T c acquittal procedure is introduced to further improve the accuracy of FDD. In validation, false alarm ratio and missing diagnosis ratio are mainly used to demonstrate the efficiency of the proposed FDD method. Results show that the abrupt sensor faults in T sup control loop can be efficiently detected and diagnosed, and the proposed method is equipped with good robustness within the noise range of 6 dBm approximately 13 dBm.",
        "BACKGROUND: Network inference is crucial for biomedicine and systems biology. Biological entities and their associations are often modeled as interaction networks. Examples include drug protein interaction or gene regulatory networks. Studying and elucidating such networks can lead to the comprehension of complex biological processes. However, usually we have only partial knowledge of those networks and the experimental identification of all the existing associations between biological entities is very time consuming and particularly expensive. Many computational approaches have been proposed over the years for network inference, nonetheless, efficiency and accuracy are still persisting open problems. Here, we propose bi-clustering tree ensembles as a new machine learning method for network inference, extending the traditional tree-ensemble models to the global network setting. The proposed approach addresses the network inference problem as a multi-label classification task. More specifically, the nodes of a network (e.g., drugs or proteins in a drug-protein interaction network) are modelled as samples described by features (e.g., chemical structure similarities or protein sequence similarities). The labels in our setting represent the presence or absence of links connecting the nodes of the interaction network (e.g., drug-protein interactions in a drug-protein interaction network). RESULTS: We extended traditional tree-ensemble methods, such as extremely randomized trees (ERT) and random forests (RF) to ensembles of bi-clustering trees, integrating background information from both node sets of a heterogeneous network into the same learning framework. We performed an empirical evaluation, comparing the proposed approach to currently used tree-ensemble based approaches as well as other approaches from the literature. We demonstrated the effectiveness of our approach in different interaction prediction (network inference) settings. For evaluation purposes, we used several benchmark datasets that represent drug-protein and gene regulatory networks. We also applied our proposed method to two versions of a chemical-protein association network extracted from the STITCH database, demonstrating the potential of our model in predicting non-reported interactions. CONCLUSIONS: Bi-clustering trees outperform existing tree-based strategies as well as machine learning methods based on other algorithms. Since our approach is based on tree-ensembles it inherits the advantages of tree-ensemble learning, such as handling of missing values, scalability and interpretability.",
        "In this work, a new clustering algorithm especially geared towards merging data arising from multiple sensors is presented. The algorithm, called PN-EAC, is based on the ensemble clustering paradigm and it introduces the novel concept of negative evidence. PN-EAC combines both positive evidence, to gather information about the elements that should be grouped together in the final partition, and negative evidence, which has information about the elements that should not be grouped together. The algorithm has been validated in the electrocardiographic domain for heartbeat clustering, extracting positive evidence from the heartbeat morphology and negative evidence from the distances between heartbeats. The best result obtained on the MIT-BIH Arrhythmia database yielded an error of 1.44%. In the St. Petersburg Institute of Cardiological Technics 12-Lead Arrhythmia Database database (INCARTDB), an error of 0.601% was obtained when using two electrocardiogram (ECG) leads. When increasing the number of leads to 4, 6, 8, 10 and 12, the algorithm obtains better results (statistically significant) than with the previous number of leads, reaching an error of 0.338%. To the best of our knowledge, this is the first clustering algorithm that is able to process simultaneously any number of ECG leads. Our results support the use of PN-EAC to combine different sources of information and the value of the negative evidence.",
        "The growing computational capacity allows the investigation of large biomolecular systems by increasingly extensive molecular dynamics simulations. The resulting huge trajectories demand efficient partition methods to discern relevant structural dissimilarity. Clustering algorithms are available to address this task, but their implementations still need to be improved to gain in computational speed and to reduce the consumption of random access memory. We propose the BitClust code which, based on a combination of Python and C programming languages, performs fast structural clustering of long molecular trajectories. BitClust takes advantage of bitwise operations applied to a bit-encoded pairwise similarity matrix. Our approach allowed us to process a half-million frame trajectory in 6 h using less than 35 GB, a task that is not affordable with any of the similar alternatives.",
        "While analyzing the performance of state-of-the-art R-CNN based generic object detectors, we find that the detection performance for objects with low object-region-percentages (ORPs) of the bounding boxes are much lower than the overall average. Elongated objects are examples. To address the problem of low ORPs for elongated object detection, we propose a hybrid approach which employs a Faster R-CNN to achieve robust detections of object parts, and a novel model-driven clustering algorithm to group the related partial detections and suppress false detections. First, we train a Faster R-CNN with partial region proposals of suitable and stable ORPs. Next, we introduce a deep CNN (DCNN) for orientation classification on the partial detections. Then, on the outputs of the Faster R-CNN and DCNN, the algorithm of adaptive model-driven clustering first initializes a model of an elongated object with a data-driven process on local partial detections, and refines the model iteratively by model-driven clustering and data-driven model updating. By exploiting Faster R-CNN to produce robust partial detections and model-driven clustering to form a global representation, our method is able to generate a tight oriented bounding box for elongated object detection. We evaluate the effectiveness of our approach on two typical elongated objects in the COCO dataset, and other typical elongated objects, including rigid objects (pens, screwdrivers and wrenches) and non-rigid objects (cracks). Experimental results show that, compared with the state-of-the-art approaches, our method achieves a large margin of improvements for both detection and localization of elongated objects in images.",
        "A number of specialized clustering methods have been developed so far for the accurate analysis of single-cell RNA-sequencing (scRNA-seq) expression data, and several reports have been published documenting the performance measures of these clustering methods under different conditions. However, to date, there are no available studies regarding the systematic evaluation of the performance measures of the clustering methods taking into consideration the sample size and cell composition of a given scRNA-seq dataset. Herein, a comprehensive performance evaluation study of 11 selected scRNA-seq clustering methods was performed using synthetic datasets with known sample sizes and number of subpopulations, as well as varying levels of transcriptome complexity. The results indicate that the overall performance of the clustering methods under study are highly dependent on the sample size and complexity of the scRNA-seq dataset. In most of the cases, better clustering performances were obtained as the number of cells in a given expression dataset was increased. The findings of this study also highlight the importance of sample size for the successful detection of rare cell subpopulations with an appropriate clustering tool.",
        "OBJECTIVES: Patients with primary HIV-1 infection (PHI) are a particular population, giving important insight about ongoing evolution of transmitted drug resistance-associated mutation (TDRAM) prevalence, HIV diversity and clustering patterns. We describe these evolutions of PHI patients diagnosed in France from 2014 to 2016. METHODS: A total of 1121 PHI patients were included. TDRAMs were characterized using the 2009 Stanford list and the French ANRS algorithm. Viral subtypes and recent transmission clusters (RTCs) were also determined. RESULTS: Patients were mainly MSM (70%) living in the Paris area (42%). TDRAMs were identified among 10.8% of patients and rose to 18.6% when including etravirine and rilpivirine TDRAMs. Prevalences of PI-, NRTI-, first-generation NNRTI-, second-generation NNRTI- and integrase inhibitor-associated TDRAMs were 2.9%, 5.0%, 4.0%, 9.4% and 5.4%, respectively. In a multivariable analysis, age >40 years and non-R5 tropic viruses were associated with a >2-fold increased risk of TDRAMs. Regarding HIV diversity, subtype B and CRF02_AG (where CRF stands for circulating recombinant form) were the two main lineages (56% and 20%, respectively). CRF02_AG was associated with higher viral load than subtype B (5.83 versus 5.40 log10 copies/mL, P=0.004). We identified 138 RTCs ranging from 2 to 14 patients and including overall 41% from the global population. Patients in RTCs were younger, more frequently born in France and more frequently MSM. CONCLUSIONS: Since 2007, the proportion of TDRAMs has been stable among French PHI patients. Non-B lineages are increasing and may be associated with more virulent CRF02_AG strains. The presence of large RTCs highlights the need for real-time cluster identification to trigger specific prevention action to achieve better control of the epidemic.",
        "Efficiently and accurately analyzing high-dimensional time series, such as the molecular dynamics (MD) trajectory of biomolecules, is a long-standing and intriguing task. Two different but related techniques, i.e., dimension reduction methods and clustering algorithms, have been developed and applied widely in this field. Here we show that the combination of these techniques enables further improvement of the analyses, especially with very complicated data. Specifically, we present an approach that combines the trajectory mapping (TM) method, which constructs slow collective variables of a time series, with density peak clustering (DPC) [A. Rodriguez and A. Laio, Science 344, 1492 (2014)SCIEAS0036-807510.1126/science.1242072], which identifies similar data points to form clusters in a static data set. We illustrate the application of the TMDPC approach with hundreds of microseconds of all-atomic MD trajectories of two proteins, the villin headpiece and protein G. The results show that TMDPC is a powerful tool for achieving the metastable states and slow dynamics of these high-dimensional time series due to the efficient consideration of the time successiveness and the geometric distances between data points.",
        "BACKGROUND: As one of the most popular data representation methods, non-negative matrix decomposition (NMF) has been widely concerned in the tasks of clustering and feature selection. However, most of the previously proposed NMF-based methods do not adequately explore the hidden geometrical structure in the data. At the same time, noise and outliers are inevitably present in the data. RESULTS: To alleviate these problems, we present a novel NMF framework named robust hypergraph regularized non-negative matrix factorization (RHNMF). In particular, the hypergraph Laplacian regularization is imposed to capture the geometric information of original data. Unlike graph Laplacian regularization which captures the relationship between pairwise sample points, it captures the high-order relationship among more sample points. Moreover, the robustness of the RHNMF is enhanced by using the L2,1-norm constraint when estimating the residual. This is because the L2,1-norm is insensitive to noise and outliers. CONCLUSIONS: Clustering and common abnormal expression gene (com-abnormal expression gene) selection are conducted to test the validity of the RHNMF model. Extensive experimental results on multi-view datasets reveal that our proposed model outperforms other state-of-the-art methods.",
        "In recent years, with the diversity and variability of cancer information, the multi-omics data have been applied in various fields. Many existing models of principal component analysis can only process single data, which makes limitations on cancer research. Therefore, in this paper, a new model called integrative principal component analysis (IPCA) is proposed to achieve the unification of multi-omics data. In addition, in order to preserve the high-order manifold structure between the data, an integrative hypergraph regularization principal component analysis (IHPCA) is further proposed by applying the hypergraph regularization constraint. The effectiveness of IHPCA method is tested on four multi-omics datasets. Experimental results show that the proposed method has better performance than other representative methods on sample clustering and common expression genes (co-expression genes) network analysis.",
        "BACKGROUND AND OBJECTIVE: The prostate cancer interventions, which need an accurate prostate segmentation, are performed under ultrasound imaging guidance. However, prostate ultrasound segmentation is facing two challenges. The first is the low signal-to-noise ratio and inhomogeneity of the ultrasound image. The second is the non-standardized shape and size of the prostate. METHODS: For prostate ultrasound image segmentation, this paper proposed an accurate and efficient method of Active shape model (ASM) with Rayleigh mixture model Clustering (ASM-RMMC). Firstly, Rayleigh mixture model (RMM) is adopted for clustering the image regions which present similar speckle distributions. These content-based clustered images are then used to initialize and guide the deformation of an ASM model. RESULTS: The performance of the proposed method is assessed on 30 prostate ultrasound images using four metrics as Mean Average Distance (MAD), Dice Similarity Coefficient (DSC), False Positive Error (FPE) and False Negative Error (FNE). The proposed ASM-RMMC reaches high segmentation accuracy with 95% +/- 0.81% for DSC, 1.86+/-0.02 pixels for MAD, 2.10% +/- 0.36% for FPE and 2.78% +/- 0.71% for FNE, respectively. Moreover, the average segmentation time is less than 8s when treating a single prostate ultrasound image through ASM-RMMC. CONCLUSIONS: This paper presents a method for prostate ultrasound image segmentation, which achieves high accuracy with less computational complexity and meets the clinical requirements.",
        "The self-expressive property of data points, that is, each data point can be linearly represented by the other data points in the same subspace, has proven effective in leading subspace clustering (SC) methods. Most self-expressive methods usually construct a feasible affinity matrix from a coefficient matrix, obtained by solving an optimization problem. However, the negative entries in the coefficient matrix are forced to be positive when constructing the affinity matrix via exponentiation, absolute symmetrization, or squaring operations. This consequently damages the inherent correlations among the data. Besides, the affine constraint used in these methods is not flexible enough for practical applications. To overcome these problems, in this article, we introduce a scaled simplex representation (SSR) for the SC problem. Specifically, the non-negative constraint is used to make the coefficient matrix physically meaningful, and the coefficient vector is constrained to be summed up to a scalar to make it more discriminative. The proposed SSR-based SC (SSRSC) model is reformulated as a linear equality-constrained problem, which is solved efficiently under the alternating direction method of multipliers framework. Experiments on benchmark datasets demonstrate that the proposed SSRSC algorithm is very efficient and outperforms the state-of-the-art SC methods on accuracy. The code can be found at https://github.com/csjunxu/SSRSC.",
        "This article presents an efficient design and implementation of a real-time spike sorting system using unsupervised clustering. We utilize the online sorting (OSort) algorithm and model it first in both floating-point and fixed-point numerical representations to accurately assess the feasibility of our hardware architecture and also reliably analyze the sorting accuracy. For efficient hardware realization of OSort, we propose a modified parallel OSort algorithm. By reducing the number of required memory accesses, the number of computations performed for the management and upkeep of cluster averages and cluster merging is substantially reduced. By limiting the number of supported clusters per channel, the classification/clustering latency is significantly reduced compared to the previously published work, making the designed OSort system applicable for in-vivo spike sorting. The proposed OSort hardware architecture utilizes a novel memory configuration scheme to parallelize the OSort algorithm, which allows us to avoid using relatively large memory queues for storing detected spike waveforms and process them concurrently to the spike cluster management. The characteristics and implementation results of the designed OSort-based spike sorting system on a Xilinx Artix-7 field-programmable gate array (FPGA) are presented. The ASIC implementation of the designed system is estimated to occupy 2.57 mm(2) in a standard 32-nm CMOS process. Post-layout power estimation shows that the ASIC will dissipate 2.78 mW, while operating at 24 kHz.",
        "MOTIVATION: Over the past decade, there have been impressive advances in determining the 3D structures of protein complexes. However, there are still many complexes with unknown structures, even when the structures of the individual proteins are known. The advent of protein sequence information provides an opportunity to leverage evolutionary information to enhance the accuracy of protein-protein interface prediction. To this end, several statistical and machine learning methods have been proposed. In particular, direct coupling analysis has recently emerged as a promising approach for identification of protein contact maps from sequential information. However, the ability of these methods to detect protein-protein inter-residue contacts remains relatively limited. RESULTS: In this work, we propose a method to integrate sequential and co-evolution information with structural and functional information to increase the performance of protein-protein interface prediction. Further, we present a post-processing clustering method that improves the average relative F1 score by 70% and 24% and the average relative precision by 80% and 36% in comparison with two state-of-the-art methods, PSICOV and GREMLIN. AVAILABILITY AND IMPLEMENTATION: https://github.com/BioMLBoston/PatchDCA. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Crop yield is an essential measure for breeders, researchers, and farmers and is composed of and may be calculated by the number of ears per square meter, grains per ear, and thousand grain weight. Manual wheat ear counting, required in breeding programs to evaluate crop yield potential, is labor-intensive and expensive; thus, the development of a real-time wheat head counting system would be a significant advancement. In this paper, we propose a computationally efficient system called DeepCount to automatically identify and count the number of wheat spikes in digital images taken under natural field conditions. The proposed method tackles wheat spike quantification by segmenting an image into superpixels using simple linear iterative clustering (SLIC), deriving canopy relevant features, and then constructing a rational feature model fed into the deep convolutional neural network (CNN) classification for semantic segmentation of wheat spikes. As the method is based on a deep learning model, it replaces hand-engineered features required for traditional machine learning methods with more efficient algorithms. The method is tested on digital images taken directly in the field at different stages of ear emergence/maturity (using visually different wheat varieties), with different canopy complexities (achieved through varying nitrogen inputs) and different heights above the canopy under varying environmental conditions. In addition, the proposed technique is compared with a wheat ear counting method based on a previously developed edge detection technique and morphological analysis. The proposed approach is validated with image-based ear counting and ground-based measurements. The results demonstrate that the DeepCount technique has a high level of robustness regardless of variables, such as growth stage and weather conditions, hence demonstrating the feasibility of the approach in real scenarios. The system is a leap toward a portable and smartphone-assisted wheat ear counting systems, results in reducing the labor involved, and is suitable for high-throughput analysis. It may also be adapted to work on Red; Green; Blue (RGB) images acquired from unmanned aerial vehicle (UAVs).",
        "Clustering is presently one of the main routing techniques employed in randomly deployed wireless sensor networks. This paper describes a novel centralized unequal clustering method for wireless sensor networks. The goals of the algorithm are to prolong the network lifetime and increase the reliability of the network while not compromising the data transmission. In the proposed method, the Base Station decides on the cluster heads according to the best scores obtained from a Type-2 Fuzzy system. The input parameters of the fuzzy system are estimated by the base station or gathered from the network with a careful design that reduces the control message exchange. The whole network is controlled by the base station in a rounds-based schedule that alternates rounds when the base station elects cluster heads, with other rounds in which the cluster heads previously elected, gather data from their contributing nodes and forward them to the base station. The setting of the number of rounds in which the Base Station keeps the same set of cluster heads is another contribution of the present paper. The results show significant improvements achieved by the proposal when compared to other current clustering methods.",
        "In many practical applications, it is crucial to perform automatic data clustering without knowing the number of clusters in advance. The evolutionary computation paradigm is good at dealing with this task, but the existing algorithms encounter several deficiencies, such as the encoding redundancy and the cross-dimension learning error. In this article, we propose a novel elastic differential evolution algorithm to solve automatic data clustering. Unlike traditional methods, the proposed algorithm considers each clustering layout as a whole and adapts the cluster number and cluster centroids inherently through the variable-length encoding and the evolution operators. The encoding scheme contains no redundancy. To enable the individuals of different lengths to exchange information properly, we develop a subspace crossover and a two-phase mutation operator. The operators employ the basic method of differential evolution and, in addition, they consider the spatial information of cluster layouts to generate offspring solutions. Particularly, each dimension of the parameter vector interacts with its correlated dimensions, which not only adapts the cluster number but also avoids the cross-dimension learning error. The experimental results show that our algorithm outperforms the state-of-the-art algorithms that it is able to identify the correct number of clusters and obtain a good cluster validation value.",
        "Most existing clustering methods employ the original multi-view data as input to learn the similarity matrix which characterizes the underlying cluster structure shared by multiple views. This reduces the flexibility of multi-view clustering methods due to the fact that multi-view data usually contains noise or the variation between multi-view data points, which should belong to the same cluster, is larger than the variation between data points belonging to different clusters. To address these problems, we propose a novel multi-view clustering model, namely adaptive latent similarity learning (ALSL) for multi-view clustering. ALSL employs the adaptively learned graph, which characterizes the relationship between clusters, as the new input to learn the latent data representation and integrates the latent similarity representation learning, manifold learning and spectral clustering into a unified framework. With the complementarity of multiple views, the latent similarity representation characterizes the underlying cluster structure shared by multiple views. Our model is intuitive and can be optimized efficiently by using the Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) algorithm. Extensive experiments on benchmark datasets have demonstrated the superiority of the proposed method.",
        "Dual-energy computed tomography (DECT) imaging plays an important role in advanced imaging applications due to its material decomposition capability. Image-domain decomposition operates directly on CT images using linear matrix inversion, but the decomposed material images can be severely degraded by noise and artifacts. This paper proposes a new method dubbed DECT-MULTRA for image-domain DECT material decomposition that combines conventional penalized weighted-least squares (PWLS) estimation with regularization based on a mixed union of learned transforms (MULTRA) model. Our proposed approach pre-learns a union of common-material sparsifying transforms from patches extracted from all the basis materials, and a union of cross-material sparsifying transforms from multi-material patches. The common-material transforms capture the common properties among different material images, while the cross-material transforms capture the cross-dependencies. The proposed PWLS formulation is optimized efficiently by alternating between an image update step and a sparse coding and clustering step, with both of these steps having closed-form solutions. The effectiveness of our method is validated with both XCAT phantom and clinical head data. The results demonstrate that our proposed method provides superior material image quality and decomposition accuracy compared to other competing methods.",
        "One of the important approaches of handling data heterogeneity in multimodal data clustering is modeling each modality using a separate similarity graph. Information from the multiple graphs is integrated by combining them into a unified graph. A major challenge here is how to preserve cluster information while removing noise from individual graphs. In this regard, a novel algorithm, termed as CoALa, is proposed that integrates noise-free approximations of multiple similarity graphs. The proposed method first approximates a graph using the most informative eigenpairs of its Laplacian which contain cluster information. The approximate Laplacians are then integrated for the construction of a low-rank subspace that best preserves overall cluster information of multiple graphs. However, this approximate subspace differs from the full-rank subspace which integrates information from all the eigenpairs of each Laplacian. Matrix perturbation theory is used to theoretically evaluate how far approximate subspace deviates from the full-rank one for a given value of approximation rank. Finally, spectral clustering is performed on the approximate subspace to identify the clusters. Experimental results on several real-life cancer and benchmark data sets demonstrate that the proposed algorithm significantly and consistently outperforms state-of-the-art integrative clustering approaches.",
        "Medical image analysis plays an important role in computer-aided liver-carcinoma diagnosis. Aiming at the existing image fuzzy clustering segmentation being not suitable to segment CT image with non-uniform background, a fast robust kernel space fuzzy clustering segmentation algorithm is proposed. Firstly, the sample in euclidean space is mapped into the high dimensional feature space through the kernel function. Then the linear weighted filtering image is obtained by combining the current pixel with its neighborhood pixels through the space information in CT image. Finally, the two-dimensional histogram between the clustered pixel and its neighborhood mean is introduced into the robust kernel space image fuzzy clustering, and the iterative expression of the fast robust fuzzy clustering in kernel space is obtained by using Lagrange multiplier method. The experimental results on four databases show that our proposed method can segment liver tumors from abdominal CT volumes effectively and automatically, and the comprehensive segmentation performance of the proposed method is superior to that of several existing methods.",
        "Characterizing and interpreting heterogeneous mixtures at the cellular level is a critical problem in genomics. Single-cell assays offer an opportunity to resolve cellular level heterogeneity, e.g., scRNA-seq enables single-cell expression profiling, and scATAC-seq identifies active regulatory elements. Furthermore, while scHi-C can measure the chromatin contacts (i.e., loops) between active regulatory elements to target genes in single cells, bulk HiChIP can measure such contacts in a higher resolution. In this work, we introduce DC3 (De-Convolution and Coupled-Clustering) as a method for the joint analysis of various bulk and single-cell data such as HiChIP, RNA-seq and ATAC-seq from the same heterogeneous cell population. DC3 can simultaneously identify distinct subpopulations, assign single cells to the subpopulations (i.e., clustering) and de-convolve the bulk data into subpopulation-specific data. The subpopulation-specific profiles of gene expression, chromatin accessibility and enhancer-promoter contact obtained by DC3 provide a comprehensive characterization of the gene regulatory system in each subpopulation.",
        "Biological, ecological, social, and technological systems are complex structures with multiple interacting parts, often represented by networks. Correlation matrices describing interdependency of the variables in such structures provide key information for comparison and classification of such systems. Classification based on correlation matrices could supplement or improve classification based on variable values, since the former reveals similarities in system structures, while the latter relies on the similarities in system states. Importantly, this approach of clustering correlation matrices is different from clustering elements of the correlation matrices, because our goal is to compare and cluster multiple networks-not the nodes within the networks. A novel approach for clustering correlation matrices, named \"snakes-&-dragons,\" is introduced and illustrated by examples from neuroscience, human microbiome, and macroeconomics.",
        "Clustering is a difficult and widely studied data mining task, with many varieties of clustering algorithms proposed in the literature. Nearly all algorithms use a similarity measure such as a distance metric (e.g., Euclidean distance) to decide which instances to assign to the same cluster. These similarity measures are generally predefined and cannot be easily tailored to the properties of a particular dataset, which leads to limitations in the quality and the interpretability of the clusters produced. In this article, we propose a new approach to automatically evolving similarity functions for a given clustering algorithm by using genetic programming. We introduce a new genetic programming-based method which automatically selects a small subset of features (feature selection) and then combines them using a variety of functions (feature construction) to produce dynamic and flexible similarity functions that are specifically designed for a given dataset. We demonstrate how the evolved similarity functions can be used to perform clustering using a graph-based representation. The results of a variety of experiments across a range of large, high-dimensional datasets show that the proposed approach can achieve higher and more consistent performance than the benchmark methods. We further extend the proposed approach to automatically produce multiple complementary similarity functions by using a multi-tree approach, which gives further performance improvements. We also analyse the interpretability and structure of the automatically evolved similarity functions to provide insight into how and why they are superior to standard distance metrics.",
        "PURPOSE: The purpose of the study was to develop and evaluate an automated digitization algorithm for high-dose-rate cervix brachytherapy, with the goal of reducing the duration of treatment planning, staff resources, variability, and potential for human error. METHODS: An automated digitization algorithm was developed and retrospectively evaluated using treatment planning data from 10 patients with cervix cancer who were treated with a titanium tandem and ovoids applicator set. Applicators were segmented, without human interaction, by thresholding CT images to isolate high-density voxels and assigning the voxels to applicator and nonapplicator structures using HDBSCAN, a density-based linkage clustering algorithm. The applicator contours were determined from the centroid of the clustered voxels on each image slice and written to a treatment plan file. Automated contours were evaluated against manual digitization using distance and dosimetric metrics. RESULTS: A close agreement between automatic and manual digitization was observed. The mean magnitude of contour disagreement for 10 patients equaled 0.3 mm. Hausdorff distances were </=1.0 mm. The applicator tip coordinates had submillimeter agreement. The median and mean dose volume histogram parameter differences were less than or equal to 1% for high-risk clinical target volume D90, high-risk clinical target volume D95, bladder D2cc, rectum D2cc, large bowel D2cc, and small bowel D2cc. The average execution time for the automated algorithm was less than 30 s. CONCLUSION: The digitization of titanium tandem and ovoids applicators for high-dose-rate brachytherapy treatment planning can be automated using straightforward thresholding and clustering algorithms. The adoption of automated digitization is expected to improve the consistency of treatment plans and reduce the duration of treatment planning.",
        "BACKGROUND AND OBJECTIVE: The functional regions clustering through microelectrode recording (MER) is a critical step in deep brain stimulation (DBS) surgery. The localization of the optimal target highly relies on the neurosurgeon's empirical assessment of the neurophysiological signal. This work presents an unsupervised clustering algorithm to get the optimal cluster result of the functional regions along the electrode trajectory. METHODS: The dataset consists of the MERs obtained from the routine bilateral DBS for PD patients. Several features have been extracted from MER and divided into groups based on the type of neurophysiological signal. We selected single feature groups rather than all features as the input samples of each division of the divisive hierarchical clustering (DHC) algorithm. And the optimal cluster result has been achieved through a feature group combination selection (FGS) method based on genetic algorithm (GA). To measure the performance of this method, we compared the accuracy and validation indexes of three methods, including DHC only, DHC with GA-based FGS and DHC with GA-based feature selection (FS) in other studies, on the universal and DBS datasets. RESULTS: Results show that the DHC with GA-based FGS achieved the optimal cluster result compared with other methods. The three borders of the STN can be identified from the cluster result. The dorsoventral sizes of the STN and dorsal STN are 4.45mm and 2.02mm. In addition, the features extracted from the multiunit activity, background unit activity and local field potential are found to be the most representative feature groups to identify the dorsal, d-v and ventral borders of the STN, respectively. CONCLUSIONS: Our clustering algorithm showed a reliable performance of the automatic identification of functional regions in DBS. The findings can provide valuable assistance for both neurosurgeons and stereotactic surgical robots in DBS surgery.",
        "Deep neural networks usually require large labeled datasets to construct accurate models; however, in many real-world scenarios, such as medical image segmentation, labelling data is a time-consuming and costly human (expert) intelligent task. Semi-supervised methods leverage this issue by making use of a small labeled dataset and a larger set of unlabeled data. In this article, we present a flexible framework for semi-supervised learning that combines the power of supervised methods that learn feature representations using state-of-the-art deep convolutional neural networks with the deep embedded clustering algorithm that assigns data points to clusters based on their probability distributions and feature representations learned by the networks. Our proposed semi-supervised learning algorithm based on deep embedded clustering (SSLDEC) learns feature representations via iterations by alternatively using labeled and unlabeled data points and computing target distributions from predictions. During this iterative procedure the algorithm uses labeled samples to keep the model consistent and tuned with labeling, as it simultaneously learns to improve feature representation and predictions. SSLDEC requires few hyper-parameters and thus does not need large labeled validation sets, which addresses one of the main limitations of many semi-supervised learning algorithms. It is also flexible and can be used with many state-of-the-art deep neural network configurations for image classification and segmentation tasks. To this end, we implemented and tested our approach on benchmark image classification tasks as well as in a challenging medical image segmentation scenario. In benchmark classification tasks, SSLDEC outperformed several state-of-the-art semi-supervised learning methods, achieving 0.46% error on MNIST with 1000 labeled points, and 4.43% error on SVHN with 500 labeled points. In the iso-intense infant brain MRI tissue segmentation task, we implemented SSLDEC on a 3D densely connected fully convolutional neural network where we achieved significant improvement over supervised-only training as well as a semi-supervised method based on pseudo-labelling. Our results show that SSLDEC can be effectively used to reduce the need for costly expert annotations, enhancing applications such as automatic medical image segmentation.",
        "BACKGROUND: Paying attention to men's health seems quite important for a variety of reasons. We evaluated the change of mortality rates due to various causes in Iranian men over the past decades. STUDY DESIGN: A cross-sectional study. METHODS: The mortality rates for deadliest causes of diseases among Iranian men during 1990-2016 were extracted from the Global Burden of Disease (GBD) study. Latent Growth Mixture Models (LGMM) were applied to determine subgroups' cause of death. In this way, the causes within each group showed similar trends of mortality rates over time. RESULTS: The LGMM clustered causes into 4 classes. Diabetes mellitus, hypertensive heart disease and neurological disorders have had increasing trend. Causes in class 2, including diarrhea, lower respiratory and other common infectious diseases, ischemic heart disease, ischemic stroke, neonatal disorders, and other non-communicable diseases manifested a slow decreasing trend. Most causes were allocated to 3rd class with a slow increase in mortality rates over time. Finally, within the last class, transport injuries and unintentional injuries revealed a decreasing trend. CONCLUSION: Most factors have rising trend, despite the fact that some have shown a very slight downward trend. Consequently, according to the four distinguished clusters resulting from LGMM, it is essential to provide programs to attain the goal of access to prevention, treatment, and support for high-risk mortality factors.",
        "Sensor network intrusion detection has attracted extensive attention. However, previous intrusion detection methods face the highly imbalanced attack class distribution problem, and they may not achieve a satisfactory performance. To solve this problem, we propose a new intrusion detection algorithm based on normalized cut spectral clustering for sensor network in this paper. The main aim is to reduce the imbalance degree among classes in an intrusion detection system. First, we design a normalized cut spectral clustering to reduce the imbalance degree between every two classes in the intrusion detection data set. Second, we train a network intrusion detection classifier on the new data set. Finally, we do extensive experiments and analyze the experimental results in detail. Simulation experiments show that our algorithm can reduce the imbalance degree among classes and reserves the distribution of the original data on the one hand, and improve effectively the detection performance on the other hand.",
        "This paper serves as a user guide to the Vienna graph clustering framework. We review our general memetic algorithm, VieClus, to tackle the graph clustering problem. A key component of our contribution are natural recombine operators that employ ensemble clusterings as well as multi-level techniques. Lastly, we combine these techniques with a scalable communication protocol, producing a system that is able to compute high-quality solutions in a short amount of time. After giving a description of the algorithms employed, we establish the connection of the graph clustering problem to protein-protein interaction networks and moreover give a description on how the software can be used, what file formats are expected, and how this can be used to find functional groups in protein-protein interaction networks.",
        "Single-cell RNA-Sequencing (scRNA-Seq), an advanced sequencing technique, enables biomedical researchers to characterize cell-specific gene expression profiles. Although studies have adapted machine learning algorithms to cluster different cell populations for scRNA-Seq data, few existing methods have utilized machine learning techniques to investigate functional pathways in classifying heterogeneous cell populations. As genes often work interactively at the pathway level, studying the cellular heterogeneity based on pathways can facilitate the interpretation of biological functions of different cell populations. In this paper, we propose a pathway-based analytic framework using Random Forests (RF) to identify discriminative functional pathways related to cellular heterogeneity as well as to cluster cell populations for scRNA-Seq data. We further propose a novel method to construct gene-gene interactions (GGIs) networks using RF that illustrates important GGIs in differentiating cell populations. The co-occurrence of genes in different discriminative pathways and 'cross-talk' genes connecting those pathways are also illustrated in our networks. Our novel pathway-based framework clusters cell populations, prioritizes important pathways, highlights GGIs and pivotal genes bridging cross-talked pathways, and groups co-functional genes in networks. These features allow biomedical researchers to better understand the functional heterogeneity of different cell populations and to pinpoint important genes driving heterogeneous cellular functions.",
        "Sparse data is known to pose challenges to cluster analysis, as the similarity between data tends to be ill-posed in the high-dimensional Hilbert space. Solutions in the literature typically extend either k-means or spectral clustering with additional steps on representation learning and/or feature weighting. However, adding these usually introduces new parameters and increases computational cost, thus inevitably lowering the robustness of these algorithms when handling massive ill-represented data. To alleviate these issues, this paper presents a class of self-organizing neural networks, called the salience-aware adaptive resonance theory (SA-ART) model. SA-ART extends Fuzzy ART with measures for cluster-wise salient feature modeling. Specifically, two strategies, i.e. cluster space matching and salience feature weighting, are incorporated to alleviate the side-effect of noisy features incurred by high dimensionality. Additionally, cluster weights are bounded by the statistical means and minimums of the samples therein, making the learning rate also self-adaptable. Notably, SA-ART allows clusters to have their own sets of self-adaptable parameters. It has the same time complexity of Fuzzy ART and does not introduce additional hyperparameters that profile cluster properties. Comparative experiments have been conducted on the ImageNet and BlogCatalog datasets, which are large-scale and include sparsely-represented data. The results show that, SA-ART achieves 51.8% and 18.2% improvement over Fuzzy ART, respectively. While both have a similar time cost, SA-ART converges faster and can reach a better local minimum. In addition, SA-ART consistently outperforms six other state-of-the-art algorithms in terms of precision and F1 score. More importantly, it is much faster and exhibits stronger robustness to large and complex data.",
        "In this article, we propose an effective mixture model-based approach to modeling and clustering positive data vectors. Our mixture model is based on the inverted Beta-Liouville (IBL) distribution which is extracted from the family of Liouville distributions. To cope with the problem of determining the appropriate number of clusters in our approach, a nonparametric Bayesian framework is used to extend the IBL mixture to an infinite mixture model in which the number of clusters is assumed to be infinite initially and will be inferred automatically during the learning process. To optimize the proposed model, we propose a convergence-guaranteed learning algorithm based on the averaged collapsed variational Bayes inference that can effectively learn model parameters with closed-form solutions. The effectiveness of the proposed infinite IBL mixture model for modeling and clustering positive vectors is validated through both synthetic and real-world data sets.",
        "Electronic medical records (EMRs) support the development of machine learning algorithms for predicting disease incidence, patient response to treatment, and other healthcare events. But so far most algorithms have been centralized, taking little account of the decentralized, non-identically independently distributed (non-IID), and privacy-sensitive characteristics of EMRs that can complicate data collection, sharing and learning. To address this challenge, we introduced a community-based federated machine learning (CBFL) algorithm and evaluated it on non-IID ICU EMRs. Our algorithm clustered the distributed data into clinically meaningful communities that captured similar diagnoses and geographical locations, and learnt one model for each community. Throughout the learning process, the data was kept local at hospitals, while locally-computed results were aggregated on a server. Evaluation results show that CBFL outperformed the baseline federated machine learning (FL) algorithm in terms of Area Under the Receiver Operating Characteristic Curve (ROC AUC), Area Under the Precision-Recall Curve (PR AUC), and communication cost between hospitals and the server. Furthermore, communities' performance difference could be explained by how dissimilar one community was to others.",
        "In most of the application scenarios of industrial control systems, the switching threshold of a device, such as a street light system, is typically set to a fixed value. To meet the requirements for a smart city, it is necessary to set a threshold that is adaptive to different conditions by fusing the multi-attribute observations of the sensors. This paper proposes a multi-attribute fusion algorithm based on fuzzy clustering and improved evidence theory. All of the observations are clustered by fuzzy clustering, where a proper clustering method is chosen, and the improved evidence theory is used to fuse the observations. In the experiments, two-dimensional observations for the street light illumination and for the ambient illumination are used in a campus-intelligent lighting system based on a narrowband Internet of things, and the results demonstrate the effectiveness of the proposed fusion algorithm. The proposed algorithm can be applied to a variety of multi-attribute fusion scenarios.",
        "We are currently witnessing a paradigm shift from evidence-based medicine to precision medicine, which has been made possible by the enormous development of technology. The advances in data mining algorithms will allow us to integrate trans-omics with clinical data, contributing to our understanding of pathological mechanisms and massively impacting on the clinical sciences. Cluster analysis is one of the main data mining techniques and allows for the exploration of data patterns that the human mind cannot capture.This chapter focuses on the cluster analysis of clinical data, using the statistical software, R. We outline the cluster analysis process, underlining some clinical data characteristics. Starting with the data preprocessing step, we then discuss the advantages and disadvantages of the most commonly used clustering algorithms and point to examples of their applications in clinical work. Finally, we briefly discuss how to perform validation of clusters. Throughout the chapter we highlight R packages suitable for each computational step of cluster analysis.",
        "This paper presents SpCLUST, a new C++ package that takes a list of sequences as input, aligns them with MUSCLE, computes their similarity matrix in parallel and then performs the clustering. SpCLUST extends a previously released software by integrating additional scoring matrices which enables it to cover the clustering of amino-acid sequences. The similarity matrix is now computed in parallel according to the master/slave distributed architecture, using MPI. Performance analysis, realized on two real datasets of 100 nucleotide sequences and 1049 amino-acids ones, show that the resulting library substantially outperforms the original Python package. The proposed package was also intensively evaluated on simulated and real genomic and protein data sets. The clustering results were compared to the most known traditional tools, such as UCLUST, CD-HIT and DNACLUST. The comparison showed that SpCLUST outperforms the other tools when clustering divergent sequences, and contrary to the others, it does not require any user intervention or prior knowledge about the input sequences.",
        "Hierarchical clustering is an important tool for extracting information from data in a multi-resolution way. It is more meaningful if driven by data, as in the case of divisive algorithms, which split data until no more division is allowed. However, they have the drawback of the splitting threshold setting. The neural networks can address this problem, because they basically depend on data. The growing hierarchical GH-EXIN neural network builds a hierarchical tree in an incremental (data-driven architecture) and self-organized way. It is a top-down technique which defines the horizontal growth by means of an anisotropic region of influence, based on the novel idea of neighborhood convex hull. It also reallocates data and detects outliers by using a novel approach on all the leaves, simultaneously. Its complexity is estimated and an analysis of its user-dependent parameters is given. The advantages of the proposed approach, with regard to the best existing networks, are shown and analyzed, qualitatively and quantitatively, both in benchmark synthetic problems and in a real application (image recognition from video), in order to test the performance in building hierarchical trees. Furthermore, an important and very promising application of GH-EXIN in two-way hierarchical clustering, for the analysis of gene expression data in the study of the colorectal cancer is described.",
        "Clustering Molecular Dynamics trajectories is a common analysis that allows grouping together similar conformations. Several algorithms have been designed and optimized to perform this routine task, and among them, Quality Threshold stands as a very attractive option. This algorithm guarantees that in retrieved clusters no pair of frames will have a similarity value greater than a specified threshold, and hence, a set of strongly correlated frames are obtained for each cluster. In this work, it is shown that various commonly used software implementations are flawed by confusing Quality Threshold with another simplistic well-known clustering algorithm published by Daura et al. (Daura, X.; van Gunsteren, W. F.; Jaun, B.; Mark, A. E.; Gademann, K.; Seebach, D. Peptide Folding: When Simulation Meets Experiment. Angew. Chemie Int. Ed. 1999, 38 (1/2), 236-240). Daura's algorithm does not impose any quality threshold for the frames contained in retrieved clusters, bringing unrelated structural configurations altogether. The advantages of using Quality Threshold whenever possible to explore Molecular Dynamic trajectories is exemplified. An in-house implementation of the original Quality Threshold algorithm has been developed in order to illustrate our comments, and its code is freely available for further use by the scientific community.",
        "In competitive team sports, players maintain a certain formation during a game to achieve effective attacks and defenses. For the quantitative game analysis and assessment of team styles, we need a general framework that can characterize such formation structures dynamically. This paper develops a clustering algorithm for formations of multiple football (soccer) games based on the Delaunay method, which defines the formation of a team as an adjacency matrix of Delaunay triangulation. We first show that heat maps of entire football games can be clustered into several average formations: \"442\", \"4141\", \"433\", \"541\", and \"343\". Then, using hierarchical clustering, each average formation is further divided into more specific patterns (clusters) in which the configurations of players are different. Our method enables the visualization, quantitative comparison, and time-series analysis for formations in different time scales by focusing on transitions between clusters at each hierarchy. In particular, we can extract team styles from multiple games regarding the positional exchange of players within the formations. Applying our algorithm to the datasets comprising football games, we extract typical transition patterns of the formation for a particular team.",
        "The widespread adoption of online courses opens opportunities for analysing learner behaviour and optimising web-based learning adapted to observed usage. Here, we introduce a mathematical framework for the analysis of time-series of online learner engagement, which allows the identification of clusters of learners with similar online temporal behaviour directly from the raw data without prescribing a priori subjective reference behaviours. The method uses a dynamic time warping kernel to create a pair-wise similarity between time-series of learner actions, and combines it with an unsupervised multiscale graph clustering algorithm to identify groups of learners with similar temporal behaviour. To showcase our approach, we analyse task completion data from a cohort of learners taking an online post-graduate degree at Imperial Business School. Our analysis reveals clusters of learners with statistically distinct patterns of engagement, from distributed to massed learning, with different levels of regularity, adherence to pre-planned course structure and task completion. The approach also reveals outlier learners with highly sporadic behaviour. A posteriori comparison against student performance shows that, whereas high-performing learners are spread across clusters with diverse temporal engagement, low performers are located significantly in the massed learning cluster, and our unsupervised clustering identifies low performers more accurately than common machine learning classification methods trained on temporal statistics of the data. Finally, we test the applicability of the method by analysing two additional data sets: a different cohort of the same course, and time-series of different format from another university.",
        "Methods of suicide have received considerable attention in suicide research. The common approach to differentiate methods of suicide is the classification into \"violent\" versus \"non-violent\" method. Interestingly, since the proposition of this dichotomous differentiation, no further efforts have been made to question the validity of such a classification of suicides. This study aimed to challenge the traditional separation into \"violent\" and \"non-violent\" suicides by generating a cluster analysis with a data-driven, machine learning approach. In a retrospective analysis, data on all officially confirmed suicides (N=77,894) in Austria between 1970 and 2016 were assessed. Based on a defined distance metric between distributions of suicides over age group and month of the year, a standard hierarchical clustering method was performed with the five most frequent suicide methods. In cluster analysis, poisoning emerged as distinct from all other methods - both in the entire sample as well as in the male subsample. Violent suicides could be further divided into sub-clusters: hanging, shooting, and drowning on the one hand and jumping on the other hand. In the female sample, two different clusters were revealed - hanging and drowning on the one hand and jumping, poisoning, and shooting on the other. Our data-driven results in this large epidemiological study confirmed the traditional dichotomization of suicide methods into \"violent\" and \"non-violent\" methods, but on closer inspection \"violent methods\" can be further divided into sub-clusters and a different cluster pattern could be identified for women, requiring further research to support these refined suicide phenotypes.",
        "PURPOSE: The radiosensitization properties of gold nanoparticles (GNPs) are investigated using a simple Geant4 cell model considering a realistic cell geometry and a clustering algorithm to characterize the number of DNA double-strand breaks (DSBs). MATERIALS AND METHODS: A mixed-physics approach is taken for accurate modeling of low-energy photon interactions in the different regions of the model using Geant4-DNA physics within the cell, and Livermore physics within gold. Density-based spatial clustering of applications with noise (DBSCAN), a clustering algorithm, is used to directly quantitate DNA DSBs after irradiation. The simulation was run using different sizes of GNPs, different distances of GNPs from the cell nucleus, and several combinations of these two conditions. RESULTS: Four types of radiation were simulated in the work: 80-keV monoenergetic photons, 100-keV monoenergetic photons, a 250-kVp photon spectrum, and a 6-MV flattening filter free (FFF) photon spectrum. A variable enhancement in DSB yield, nucleus dose, and cell dose was observed when there are GNPs in the cell cytoplasm, and increases with larger GNPs and proximity to the nucleus. The distance of the GNPs from the nucleus has a large impact on the DSB yield and nucleus dose, but little to no effect on the cell dose. The cell dose enhancement factor of 80 keV photons varies from 1.037-1.125 at 0.2 microm for 30-100 nm GNPs to 1.040-1.127 at 4 mum. The DSB enhancement factor varies from 1.050 to 1.174 at 0.2 microm to a marginal effect of <1.01 at 4 mum. For 100 keV, the dose enhancement factor is from 1.142-1.470 at 0.2 microm to 1.106-1.371 at 4 mum. The DSB enhancement factor varies from 1.249-1.813 at 0.2 microm to almost no effect at 4 mum. For 250 kVp, the dose enhancement factor is from 1.117-1.393 at 0.2 mum to 1.110-1.342 at 4 mum. The DSB enhancement factor varies from 1.183-1.600 at 0.2 mum to a marginal effect of ~1.03 at 4 mum. A 6-MV FFF shows a dose enhancement factor of 1.061-1.103 at 0.2 mum and 1.053-1.107 at 4 mum. The DSB yield varies from 1.070-1.143 at 0.2 mum to a marginal effect at 4 mum. CONCLUSION: The stark difference in behavior for DSB yield when compared to cell dose highlights the importance of evaluating more complex radiobiological quantities rather than dose alone when evaluating the radiosensitization properties from metallic nanomaterials. The nucleus dose showed similar characteristics to the DSB yield demonstrating the ability of the method to predict DNA damage and its relationship with nuclear dose. The proposed method provides a way to explore the radiobiological mechanisms of radiation-induced DNA damages, and it aids to evaluate the physical radiosensitization properties of GNP-aided radiotherapy, which can be easily combined with radiochemical DSB quantitation in order to better understand the intricate DNA damage induction mechanisms that are involved in GNP-aided radiotherapy.",
        "Cardiovascular disease (CVD) refers to a state that indicates narrowed or blocked blood vessels, and it can lead to cardiac arrest, chest pain (angina) or stroke. CVD is a leading cause of silent massive heart attacks and is a major threat to life. The mere prediction of the presence or absence of CVD alone is inefficient in current scenarios. Rather, a major need has arisen for the prediction of CVD, the acquisition of knowledge about CVD and the assessment of the likelihood that an individual will experience cardiac arrest. The objective of establishing an individual CVD risk assessment has been attained in this paper using a hybrid model. The CVD of an individual is due to various controllable and uncontrollable factors. The computation and analysis of all these factors are difficult and time consuming. Only a few attributes are identified to be the most critical. This optimization of the critical features is performed using a modified Differential Evolution (DE) algorithm. The identified critical factors are sufficient to predict the presence/absence of CVD. In this paper, these identified critical features of individuals are considered using Cox regression analysis that evaluates the prevalence rates of the critical attributes. These individual prevalence rates together predict the cumulative prevalence ratios of the respective individuals. This cumulative prevalence ratio of an individual, along with the class attribute, is processed using the 2-means clustering technique to determine the risk of a particular individual developing CVD. The evaluation of the risk assessment model is carried out in this paper by calculating the prediction accuracy of the Cox regression analysis and the Davies-Bouldin (DB) index for 2-means clustering. The Cox regression analysis results in a 91% CVD prediction accuracy using the critical attributes and is comparatively higher than that of other models. The DB index of 2-means clustering with specific initial means for clusters of individuals with CVD is 0.282 and that for clusters of individuals without CVD is 0.2836, which are comparatively lower than those of the traditional k-means clustering algorithm.",
        "PURPOSE: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) measures changes in the concentration of an administered contrast agent to quantitatively evaluate blood circulation in a tumor or normal tissues. This method uses a pharmacokinetic analysis based on the time course of a reference region, such as muscle, rather than arterial input function. However, it is difficult to manually define a homogeneous reference region. In the present study, we developed a method for automatic extraction of the reference region using a clustering algorithm based on a time course pattern for DCE-MRI studies of patients with prostate cancer. METHODS: Two feature values related to the shape of the time course were extracted from the time course of all voxels in the DCE-MRI images. Each voxel value of T1-weighted images acquired before administration were also added as anatomical data. Using this three-dimensional feature vector, all voxels were segmented into five clusters by the Gaussian mixture model, and one of these clusters that included the gluteus muscle was selected as the reference region. RESULTS: Each region of arterial vessel, muscle, and fat was segmented as a different cluster from the tumor and normal tissues in the prostate. In the extracted reference region, other tissue elements including scattered fat and blood vessels were removed from the muscle region. CONCLUSIONS: Our proposed method can automatically extract the reference region using the clustering algorithm with three types of features based on the time course pattern and anatomical data. This method may be useful for evaluating tumor circulatory function in DCE-MRI studies.",
        "Structure determination of proteins and macromolecular complexes by single-particle cryo-electron microscopy (cryo-EM) is poised to revolutionize structural biology. An early challenging step in the cryo-EM pipeline is the detection and selection of particles from two-dimensional micrographs (particle picking). Most existing particle-picking methods require human intervention to deal with complex (irregular) particle shapes and extremely low signal-to-noise ratio (SNR) in cryo-EM images. Here, we design a fully automated super-clustering approach for single particle picking (SuperCryoEMPicker) in cryo-EM micrographs, which focuses on identifying, detecting, and picking particles of the complex and irregular shapes in micrographs with extremely low signal-to-noise ratio (SNR). Our method first applies advanced image processing procedures to improve the quality of the cryo-EM images. The binary mask image-highlighting protein particles are then generated from each individual cryo-EM image using the super-clustering (SP) method, which improves upon base clustering methods (i.e., k-means, fuzzy c-means (FCM), and intensity-based cluster (IBC) algorithm) via a super-pixel algorithm. SuperCryoEMPicker is tested and evaluated on micrographs of beta-galactosidase and 80S ribosomes, which are examples of cryo-EM data exhibiting complex and irregular particle shapes. The results show that the super-particle clustering method provides a more robust detection of particles than the base clustering methods, such as k-means, FCM, and IBC. SuperCryoEMPicker automatically and effectively identifies very complex particles from cryo-EM images of extremely low SNR. As a fully automated particle detection method, it has the potential to relieve researchers from laborious, manual particle-labeling work and therefore is a useful tool for cryo-EM protein structure determination.",
        "OBJECTIVE: The supply territories of main cerebral arteries are predominantly identified based on distribution of infarct lesions in patients with large arterial occlusion; whereas, there is no consensus atlas regarding the supply territories of smaller end-arteries. In this study, we applied a data-driven approach to construct a stroke atlas of the brain using hierarchical density clustering in large number of infarct lesions, assuming that voxels/regions supplied by a common end-artery tend to infarct together. METHODS: A total of 793 infarct lesions on MRI scans of 458 patients were segmented and coregistered to MNI-152 standard brain space. Applying a voxel-wise data-driven hierarchical density clustering algorithm, we identified those voxels that were most likely to be part of same infarct lesions in our dataset. A step-wise clustering scheme was applied, where the clustering threshold was gradually decreased to form the first 20 mother (>50cm(3)) or main (1-50cm(3)) clusters in addition to any possible number of tiny clusters (<1cm(3)); and then, any resultant mother clusters were iteratively subdivided using the same scheme. Also, in a randomly selected 2/3 subset of our cohort, a bootstrapping cluster analysis with 100 permutations was performed to assess the statistical robustness of proposed clusters. RESULTS: Approximately 91% of the MNI-152 brain mask was covered by 793 infarct lesions across patients. The covered area of brain was parcellated into 4 mother, 16 main, and 123 tiny clusters at the first hierarchy level. Upon iterative clustering subdivision of mother clusters, the brain tissue was eventually parcellated into 1 mother cluster (62.6cm(3)), 181 main clusters (total volume 1107.3cm(3)), and 917 tiny clusters (total volume of 264.8cm(3)). In bootstrap analysis, only 0.12% of voxels, were labelled as \"unstable\" - with a greater reachability distance in cluster scheme compared to their corresponding mean bootstrapped reachability distance. On visual assessment, the mother/main clusters were formed along supply territories of main cerebral arteries at initial hierarchical levels, and then tiny clusters emerged in deep white matter and gray matter nuclei prone to small vessel ischemic infarcts. CONCLUSIONS: Applying voxel-wise data-driven hierarchical density clustering on a large number of infarct lesions, we have parcellated the brain tissue into clusters of voxels that tend to be part of same infarct lesion, and presumably representing end-arterial supply territories. This hierarchical stroke atlas of the brain is shared publicly, and can potentially be applied for future infarct location-outcome analysis.",
        "OBJECTIVES: The aim of this study was to identify, with soft clustering methods, multimorbidity patterns in the electronic health records of a population >/=65 years, and to analyse such patterns in accordance with the different prevalence cut-off points applied. Fuzzy cluster analysis allows individuals to be linked simultaneously to multiple clusters and is more consistent with clinical experience than other approaches frequently found in the literature. DESIGN: A cross-sectional study was conducted based on data from electronic health records. SETTING: 284 primary healthcare centres in Catalonia, Spain (2012). PARTICIPANTS: 916 619 eligible individuals were included (women: 57.7%). PRIMARY AND SECONDARY OUTCOME MEASURES: We extracted data on demographics, International Classification of Diseases version 10 chronic diagnoses, prescribed drugs and socioeconomic status for patients aged >/=65. Following principal component analysis of categorical and continuous variables for dimensionality reduction, machine learning techniques were applied for the identification of disease clusters in a fuzzy c-means analysis. Sensitivity analyses, with different prevalence cut-off points for chronic diseases, were also conducted. Solutions were evaluated from clinical consistency and significance criteria. RESULTS: Multimorbidity was present in 93.1%. Eight clusters were identified with a varying number of disease values: nervous and digestive; respiratory, circulatory and nervous; circulatory and digestive; mental, nervous and digestive, female dominant; mental, digestive and blood, female oldest-old dominant; nervous, musculoskeletal and circulatory, female dominant; genitourinary, mental and musculoskeletal, male dominant; and non-specified, youngest-old dominant. Nuclear diseases were identified for each cluster independently of the prevalence cut-off point considered. CONCLUSIONS: Multimorbidity patterns were obtained using fuzzy c-means cluster analysis. They are clinically meaningful clusters which support the development of tailored approaches to multimorbidity management and further research.",
        "Single-cell RNA-sequencing (scRNA-seq) provides new opportunities to gain a mechanistic understanding of many biological processes. Current approaches for single cell clustering are often sensitive to the input parameters and have difficulty dealing with cell types with different densities. Here, we present Panoramic View (PanoView), an iterative method integrated with a novel density-based clustering, Ordering Local Maximum by Convex hull (OLMC), that uses a heuristic approach to estimate the required parameters based on the input data structures. In each iteration, PanoView will identify the most confident cell clusters and repeat the clustering with the remaining cells in a new PCA space. Without adjusting any parameter in PanoView, we demonstrated that PanoView was able to detect major and rare cell types simultaneously and outperformed other existing methods in both simulated datasets and published single-cell RNA-sequencing datasets. Finally, we conducted scRNA-Seq analysis of embryonic mouse hypothalamus, and PanoView was able to reveal known cell types and several rare cell subpopulations.",
        "Deciphering the key mechanisms of morphogenesis during embryonic development is crucial to understanding the guiding principles of the body plan and promote applications in biomedical research fields. Although several computational tissue reconstruction methods using cellular gene expression data have been proposed, those methods are insufficient with regard to arranging cells in their correct positions in tissues or organs unless spatial information is explicitly provided. Here, we report SPRESSO, a new in silico three-dimensional (3D) tissue reconstruction method using stochastic self-organizing map (stochastic-SOM) clustering, to estimate the spatial domains of cells in tissues or organs from only their gene expression profiles. With only five gene sets defined by Gene Ontology (GO), we successfully demonstrated the reconstruction of a four-domain structure of mid-gastrula mouse embryo (E7.0) with high reproducibility (success rate = 99%). Interestingly, the five GOs contain 20 genes, most of which are related to differentiation and morphogenesis, such as activin A receptor and Wnt family member genes. Further analysis indicated that Id2 is the most influential gene contributing to the reconstruction. SPRESSO may provide novel and better insights on the mechanisms of 3D structure formation of living tissues via informative genes playing a role as spatial discriminators.",
        "Monitoring of marine polluted areas is an emergency task, where efficiency and low-power consumption are challenging for the recovery of marine monitoring equipment. Wireless sensor networks (WSNs) offer the potential for low-energy recovery of marine observation beacons. Reducing and balancing network energy consumption are major problems for this solution. This paper presents an energy-saving clustering algorithm for wireless sensor networks based on k-means algorithm and fuzzy logic system (KFNS). The algorithm is divided into three phases according to the different demands of each recovery phase. In the monitoring phase, a distributed method is used to select boundary nodes to reduce network energy consumption. The cluster routing phase solves the extreme imbalance of energy of nodes for clustering. In the recovery phase, the inter-node weights are obtained based on the fuzzy membership function. The Dijkstra algorithm is used to obtain the minimum weight path from the node to the base station, and the optimal recovery order of the nodes is obtained by using depth-first search (DFS). We compare the proposed algorithm with existing representative methods. Experimental results show that the algorithm has a longer life cycle and a more efficient recovery strategy.",
        "Principal component analysis (PCA) is a widely used method for evaluating low-dimensional data. Some variants of PCA have been proposed to improve the interpretation of the principal components (PCs). One of the most common methods is sparse PCA which aims at finding a sparse basis to improve the interpretability over the dense basis of PCA. However, the performances of these improved methods are still far from satisfactory because the data still contain redundant PCs. In this paper, a novel method called PCA based on graph Laplacian and double sparse constraints (GDSPCA) is proposed to improve the interpretation of the PCs and consider the internal geometry of the data. In detail, GDSPCA utilizes L2,1-norm and L1-norm regularization terms simultaneously to enforce the matrix to be sparse by filtering redundant and irrelative PCs, where the L2,1-norm regularization term can produce row sparsity, while the L1-norm regularization term can enforce element sparsity. This way, we can make a better interpretation of the new PCs in low-dimensional subspace. Meanwhile, the method of GDSPCA integrates graph Laplacian into PCA to explore the geometric structure hidden in the data. A simple and effective optimization solution is provided. Extensive experiments on multi-view biological data demonstrate the feasibility and effectiveness of the proposed approach.",
        "Schizophrenia has a 1% incidence rate world-wide and those diagnosed present with positive (e.g. hallucinations, delusions), negative (e.g. apathy, asociality), and cognitive symptoms. However, both symptom burden and associated brain alterations are highly heterogeneous and intimately linked to prognosis. In this study, we present a method to predict individual symptom profiles by first deriving clinical subgroups and then using machine learning methods to perform subject-level classification based on magnetic resonance imaging (MRI) derived neuroanatomical measures. Symptomatic and MRI data of 167 subjects were used. Subgroups were defined using hierarchical clustering of clinical data resulting in 3 stable clusters: 1) high symptom burden, 2) predominantly positive symptom burden, and 3) mild symptom burden. Cortical thickness estimates were obtained in 78 regions of interest and were input, along with demographic data, into three machine learning models (logistic regression, support vector machine, and random forest) to predict subgroups. Random forest performance metrics for predicting the group membership of the high and mild symptom burden groups exceeded those of the baseline comparison of the entire schizophrenia population versus normal controls (AUC: 0.81 and 0.78 vs. 0.75). Additionally, an analysis of the most important features in the random forest classification demonstrated consistencies with previous findings of regional impairments and symptoms of schizophrenia.",
        "In this paper, a new approach is proposed for localization and segmentation of the optic disc in human retina images. This new approach can find the boundary of the optic disc by an initial fuzzy clustering means algorithm. The proposed approach uses active contour model evolution based on a fuzzy clustering algorithm. The robustness of the proposed approach was evaluated with retinal imaging medical databases, such as DRIVE, STARE, DIARETDB1, and DRIONS. These bases contained images affected by different abnormalities, for example, diabetes, retinitis pigmentosa, and age-related macular degeneration AMD. A success detection rate with 100% accuracy was achieved for the DRIVE, DIRATEDB1, and DRIONS-DB databases, and 97.53% for the STARE database. For the optic disc segmentation, the method achieved an average accuracy and overlap in the range of 97.01-99.46% and 78.35-84.56% in these four databases. The result was compared with various methods in the literature, and it was concluded that the proposed method is more accurate than the other existing methods. Graphical abstract.",
        "The deoxyribonucleic acid (DNA) molecule damage simulations with an atom level geometric model use the traversal algorithm that has the disadvantages of quite time-consuming, slow convergence and high-performance computer requirement. Therefore, this work presents a density-based spatial clustering of applications with noise (DBSCAN) clustering algorithm based on the spatial distributions of energy depositions and hydroxyl radicals (.OH). The algorithm with probability and statistics can quickly get the DNA strand break yields and help to study the variation pattern of the clustered DNA damage. Firstly, we simulated the transportation of protons and secondary particles through the nucleus, as well as the ionization and excitation of water molecules by using Geant4-DNA that is the Monte Carlo simulation toolkit for radiobiology, and got the distributions of energy depositions and hydroxyl radicals. Then we used the damage probability functions to get the spatial distribution dataset of DNA damage points in a simplified geometric model. The DBSCAN clustering algorithm based on damage points density was used to determine the single-strand break (SSB) yield and double-strand break (DSB) yield. Finally, we analyzed the DNA strand break yield variation trend with particle linear energy transfer (LET) and summarized the variation pattern of damage clusters. The simulation results show that the new algorithm has a faster simulation speed than the traversal algorithm and a good precision result. The simulation results have consistency when compared to other experiments and simulations. This work achieves more precise information on clustered DNA damage induced by proton radiation at the molecular level with high speed, so that it provides an essential and powerful research method for the study of radiation biological damage mechanism.",
        "Clustering homologous sequences based on their similarity is a problem that appears in many bioinformatics applications. The fact that sequences cluster is ultimately the result of their phylogenetic relationships. Despite this observation and the natural ways in which a tree can define clusters, most applications of sequence clustering do not use a phylogenetic tree and instead operate on pairwise sequence distances. Due to advances in large-scale phylogenetic inference, we argue that tree-based clustering is under-utilized. We define a family of optimization problems that, given an arbitrary tree, return the minimum number of clusters such that all clusters adhere to constraints on their heterogeneity. We study three specific constraints, limiting (1) the diameter of each cluster, (2) the sum of its branch lengths, or (3) chains of pairwise distances. These three problems can be solved in time that increases linearly with the size of the tree, and for two of the three criteria, the algorithms have been known in the theoretical computer scientist literature. We implement these algorithms in a tool called TreeCluster, which we test on three applications: OTU clustering for microbiome data, HIV transmission clustering, and divide-and-conquer multiple sequence alignment. We show that, by using tree-based distances, TreeCluster generates more internally consistent clusters than alternatives and improves the effectiveness of downstream applications. TreeCluster is available at https://github.com/niemasd/TreeCluster.",
        "We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78+/-8.76 which is significantly better than the 133.17+/-9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97+/-10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",
        "In identifying subgroups of a heterogeneous disease or condition, it is often desirable to identify both the observations and the features which differ between subgroups. For instance, it may be that there is a subgroup of individuals with a certain disease who differ from the rest of the population based on the expression profile for only a subset of genes. Identifying the subgroup of patients and subset of genes could lead to better-targeted therapy. We can represent the subgroup of individuals and genes as a bicluster, a submatrix, U , of a larger data matrix, X , such that the features and observations in U differ from those not contained in U . We present a novel two-step method, SC-Biclust, for identifying U . In the first step, the observations in the bicluster are identified to maximize the sum of the weighted between-cluster feature differences. In the second step, features in the bicluster are identified based on their contribution to the clustering of the observations. This versatile method can be used to identify biclusters that differ on the basis of feature means, feature variances, or more general differences. The bicluster identification accuracy of SC-Biclust is illustrated through several simulated studies. Application of SC-Biclust to pain research illustrates its ability to identify biologically meaningful subgroups.",
        "Rapid advance in single-cell RNA sequencing (scRNA-seq) allows measurement of the expression of genes at single-cell resolution in complex disease or tissue. While many methods have been developed to detect cell clusters from the scRNA-seq data, this task currently remains a main challenge. We proposed a multi-objective optimization-based fuzzy clustering approach for detecting cell clusters from scRNA-seq data. First, we conducted initial filtering and SCnorm normalization. We considered various case studies by selecting different cluster numbers ( c l = 2 to a user-defined number), and applied fuzzy c-means clustering algorithm individually. From each case, we evaluated the scores of four cluster validity index measures, Partition Entropy ( P E ), Partition Coefficient ( P C ), Modified Partition Coefficient ( M P C ), and Fuzzy Silhouette Index ( F S I ). Next, we set the first measure as minimization objective ( downward arrow) and the remaining three as maximization objectives ( upward arrow), and then applied a multi-objective decision-making technique, TOPSIS, to identify the best optimal solution. The best optimal solution (case study) that had the highest TOPSIS score was selected as the final optimal clustering. Finally, we obtained differentially expressed genes (DEGs) using Limma through the comparison of expression of the samples between each resultant cluster and the remaining clusters. We applied our approach to a scRNA-seq dataset for the rare intestinal cell type in mice [GEO ID: GSE62270, 23,630 features (genes) and 288 cells]. The optimal cluster result (TOPSIS optimal score= 0.858) comprised two clusters, one with 115 cells and the other 91 cells. The evaluated scores of the four cluster validity indices, F S I , P E , P C , and M P C for the optimized fuzzy clustering were 0.482, 0.578, 0.607, and 0.215, respectively. The Limma analysis identified 1240 DEGs (cluster 1 vs. cluster 2). The top ten gene markers were Rps21, Slc5a1, Crip1, Rpl15, Rpl3, Rpl27a, Khk, Rps3a1, Aldob and Rps17. In this list, Khk (encoding ketohexokinase) is a novel marker for the rare intestinal cell type. In summary, this method is useful to detect cell clusters from scRNA-seq data.",
        "Chromatin immunoprecipitation combined with next-generation sequencing (ChIP-Seq) technology has enabled the identification of transcription factor binding sites (TFBSs) on a genome-wide scale. To effectively and efficiently discover TFBSs in the thousand or more DNA sequences generated by a ChIP-Seq data set, we propose a new algorithm named AP-ChIP. First, we set two thresholds based on probabilistic analysis to construct and further filter the cluster subsets. Then, we use Affinity Propagation (AP) clustering on the candidate cluster subsets to find the potential motifs. Experimental results on simulated data show that the AP-ChIP algorithm is able to make an almost accurate prediction of TFBSs in a reasonable time. Also, the validity of the AP-ChIP algorithm is tested on a real ChIP-Seq data set.",
        "Cancer subtyping is of great importance for the prediction, diagnosis, and precise treatment of cancer patients. Many clustering methods have been proposed for cancer subtyping. In 2014, a clustering algorithm named Clustering by Fast Search and Find of Density Peaks (CFDP) was proposed and published in Science, which has been applied to cancer subtyping and achieved attractive results. However, CFDP requires to set two key parameters (cluster centers and cutoff distance) manually, while their optimal values are difficult to be determined. To overcome this limitation, an automatic clustering method named PSO-CFDP is proposed in this paper, in which cluster centers and cutoff distance are automatically determined by running an improved particle swarm optimization (PSO) algorithm multiple times. Experiments using PSO-CFDP, as well as LR-CFDP, STClu, CH-CCFDAC, and CFDP, were performed on four benchmark data-sets and two real cancer gene expression datasets. The results show that PSO-CFDP can determine cluster centers and cutoff distance automatically within controllable time/cost and, therefore, improve the accuracy of cancer subtyping.",
        "Recent studies have shown that many essential genes (EGs) change their essentiality across various contexts. Finding contextual EGs in pathogenic conditions may facilitate the identification of therapeutic targets. We propose link clustering as an indicator of contextual EGs that are non-central in protein-protein interaction (PPI) networks. In various human and yeast PPI networks, we found that 29-47% of EGs were better characterized by link clustering than by centrality. Importantly, non-central EGs were prone to change their essentiality across different human cell lines and between species. Compared with central EGs and non-EGs, non-central EGs had intermediate levels of expression and evolutionary conservation. In addition, non-central EGs exhibited a significant impact on communities at lower hierarchical levels, suggesting that link clustering is associated with contextual essentiality, as it depicts locally important nodes in network structures.",
        "Neuroprosthesis enables the brain control on the external devices purely using neural activity for paralyzed people. Supervised learning decoders recalibrate or re-fit the discrepancy between the desired target and decoder's output, where the correction may over-dominate the user's intention. Reinforcement learning decoder allows users to actively adjust their brain patterns through trial and error, which better represents the subject's motive. The computational challenge is to quickly establish new state-action mapping before the subject becomes frustrated. Recently proposed quantized attention-gated kernel reinforcement learning (QAGKRL) explores the optimal nonlinear neural-action mapping in the Reproducing Kernel Hilbert Space (RKHS). However, considering all past data in RKHS is less efficient and sensitive to detect the new neural patterns emerging in brain control. In this paper, we propose a clustering-based kernel RL algorithm. New neural patterns emerge and are clustered to represent the novel knowledge in brain control. The current neural data only activate the nearest subspace in RKHS for more efficient decoding. The dynamic clustering makes our algorithm more sensitive to new brain patterns. We test our algorithm on both the synthetic and real-world spike data. Compared with QAGKRL, our algorithm can achieve a quicker knowledge adaptation in brain control with less computational complexity.",
        "To overcome the two-class imbalanced classification problem existing in the diagnosis of breast cancer, a hybrid of Random Over Sampling Example, K-means and Support vector machine (RK-SVM) model is proposed which is based on sample selection. Random Over Sampling Example (ROSE) is utilized to balance the dataset and further improve the diagnosis accuracy by Support Vector Machine (SVM). As there is one different sample selection factor via clustering that encourages selecting the samples near the class boundary. The purpose of clustering here is to reduce the risk of removing useful samples and improve the efficiency of sample selection. To test the performance of the new hybrid classifier, it is implemented on breast cancer datasets and the other three datasets from the University of California Irvine (UCI) machine learning repository, which are commonly used datasets in class imbalanced learning. The extensive experimental results show that our proposed hybrid method outperforms most of the competitive algorithms in term of G-mean and accuracy indices. Additionally, experimental results show that this method also performs superiorly for binary problems.",
        "PURPOSE: Lymphoma detection and segmentation from PET images are critical tasks for cancer staging and treatment monitoring. However, it is still a challenge owing to the complexities of lymphoma PET data themselves, and the huge computational burdens and memory requirements for 3D volume data. In this work, an entropy-based optimization strategy for clustering is proposed to detect and segment lymphomas in 3D PET images. METHODS: To reduce computational complexity and add more feature information, billions of voxels in 3D volume data are first aggregated into supervoxels. Then, such supervoxels serve as basic data units for further clustering by using DBSCAN algorithm, in which some new feature attributes based on physical spatial information and prior knowledge are proposed. In addition, more importantly, an entropy-based objective function is constructed to search the most appropriate parameters of DBSCAN to obtain the optimal clustering results by using a genetic algorithm. This step allows to automatically adapt the parameters to each patient. Finally, a series of comparison experiments among various feature attributes are performed. RESULTS: 48 patient data are conducted, showing the combination of three features, supervoxel intensity, geographic coordinates and organ distributions, can achieve good performance and the proposed entropy-based optimization scheme has more advantages than the existing methods. CONCLUSION: The proposed entropy-based optimization strategy for clustering by integrating physical spatial attributes and prior knowledge can achieve better performance than traditional methods.",
        "Trajectory clustering and path modelling are two core tasks in intelligent transport systems with a wide range of applications, from modeling drivers' behavior to traffic monitoring of road intersections. Traditional trajectory analysis considers them as separate tasks, where the system first clusters the trajectories into a known number of clusters and then the path taken in each cluster is modelled. However, such a hierarchy does not allow the knowledge of the path model to be used to improve the performance of trajectory clustering. Based on the distance dependent Chinese restaurant process (DDCRP), a trajectory analysis system that simultaneously performs trajectory clustering and path modelling was proposed. Unlike most traditional approaches where the number of clusters should be known, the proposed method decides the number of clusters automatically. The proposed algorithm was tested on two publicly available trajectory datasets, and the experimental results recorded better performance and considerable improvement in both datasets for the task of trajectory clustering compared to traditional approaches. The study proved that the proposed method is an appropriate candidate to be used for trajectory clustering and path modelling.",
        "Fingerprints have long been used in automated fingerprint identification or verification systems. Singular points (SPs), namely the core and delta point, are the basic features widely used for fingerprint registration, orientation field estimation, and fingerprint classification. In this study, we propose an adaptive method to detect SPs in a fingerprint image. The algorithm consists of three stages. First, an innovative enhancement method based on singular value decomposition is applied to remove the background of the fingerprint image. Second, a blurring detection and boundary segmentation algorithm based on the innovative image enhancement is proposed to detect the region of impression. Finally, an adaptive method based on wavelet extrema and the Henry system for core point detection is proposed. Experiments conducted using the FVC2002 DB1 and DB2 databases prove that our method can detect SPs reliably.",
        "Background and objectives: Assessment of drugs toxicity and associated biomarker genes is one of the most important tasks in the pre-clinical phase of drug development pipeline as well as in toxicogenomic studies. There are few statistical methods for the assessment of doses of drugs (DDs) toxicity and their associated biomarker genes. However, these methods consume more time for computation of the model parameters using the EM (expectation-maximization) based iterative approaches. To overcome this problem, in this paper, an attempt is made to propose an alternative approach based on hierarchical clustering (HC) for the same purpose. Methods and materials: There are several types of HC approaches whose performance depends on different similarity/distance measures. Therefore, we explored suitable combinations of distance measures and HC methods based on Japanese Toxicogenomics Project (TGP) datasets for better clustering/co-clustering between DDs and genes as well as to detect toxic DDs and their associated biomarker genes. Results: We observed that Word's HC method with each of Euclidean, Manhattan, and Minkowski distance measures produces better clustering/co-clustering results. For an example, in the case of the glutathione metabolism pathway (GMP) dataset LOC100359539/Rrm2, Gpx6, RGD1562107, Gstm4, Gstm3, G6pd, Gsta5, Gclc, Mgst2, Gsr, Gpx2, Gclm, Gstp1, LOC100912604/Srm, Gstm4, Odc1, Gsr, Gss are the biomarker genes and Acetaminophen_Middle, Acetaminophen_High, Methapyrilene_High, Nitrofurazone_High, Nitrofurazone_Middle, Isoniazid_Middle, Isoniazid_High are their regulatory (associated) DDs explored by our proposed co-clustering algorithm based on the distance and HC method combination Euclidean: Word. Similarly, for the peroxisome proliferator-activated receptor signaling pathway (PPAR-SP) dataset Cpt1a, Cyp8b1, Cyp4a3, Ehhadh, Plin5, Plin2, Fabp3, Me1, Fabp5, LOC100910385, Cpt2, Acaa1a, Cyp4a1, LOC100365047, Cpt1a, LOC100365047, Angptl4, Aqp7, Cpt1c, Cpt1b, Me1 are the biomarker genes and Aspirin_Low, Aspirin_Middle, Aspirin_High, Benzbromarone_Middle, Benzbromarone_High, Clofibrate_Middle, Clofibrate_High, WY14643_Low, WY14643_High, WY14643_Middle, Gemfibrozil_Middle, Gemfibrozil_High are their regulatory DDs. Conclusions: Overall, the methods proposed in this article, co-cluster the genes and DDs as well as detect biomarker genes and their regulatory DDs simultaneously consuming less time compared to other mentioned methods. The results produced by the proposed methods have been validated by the available literature and functional annotation.",
        "Identifying those patient groups, who have unwanted outcomes, in the early stages is crucial to providing the most appropriate level of care. In this study, we intend to find distinctive patterns in health service use (HSU) of transport accident injured patients within the first week post-injury. Aiming those patterns that are associated with the outcome of interest. To recognize these patterns, we propose a multi-objective optimization model that minimizes the k-medians cost function and regression error simultaneously. Thus, we use a semi-supervised clustering approach to identify patient groups based on HSU patterns and their association with total cost. To solve the optimization problem, we introduce an evolutionary algorithm using stochastic gradient descent and Pareto optimal solutions. As a result, we find the best optimal clusters by minimizing both objective functions. The results show that the proposed semi-supervised approach identifies distinct groups of HSUs and contributes to predict total cost. Also, the experiments prove the performance of the multi-objective approach in comparison with single- objective approaches.",
        "Remote sensing big data (RSBD) is generally characterized by huge volumes, diversity, and high dimensionality. Mining hidden information from RSBD for different applications imposes significant computational challenges. Clustering is an important data mining technique widely used in processing and analyzing remote sensing imagery. However, conventional clustering algorithms are designed for relatively small datasets. When applied to problems with RSBD, they are, in general, too slow or inefficient for practical use. In this paper, we proposed a parallel subsampling-based clustering (PARSUC) method for improving the performance of RSBD clustering in terms of both efficiency and accuracy. PARSUC leverages a novel subsampling-based data partitioning (SubDP) method to realize three-step parallel clustering, effectively solving the notable performance bottleneck of the existing parallel clustering algorithms; that is, they must cope with numerous repeated calculations to get a reasonable result. Furthermore, we propose a centroid filtering algorithm (CFA) to eliminate subsampling errors and to guarantee the accuracy of the clustering results. PARSUC was implemented on a Hadoop platform by using the MapReduce parallel model. Experiments conducted on massive remote sensing imageries with different sizes showed that PARSUC (1) provided much better accuracy than conventional remote sensing clustering algorithms in handling larger image data; (2) achieved notable scalability with increased computing nodes added; and (3) spent much less time than the existing parallel clustering algorithm in handling RSBD.",
        "The identification of generalizable treatment response classes (TRC[s]) in major depressive disorder (MDD) would facilitate comparisons across studies and the development of treatment prediction algorithms. Here, we investigated whether such stable TRCs can be identified and predicted by clinical baseline items. We analyzed data from an observational MDD cohort (Munich Antidepressant Response Signature [MARS] study, N = 1017), treated individually by psychopharmacological and psychotherapeutic means, and a multicenter, partially randomized clinical/pharmacogenomic study (Genome-based Therapeutic Drugs for Depression [GENDEP], N = 809). Symptoms were evaluated up to week 16 (or discharge) in MARS and week 12 in GENDEP. Clustering was performed on 809 MARS patients (discovery sample) using a mixed model with the integrated completed likelihood criterion for the assessment of cluster stability, and validated through a distinct MARS validation sample and GENDEP. A random forest algorithm was used to identify prediction patterns based on 50 clinical baseline items. From the clustering of the MARS discovery sample, seven TRCs emerged ranging from fast and complete response (average 4.9 weeks until discharge, 94% remitted patients) to slow and incomplete response (10% remitted patients at week 16). These proved stable representations of treatment response dynamics in both the MARS and the GENDEP validation sample. TRCs were strongly associated with established response markers, particularly the rate of remitted patients at discharge. TRCs were predictable from clinical items, particularly personality items, life events, episode duration, and specific psychopathological features. Prediction accuracy improved significantly when cluster-derived slopes were modelled instead of individual slopes. In conclusion, model-based clustering identified distinct and clinically meaningful treatment response classes in MDD that proved robust with regard to capturing response profiles of differently designed studies. Response classes were predictable from clinical baseline characteristics. Conceptually, model-based clustering is translatable to any outcome measure and could advance the large-scale integration of studies on treatment efficacy or the neurobiology of treatment response.",
        "BACKGROUND: microRNA (miRNA) is a short RNA (~ 22 nt) that regulates gene expression at the posttranscriptional level. Aberration of miRNA expressions could affect their targeting mRNAs involved in cancer-related signaling pathways. We conduct clustering analysis of miRNA and mRNA using expression data from the Cancer Genome Atlas (TCGA). We combine the Hungarian algorithm and blossom algorithm in graph theory. Data analysis is done using programming language R and Python. METHODS: We first quantify edge-weights of the miRNA-mRNA pairs by combining their expression correlation coefficient in tumor (T_CC) and correlation coefficient in normal (N_CC). We thereby introduce a bipartite graph partition procedure to identify cluster candidates. Specifically, we propose six weight formulas to quantify the change of miRNA-mRNA expression T_CC relative to N_CC, and apply the traditional hierarchical clustering to subjectively evaluate the different weight formulas of miRNA-mRNA pairs. Among these six different weight formulas, we choose the optimal one, which we define as the integrated mean value weights, to represent the connections between miRNA and mRNAs. Then the Hungarian algorithm and the blossom algorithm are employed on the miRNA-mRNA bipartite graph to passively determine the clusters. The combination of Hungarian and the blossom algorithms is dubbed maximum weighted merger method (MWMM). RESULTS: MWMM identifies clusters of different sizes that meet the mathematical criterion that internal connections inside a cluster are relatively denser than external connections outside the cluster and biological criterion that the intra-cluster Gene Ontology (GO) term similarities are larger than the inter-cluster GO term similarities. MWMM is developed using breast invasive carcinoma (BRCA) as training data set, but can also applies to other cancer type data sets. MWMM shows advantage in GO term similarity in most cancer types, when compared to other algorithms. CONCLUSIONS: miRNAs and mRNAs that are likely to be affected by common underlying causal factors in cancer can be clustered by MWMM approach and potentially be used as candidate biomarkers for different cancer types and provide clues for targets of precision medicine in cancer treatment.",
        "Single-cell RNA sequencing (scRNA-seq) technology provides quantitative gene expression profiles at single-cell resolution. As a result, researchers have established new ways to explore cell population heterogeneity and genetic variability of cells. One of the current research directions for scRNA-seq data is to identify different cell types accurately through unsupervised clustering methods. However, scRNA-seq data analysis is challenging because of their high noise level, high dimensionality and sparsity. Moreover, the impact of multiple latent factors on gene expression heterogeneity and on the ability to accurately identify cell types remains unclear. How to overcome these challenges to reveal the biological difference between cell types has become the key to analyze scRNA-seq data. For these reasons, the unsupervised learning for cell population discovery based on scRNA-seq data analysis has become an important research area. A cell similarity assessment method plays a significant role in cell clustering. Here, we present BioRank, a new cell similarity assessment method based on annotated gene sets and gene ranks. To evaluate the performances, we cluster cells by two classical clustering algorithms based on the similarity between cells obtained by BioRank. In addition, BioRank can be used by any clustering algorithm that requires a similarity matrix. Applying BioRank to 12 public scRNA-seq datasets, we show that it is better than or at least as well as several popular similarity assessment methods for single cell clustering.",
        "MOTIVATION: The identification of sub-populations of patients with similar characteristics, called patient subtyping, is important for realizing the goals of precision medicine. Accurate subtyping is crucial for tailoring therapeutic strategies that can potentially lead to reduced mortality and morbidity. Model-based clustering, such as Gaussian mixture models, provides a principled and interpretable methodology that is widely used to identify subtypes. However, they impose identical marginal distributions on each variable; such assumptions restrict their modeling flexibility and deteriorates clustering performance. RESULTS: In this paper, we use the statistical framework of copulas to decouple the modeling of marginals from the dependencies between them. Current copula-based methods cannot scale to high dimensions due to challenges in parameter inference. We develop HD-GMCM, that addresses these challenges and, to our knowledge, is the first copula-based clustering method that can fit high-dimensional data. Our experiments on real high-dimensional gene-expression and clinical datasets show that HD-GMCM outperforms state-of-the-art model-based clustering methods, by virtue of modeling non-Gaussian data and being robust to outliers through the use of Gaussian mixture copulas. We present a case study on lung cancer data from TCGA. Clusters obtained from HD-GMCM can be interpreted based on the dependencies they model, that offers a new way of characterizing subtypes. Empirically, such modeling not only uncovers latent structure that leads to better clustering but also meaningful clinical subtypes in terms of survival rates of patients. AVAILABILITY AND IMPLEMENTATION: An implementation of HD-GMCM in R is available at: https://bitbucket.org/cdal/hdgmcm/. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "The accessibility of a huge amount of protein-protein interaction (PPI) data has allowed to do research on biological networks that reveal the structure of a protein complex, pathways and its cellular organization. A key demand in computational biology is to recognize the modular structure of such biological networks. The detection of protein complexes from the PPI network, is one of the most challenging and significant problems in the post-genomic era. In Bioinformatics, the frequently employed approach for clustering the networks is Markov Clustering (MCL). Many of the researches for protein complex detection were done on the static PPI network, which suffers from a few drawbacks. To resolve this problem, this paper proposes an approach to detect the dynamic protein complexes through Markov Clustering based on Elephant Herd Optimization Approach (DMCL-EHO). Initially, the proposed method divides the PPI network into a set of dynamic subnetworks under various time points by combining the gene expression data and secondly, it employs the clustering analysis on every subnetwork using the MCL along with Elephant Herd Optimization approach. The experimental analysis was employed on different PPI network datasets and the proposed method surpasses various existing approaches in terms of accuracy measures. This paper identifies the common protein complexes that are expressively enriched in gold-standard datasets and also the pathway annotations of the detected protein complexes using the KEGG database.",
        "Due to the limitation of the fixed structures of neighborhood windows, the quality of spatial information obtained from the neighborhood pixels may be affected by noise. In order to compensate this drawback, a robust fuzzy c-means clustering with non-neighborhood spatial information (FCM_NNS) is presented. Through incorporating non-neighborhood spatial information, the robustness performance of the proposed FCM_NNS with respect to the noise can be significantly improved. The results indicate that FCM_NNS is very effective and robust to noisy aliasing images. Moreover, the comparison of other seven roughness indexes indicates that the proposed FCM_NNS-based F index can characterize the aliasing degree in the surface images and is highly correlated with surface roughness (R(2) = 0.9327 for thirty grinding samples).",
        "There are more than 2000 pedestrians reported to be involved in traffic crashes with vehicles in North Carolina every year. 10%-20% of them are killed or severely injured. Research studies need to be conducted in order to identify the contributing factors and develop countermeasures to improve safety for pedestrians. However, due to the heterogeneity inherent in crash data, which arises from unobservable factors that are not reported by law enforcement agencies and/or cannot be collected from state crash records, it is not easy to identify and evaluate factors that affect the injury severity of pedestrians in such crashes. By taking advantage of the latent class clustering (LCC), this research firstly applies the LCC approach to identify the latent classes and classify the crashes with different distribution characteristics of contributing factors to the pedestrian-vehicle crashes. By considering the inherent ordered nature of the traffic crash severity data, a partial proportional odds (PPO) model is then developed and utilized to explore the major factors that significantly affect the pedestrian injury severities resulting from pedestrian-vehicle crashes for each latent class previously obtained in the LCC. This study uses police reported pedestrian crash data collected from 2007 to 2014 in North Carolina, containing a variety of features of motorist, pedestrian, environmental, roadway characteristics. Parameter estimates and associated marginal effects are mainly used to interpret the models and evaluate the significance of each independent variable. Lastly, policy recommendations are made and future research directions are also given.",
        "Heart rate has been measured comfortably using a camera without the skin-contact by the development of vision-based measurement. Despite the potential of the vision-based measurement, it has still presented limited ability due to the noise of illumination variance and motion artifacts. Remote ballistocardiography (BCG) was used to estimate heart rate from the ballistocardiographic head movements generated by the flow of blood through the carotid arteries. It was robust to illumination variance but still limited in the motion artifacts such as facial expressions and voluntary head motions. Recent studies on remote BCG focus on the improvement of signal extraction by minimizing the motion artifacts. They simply estimated the heart rate from the cardiac signal using peak detection and fast fourier transform (FFT). However, the heart rate estimation based on peak detection and FFT depend on the robust signal estimation. Thus, if the cardiac signal is contaminated with some noise, the heart rate cannot be estimated accurately. This study aimed to develop a novel method to improve heart rate estimation from ballistocardiographic head movements using the unsupervised clustering. First, the ballistocardiographic head movements were measured from facial video by detecting facial points using the good-feature-to-track (GFTT) algorithm and by tracking using the Kanade-Lucas-Tomasi (KLT) tracker. Second, the cardiac signal was extracted from the ballistocardiographic head movements by bandpass filter and principal component analysis (PCA). The relative power density (RPD) was extracted from its power spectrum between 0.75 Hz and 2.5 Hz. Third, the unsupervised clustering was performed to construct a model to estimate the heart rate from the RPD using the dataset consisting of the RPD and the heart rate measured from electrocardiogram (ECG). Finally, the heart rate was estimated from the RPD using the model. The proposed method was verified by comparing it with previous methods using the peak detection and the FFT. As a result, the proposed method estimated a more accurate heart rate than previous methods in three experiments by levels of the motion artifacts consisting of facial expressions and voluntary head motions. The four main contributions are as follows: (1) the unsupervised clustering improved the heart rate estimation by overcoming the motion artifacts (i.e., facial expressions and voluntary head motions); (2) the proposed method was verified by comparing with the previous methods using the peak detection and the FFT; (3) the proposed method can be combined with existing vision-based measurement and can improve their performance; (4) the proposed method was tested by three experiments considering the realistic environment including the motion artifacts, thus, it increases the possibility of the non-contact measurement in daily life.",
        "In this paper, we propose a human action recognition method using HOIRM (histogram of oriented interest region motion) feature fusion and a BOW (bag of words) model based on AP (affinity propagation) clustering. First, a HOIRM feature extraction method based on spatiotemporal interest points ROI is proposed. HOIRM can be regarded as a middle-level feature between local and global features. Then, HOIRM is fused with 3D HOG and 3D HOF local features using a cumulative histogram. The method further improves the robustness of local features to camera view angle and distance variations in complex scenes, which in turn improves the correct rate of action recognition. Finally, a BOW model based on AP clustering is proposed and applied to action classification. It obtains the appropriate visual dictionary capacity and achieves better clustering effect for the joint description of a variety of features. The experimental results demonstrate that by using the fused features with the proposed BOW model, the average recognition rate is 95.75% in the KTH database, and 88.25% in the UCF database, which are both higher than those by using only 3D HOG+3D HOF or HOIRM features. Moreover, the average recognition rate achieved by the proposed method in the two databases is higher than that obtained by other methods.",
        "Background: In this study, we used a variety of factors that affect urbanization in Iran to evaluate different provinces in Iran in terms of the level of urbanization. Methods: Using information from census 2011, we collected data on 33 indicators related to urbanization in 31 provinces in Iran. To rank the provinces we used density-based hierarchical clustering scheme. To determine similarities or differences between the provinces, the square of the Euclidean distance dissimilarity coefficient; Ward's algorithm was used to merge the provinces to minimize intra-cluster variance. One-way analysis of variance (ANOVA) was used to determine the variance between the variables used to rank the provinces in terms of different levels of urbanization. Statistical analysis was performed using SPSS. Results: The provinces in Iran were combined with each other in 30 stages and classified into four levels. Taking into account the variables used to rank the level of urbanization, Tehran, and Alborz provinces were at the highest level of urbanization. On the other hand, the provinces of Sistan and Baluchistan, Kerman, North Khorasan, South Khorasan, Hormozgan, and Bushehr were at the lowest level of urbanization. Conclusion: Identification of provinces at the same level of urbanization can help us to discover the strengths and weaknesses in the infrastructures of each of them. Given the differences between various levels of urbanization, the identification of factors that are effective in the process of urbanization can help to access more information required for designing plans for the years to come.",
        "Simulations of acoustic wave propagation, including both the forward and the backward propagations of the wave (also known as full-wave simulations), are increasingly utilized in ultrasound imaging due to their ability to more accurately model important acoustic phenomena. Realistic anatomic models, particularly those of the abdominal wall, are needed to take full advantage of the capabilities of these simulation tools. We describe a method for converting fat-water-separated magnetic resonance imaging (MRI) volumes to anatomical models for ultrasound simulations. These acoustic models are used to map acoustic imaging parameters, such as speed of sound and density, to grid points in an ultrasound simulation. The tissues of these models are segmented from the MRI volumes into five primary classes of tissue in the human abdominal wall (skin, fat, muscle, connective tissue, and nontissue). This segmentation is achieved using an unsupervised machine learning algorithm, fuzzy c-means clustering (FCM), on a multiscale feature representation of the MRI volumes. We describe an automated method for utilizing FCM weights to produce a model that achieves approximately 90 % agreement with manual segmentation. Two-dimensional (2-D) and three-dimensional (3-D) full-wave nonlinear ultrasound simulations are conducted, demonstrating the utility of realistic 3-D abdominal wall models over previously available 2-D abdominal wall models.",
        "BACKGROUND: Identifying implausible clinical observations (e.g., laboratory test and vital sign values) in Electronic Health Record (EHR) data using rule-based procedures is challenging. Anomaly/outlier detection methods can be applied as an alternative algorithmic approach to flagging such implausible values in EHRs. METHODS: The primary objectives of this research were to develop and test an unsupervised clustering-based anomaly/outlier detection approach for detecting implausible observations in EHR data as an alternative algorithmic solution to the existing procedures. Our approach is built upon two underlying hypotheses that, (i) when there are large number of observations, implausible records should be sparse, and therefore (ii) if these data are clustered properly, clusters with sparse populations should represent implausible observations. To test these hypotheses, we applied an unsupervised clustering algorithm to EHR observation data on 50 laboratory tests from Partners HealthCare. We tested different specifications of the clustering approach and computed confusion matrix indices against a set of silver-standard plausibility thresholds. We compared the results from the proposed approach with conventional anomaly detection (CAD) approaches, including standard deviation and Mahalanobis distance. RESULTS: We found that the clustering approach produced results with exceptional specificity and high sensitivity. Compared with the conventional anomaly detection approaches, our proposed clustering approach resulted in significantly smaller number of false positive cases. CONCLUSION: Our contributions include (i) a clustering approach for identifying implausible EHR observations, (ii) evidence that implausible observations are sparse in EHR laboratory test results, (iii) a parallel implementation of the clustering approach on i2b2 star schema, and (3) a set of silver-standard plausibility thresholds for 50 laboratory tests that can be used in other studies for validation. The proposed algorithmic solution can augment human decisions to improve data quality. Therefore, a workflow is needed to complement the algorithm's job and initiate necessary actions that need to be taken in order to improve the quality of data.",
        "BACKGROUND: How can we obtain fast and high-quality clusters in genome scale bio-networks? Graph clustering is a powerful tool applied on bio-networks to solve various biological problems such as protein complexes detection, disease module detection, and gene function prediction. Especially, MCL (Markov Clustering) has been spotlighted due to its superior performance on bio-networks. MCL, however, is skewed towards finding a large number of very small clusters (size 1-3) and fails to detect many larger clusters (size 10+). To resolve this fragmentation problem, MLR-MCL (Multi-level Regularized MCL) has been developed. MLR-MCL still suffers from the fragmentation and, in cases, unrealistically large clusters are generated. RESULTS: In this paper, we propose PS-MCL (Parallel Shotgun Coarsened MCL), a parallel graph clustering method outperforming MLR-MCL in terms of running time and cluster quality. PS-MCL adopts an efficient coarsening scheme, called SC (Shotgun Coarsening), to improve graph coarsening in MLR-MCL. SC allows merging multiple nodes at a time, which leads to improvement in quality, time and space usage. Also, PS-MCL parallelizes main operations used in MLR-MCL which includes matrix multiplication. CONCLUSIONS: Experiments show that PS-MCL dramatically alleviates the fragmentation problem, and outperforms MLR-MCL in quality and running time. We also show that the running time of PS-MCL is effectively reduced with parallelization.",
        "BACKGROUND: MR tractography from diffusion tensor imaging provides a non-invasive way to explore white matter pathways in the human brain. However, a challenge to extracting reliable anatomical information from these data is the use of reliable and effective clustering methodologies. In this paper, we implemented a new version of a robust unsupervised clustering method from MR tractography data using the density-based spatial clustering of applications with noise (DBSCAN) algorithm. NEW METHOD: Conventional DBSCAN clustering methods for MR tractography data use each fiber's start and end point as well as the distance between start and end points. Instead, in this study, we extracted and used a fiber-distance matrix generated for all fiber combinations from the tractography dataset in DBSCAN clustering. The two DBSCAN parameters-minimum point number and maximum radius of the neighborhood-were selected according to the value generated with the cluster stability index (CSI). RESULTS: Performing the proposed CSI-optimized DBSCAN-based clustering method on MR tractography data of the superior longitudinal fasciculus generated 6 robust, non-overlapping, clusters that are neuroanatomically related. COMPARISON WITH EXISTING METHODS: Conventional DBSCAN-based clustering methods have intrinsic error potential in the clustering results due to deviations in fiber shape and fiber location. The proposed method did not exhibit clustering error caused by deviation in fiber trajectory or fiber location. CONCLUSIONS: We implemented a new, robust DBSCAN-based fiber clustering method for MR tractography data. The CSI-optimized DBSCAN-based unsupervised clustering is applicable to investigation of the neuroconnectome and the fiber structure of the brain.",
        "One of the most significant challenges in Internet of Things (IoT) environments is the protection of privacy. Failing to guarantee the privacy of sensitive data collected and shared over IoT infrastructures is a critical barrier that delays the wide penetration of IoT technologies in several user-centric application domains. Location information is the most common dynamic information monitored and lies among the most sensitive ones from a privacy perspective. This article introduces a novel mechanism that aims to protect the privacy of location information across Data Centric Sensor Networks (DCSNs) that monitor the location of mobile objects in IoT systems. The respective data dissemination protocols proposed enhance the security of DCSNs rendering them less vulnerable to intruders interested in obtaining the location information monitored. In this respect, a dynamic clustering algorithm is that clusters the DCSN nodes not only based on the network topology, but also considering the current location of the objects monitored. The proposed techniques do not focus on the prevention of attacks, but on enhancing the privacy of sensitive location information once IoT nodes have been compromised. They have been extensively assessed via series of experiments conducted over the IoT infrastructure of FIT IoT-LAB and the respective evaluation results indicate that the dynamic clustering algorithm proposed significantly outperforms existing solutions focusing on enhancing the privacy of location information in IoT.",
        "Nature-inspired algorithms are based on the concepts of self-organization and complex biological systems. They have been designed by researchers and scientists to solve complex problems in various environmental situations by observing how naturally occurring phenomena behave. The introduction of nature-inspired algorithms has led to new branches of study such as neural networks, swarm intelligence, evolutionary computation, and artificial immune systems. Particle swarm optimization (PSO), social spider optimization (SSO), and other nature-inspired algorithms have found some success in solving clustering problems but they may converge to local optima due to the lack of balance between exploration and exploitation. In this paper, we propose a novel implementation of SSO, namely social spider optimization for data clustering using single centroid representation and enhanced mating operation (SSODCSC) in order to improve the balance between exploration and exploitation. In SSODCSC, we implemented each spider as a collection of a centroid and the data instances close to it. We allowed non-dominant male spiders to mate with female spiders by converting them into dominant males. We found that SSODCSC produces better values for the sum of intra-cluster distances, the average CPU time per iteration (in seconds), accuracy, the F-measure, and the average silhouette coefficient as compared with the K-means and other nature-inspired techniques. When the proposed algorithm is compared with other nature-inspired algorithms with respect to Patent corpus datasets, the overall percentage increase in the accuracy is approximately 13%. When it is compared with other nature-inspired algorithms with respect to UCI datasets, the overall percentage increase in the F-measure value is approximately 10%. For completeness, the best K cluster centroids (the best K spiders) returned by SSODCSC were specified. To show the significance of the proposed algorithm, we conducted a one-way ANOVA test on the accuracy values and the F-measure values returned by the clustering algorithms.",
        "A clustering framework is introduced to analyze the microscopic structural organization of molecular pairs in liquids and solutions. A molecular pair is represented by a representative vector (RV). To obtain RV, intermolecular atom distances in the pair are extracted from simulation trajectory as components of the key feature vector (KFV). A specific scheme is then suggested to transform KFV to RV by removing the influence of permutational molecular symmetry on the KFV as the predicted clusters should be independent of possible permutations of identical atoms in the pair. After RVs of pairs are obtained, a clustering analysis technique is finally used to classify all the RVs of molecular pairs into the clusters. The framework is applied to analyze trajectory from molecular dynamics simulations of an ionic liquid (trihexyltetradecylphosphonium bis(oxalato)borate ([P6,6,6,14 ][BOB])). The molecular pairs are successfully categorized into physically meaningful clusters, and their effectiveness is evaluated by computing the product moment correlation coefficient (PMCC). (Willett, Winterman, and Bawden, J. Chem. Inf. Comput. Sci. 1986, 26, 109-118; Downs, Willett, and Fisanick, J. Chem. Inf. Comput. Sci. 1994, 34, 1094-1102) It is observed that representative configurations of two clusters are related to two energy local minimum structures optimized by density functional theory (DFT) calculation, respectively. Several widely used clustering analysis techniques of both nonhierarchical (k-means) and hierarchical clustering algorithms are also evaluated and compared with each other. The proposed KFV technique efficiently reveals local molecular pair structures in the simulated complex liquid. It is a method, which is highly useful for liquids and solutions in particular with strong intermolecular interactions. (c) 2019 Wiley Periodicals, Inc.",
        "BACKGROUND: High-dimensional data of discrete and skewed nature is commonly encountered in high-throughput sequencing studies. Analyzing the network itself or the interplay between genes in this type of data continues to present many challenges. As data visualization techniques become cumbersome for higher dimensions and unconvincing when there is no clear separation between homogeneous subgroups within the data, cluster analysis provides an intuitive alternative. The aim of applying mixture model-based clustering in this context is to discover groups of co-expressed genes, which can shed light on biological functions and pathways of gene products. RESULTS: A mixture of multivariate Poisson-log normal (MPLN) model is developed for clustering of high-throughput transcriptome sequencing data. Parameter estimation is carried out using a Markov chain Monte Carlo expectation-maximization (MCMC-EM) algorithm, and information criteria are used for model selection. CONCLUSIONS: The mixture of MPLN model is able to fit a wide range of correlation and overdispersion situations, and is suited for modeling multivariate count data from RNA sequencing studies. All scripts used for implementing the method can be found at https://github.com/anjalisilva/MPLNClust .",
        "AIM: To quantify variations in health-related behaviors (HRB) clustering of older adults in Western and Eastern countries. METHODS: Using six aging cohorts from the USA, England, Europe, Japan, Korea and China, latent class analysis was applied to access the clustering of smoking, alcohol consumption, physical activity and social activity. RESULTS: A total of 104 552 participants (55% women) aged >/=50 years in 2010 were included. Despite a different number of clusters identified, three consistent cluster profiles emerged: \"Multiple-HRB\" (ex-/never smoking, moderate drinking, frequent physical and social activity); \"Inactives\" (socially and physically inactive without other risk behaviors); and \"(ex-)Smokers with Risk Behaviors\". Sex and cohort variations were shown. For men in Western cohorts, \"Multiple-HRB\" was the predominant cluster, whereas their Asian counterparts were more likely to be members of the \"Smokers with risk behavior\" and \"Inactives\" clusters. Most women, particularly those in Asian cohorts, were never smokers and non-drinkers, and most of them belonged to the socially \"Inactives\" cluster. CONCLUSIONS: We provide a person-centered understanding of HRB clustering of older adults over selected countries by sex, informing tailored health promotion for the target population. Geriatr Gerontol Int 2019; 19: 930-937.",
        "BACKGROUND: Clustering methods are essential to partitioning biological samples being useful to minimize the information complexity in large datasets. Tools in this context usually generates data with greed algorithms that solves some Data Mining difficulties which can degrade biological relevant information during the clustering process. The lack of standardization of metrics and consistent bases also raises questions about the clustering efficiency of some methods. Benchmarks are needed to explore the full potential of clustering methods - in which alignment-free methods stand out - and the good choice of dataset makes it essentials. RESULTS: Here we present a new approach to Data Mining in large protein sequences datasets, the Rapid Alignment Free Tool for Sequences Similarity Search to Groups (RAFTS(3)G), a method to clustering aiming of losing less biological information in the processes of generation groups. The strategy developed in our algorithm is optimized to be more astringent which reflects increase in accuracy and sensitivity in the generation of clusters in a wide range of similarity. RAFTS(3)G is the better choice compared to three main methods when the user wants more reliable result even ignoring the ideal threshold to clustering. CONCLUSION: In general, RAFTS(3)G is able to group up to millions of biological sequences into large datasets, which is a remarkable option of efficiency in clustering. RAFTS(3)G compared to other \"standard-gold\" methods in the clustering of large biological data maintains the balance between the reduction of biological information redundancy and the creation of consistent groups. We bring the binary search concept applied to grouped sequences which shows maintaining sensitivity/accuracy relation and up to minimize the time of data generated with RAFTS(3)G process.",
        "BACKGROUND: Stability of spatial components is frequently used as a post-hoc selection criteria for choosing the dimensionality of an independent component analysis (ICA) of functional magnetic resonance imaging (fMRI) data. Although the stability of the ICA temporal courses differs from that of spatial components, temporal stability has not been considered during dimensionality decisions. NEW METHOD: The current study aims to (1) develop an algorithm to incorporate temporal course stability into dimensionality selection and (2) test the impact of temporal course on the stability of the ICA decomposition of fMRI data via tensor clustering. Resting state fMRI data were analyzed with two popular ICA algorithms, InfomaxICA and FastICA, using our new method and results were compared with model order selection based on spatial or temporal criteria alone. RESULTS: Hierarchical clustering indicated that the stability of the ICA decomposition incorporating spatiotemporal tensor information performed similarly when compared to current best practice. However, we found that component spatiotemporal stability and convergence of the model varied significantly with model order. Considering both may lead to methodological improvements for determining ICA model order. Selected components were also significantly associated with relevant behavioral variables. Comparison with Existing Method: The Kullback-Leibler information criterion algorithm suggests the optimal model order for group ICA is 40, compared to the proposed method with an optimal model order of 20. CONCLUSION: The current study sheds new light on the importance of temporal course variability in ICA of fMRI data.",
        "In recent years, image processing in a Euclidean domain has been well studied. Practical problems in computer vision and geometric modeling involve image data defined in irregular domains, which can be modeled by huge graphs. In this paper, a wavelet frame-based fuzzy C -means (FCM) algorithm for segmenting images on graphs is presented. To enhance its robustness, images on graphs are first filtered by using spatial information. Since a real image usually exhibits sparse approximation under a tight wavelet frame system, feature spaces of images on graphs can be obtained. Combining the original and filtered feature sets, this paper uses the FCM algorithm for segmentation of images on graphs contaminated by noise of different intensities. Finally, some supporting numerical experiments and comparison with other FCM-related algorithms are provided. Experimental results reported for synthetic and real images on graphs demonstrate that the proposed algorithm is effective and efficient, and has a better ability for segmentation of images on graphs than other improved FCM algorithms existing in the literature. The approach can effectively remove noise and retain feature details of images on graphs. It offers a new avenue for segmenting images in irregular domains.",
        "Although within-cluster information is commonly used in most clustering approaches, other important information such as between-cluster information is rarely considered in some cases. Hence, in this study, we propose a new novel measure of between-cluster distance in subspace, which is to maximize the distance between the center of a cluster and the points that do not belong to this cluster. Based on this idea, we firstly design an optimization objective function integrating the between-cluster distance and entropy regularization in this paper. Then, updating rules are given by theoretical analysis. In the following, the properties of our proposed algorithm are investigated, and the performance is evaluated experimentally using two synthetic and seven real-life datasets. Finally, the experimental studies demonstrate that the results of the proposed algorithm (ERKM) outperform most existing state-of-the-art k-means-type clustering algorithms in most cases.",
        "Malaria is a serious public health threat in Yunnan Province of China and has been frequently reported in some endemic regions, such as Tengchong County, with high morbidity. It is essential to analyze the characteristics of malaria cases and identify vulnerable populations. Previous studies about vulnerable populations have mostly used a statistical grouping method to count frequence from a single aspect rather than defined clustered groups. Based on descriptive analysis of the temporal variation and demographic structure of the populations with malaria infection, we used a k-prototypes clustering algorithm to cluster vulnerable populations in Tengchong County in three dimensions, according to sex, age, and occupation. The results indicated that a high incidence of malaria occurred mainly in young male farmers and young or middle-aged male migrant workers. Imported cases, low education level, lack of mosquito bite prevention, and risk behaviors contributed to the high malaria incidence in these groups. Double verification ensured the reliability of this method and reasonability of the results. In addition, we highlighted the importance of targeting prevention and control of malaria for vulnerable groups. We provided suggestions of policies and measures to be implemented by regional governments and at household and individual levels for farmers and migrant workers respectively. Using the k-prototypes clustering algorithm, we efficiently identified those populations at greatest risk of malaria infection. Our results may serve as scientific guidance for targeted malaria prevention and control in Yunnan Province.",
        "The estimation of vascular network topology in complex networks is important in understanding the relationship between vascular changes and a wide spectrum of diseases. Automatic classification of the retinal vascular trees into arteries and veins is of direct assistance to the ophthalmologist in terms of diagnosis and treatment of eye disease. However, it is challenging due to their projective ambiguity and subtle changes in appearance, contrast, and geometry in the imaging process. In this paper, we propose a novel method that is capable of making the artery/vein (A/V) distinction in retinal color fundus images based on vascular network topological properties. To this end, we adapt the concept of dominant set clustering and formalize the retinal blood vessel topology estimation and the A/V classification as a pairwise clustering problem. The graph is constructed through image segmentation, skeletonization, and identification of significant nodes. The edge weight is defined as the inverse Euclidean distance between its two end points in the feature space of intensity, orientation, curvature, diameter, and entropy. The reconstructed vascular network is classified into arteries and veins based on their intensity and morphology. The proposed approach has been applied to five public databases, namely INSPIRE, IOSTAR, VICAVR, DRIVE, and WIDE, and achieved high accuracies of 95.1%, 94.2%, 93.8%, 91.1%, and 91.0%, respectively. Furthermore, we have made manual annotations of the blood vessel topologies for INSPIRE, IOSTAR, VICAVR, and DRIVE datasets, and these annotations are released for public access so as to facilitate researchers in the community.",
        "We report a multivariate curve resolution (MCR)-based spectral deconvolution workflow for untargeted gas chromatography-mass spectrometry metabolomics. As an essential step in preprocessing such data, spectral deconvolution computationally separates ions that are in the same mass spectrum but belong to coeluting compounds that are not resolved completely by chromatography. As a result of this computational separation, spectral deconvolution produces pure fragmentation mass spectra. Traditionally, spectral deconvolution has been achieved by using a model peak approach. We describe the fundamental differences between the model peak-based and the MCR-based spectral deconvolution and report ADAP-GC 4.0 that employs the latter approach while overcoming the associated computational complexity. ADAP-GC 4.0 has been evaluated using GC-TOF data sets from a 27-standards mixture at different dilutions and urine with the mixture spiked in, and GC Orbitrap data sets from mixtures of different standards. It produced the average matching scores 960, 959, and 926 respectively. Moreover, its performance has been compared against MS-DIAL, eRah, and ADAP-GC 3.2, and ADAP-GC 4.0 demonstrated a higher number of matched compounds and up to 6% increase of the average matching score.",
        "Appropriate ways to measure the similarity between single-cell RNA-sequencing (scRNA-seq) data are ubiquitous in bioinformatics, but using single clustering or classification methods to process scRNA-seq data is generally difficult. This has led to the emergence of integrated methods and tools that aim to automatically process specific problems associated with scRNA-seq data. These approaches have attracted a lot of interest in bioinformatics and related fields. In this paper, we systematically review the integrated methods and tools, highlighting the pros and cons of each approach. We not only pay particular attention to clustering and classification methods but also discuss methods that have emerged recently as powerful alternatives, including nonlinear and linear methods and descending dimension methods. Finally, we focus on clustering and classification methods for scRNA-seq data, in particular, integrated methods, and provide a comprehensive description of scRNA-seq data and download URLs.",
        "Ecoregionalization is the process by which a territory is classified in similar areas according to specific environmental and climatic factors. The climate and the environment strongly influence the presence and distribution of vectors responsible for significant human and animal diseases worldwide. In this paper, we developed a map of the eco-climatic regions of Italy adopting a data-driven spatial clustering approach using recent and detailed spatial data on climatic and environmental factors. We selected seven variables, relevant for a broad set of human and animal vector-borne diseases (VBDs): standard deviation of altitude, mean daytime land surface temperature, mean amplitude and peak timing of the annual cycle of land surface temperature, mean and amplitude of the annual cycle of greenness value, and daily mean amount of rainfall. Principal Component Analysis followed by multivariate geographic clustering using the k-medoids technique were used to group the pixels with similar characteristics into different ecoregions, and at different spatial resolutions (250 m, 1 km and 2 km). We showed that the spatial structure of ecoregions is generally maintained at different spatial resolutions and we compared the resulting ecoregion maps with two datasets related to Bluetongue vectors and West Nile Disease (WND) outbreaks in Italy. The known characteristics of Culicoides imicola habitat were well captured by 2/22 specific ecoregions (at 250 m resolution). Culicoides obsoletus/scoticus occupy all sampled ecoregions, according to its known widespread distribution across the peninsula. WND outbreak locations strongly cluster in 4/22 ecoregions, dominated by human influenced landscape, with intense cultivations and complex irrigation network. This approach could be a supportive tool in case of VBDs, defining pixel-based areas that are conducive environment for VBD spread, indicating where surveillance and prevention measures could be prioritized in Italy. Also, ecoregions suitable to specific VBDs vectors could inform entomological surveillance strategies.",
        "Multiple kernel clustering (MKC) has been intensively studied during the last few decades. Even though they demonstrate promising clustering performance in various applications, existing MKC algorithms do not sufficiently consider the intrinsic neighborhood structure among base kernels, which could adversely affect the clustering performance. In this paper, we propose a simple yet effective neighbor-kernel-based MKC algorithm to address this issue. Specifically, we first define a neighbor kernel, which can be utilized to preserve the block diagonal structure and strengthen the robustness against noise and outliers among base kernels. After that, we linearly combine these base neighbor kernels to extract a consensus affinity matrix through an exact-rank-constrained subspace segmentation. The naturally possessed block diagonal structure of neighbor kernels better serves the subsequent subspace segmentation, and in turn, the extracted shared structure is further refined through subspace segmentation based on the combined neighbor kernels. In this manner, the above two learning processes can be seamlessly coupled and negotiate with each other to achieve better clustering. Furthermore, we carefully design an efficient iterative optimization algorithm with proven convergence to address the resultant optimization problem. As a by-product, we reveal an interesting insight into the exact-rank constraint in ridge regression by careful theoretical analysis: it back-projects the solution of the unconstrained counterpart to its principal components. Comprehensive experiments have been conducted on several benchmark data sets, and the results demonstrate the effectiveness of the proposed algorithm.",
        "Absent a priori knowledge, unsupervised techniques identify meaningful clusters that can form the basis for subsequent analyses. This study explored the problem of inferring comorbidity-based profiles of complex diseases through unsupervised clustering methodologies. This study first considered the K-Modes algorithm, followed by, the self organizing map (SOM) technique to extract co-morbidity based clusters from a healthcare discharge dataset. After validation of general cluster composition for diabetes mellitus, co-morbidity based clusters were identified for pregnancy. The SOM technique was found to infer distinct clusterings of pregnancy ranging from normal birth to preterm birth, and potentially interesting comorbidities that could be validated by published literature The promising results suggest that the SOM technique is a valuable unsupervised clustering method for discovering co-morbidity based clusters.",
        "Most of the existing Bayesian clustering algorithms perform well on the balanced data. When the data are highly imbalanced, these Bayesian clustering algorithms tend to strongly favor the larger clusters, but provide a notably low detection of the smaller clusters. In this paper, we present an incremental local distribution-based clustering algorithm with the Bayesian adaptive resonance theory (ILBART). This algorithm is developed to adapt itself to a changing environment without using any predefined parameters. The algorithm not only accurately finds the clusters, even in data sets with a severely imbalanced distribution, but also efficiently processes the dynamic data according to the evolving relationships among the clusters. We test our proposed algorithm with experiments conducted on several imbalanced data sets. The experimental results show that our proposed algorithm performs well for clustering imbalanced data and can also obtain a better performance than many other relevant clustering algorithms in several performance indices.",
        "Currently, wireless sensor network (WSN) protocols are mainly used to achieve low power consumption of the network, but there are few studies on the quality of services (QoS) of these networks. Coverage can be used as a measure of the WSN's QoS, which can further reflect the quality of data information. Additionally, the coverage requirements of regional monitoring target points are different in real applications. On this basis, this paper proposes an energy-efficient clustering routing protocol based on a high-QoS node deployment with an inter-cluster routing mechanism (EECRP-HQSND-ICRM) in WSNs. First, this paper proposes formula definitions for information integrity, validity, and redundancy from the coverage rate and introduces a node deployment strategy based on twofold coverage. Then, in order to satisfy the uniformity of the distribution of cluster heads (CHs), the monitoring area is divided into four small areas centered on the base station (BS), and the CHs are selected in the respective cells. Finally, combined with the practical application of the WSN, this paper optimizes the Dijkstra algorithm, including: (1) nonessential paths neglecting considerations, and (2) a simultaneous introduction of end-to-end weights and path weights, achieving the selection of optimal information transmission paths between the CHs. The simulation results show that, compared with the general node deployment strategies, the deployment strategy of the proposed protocol has higher information integrity and validity, as well as lower redundancy. Meanwhile, compared with some classic protocols, this protocol can greatly reduce and balance network energy consumption and extend the network lifetime.",
        "As a powerful approach for exploratory data analysis, unsupervised clustering is a fundamental task in computer vision and pattern recognition. Many clustering algorithms have been developed, but most of them perform unsatisfactorily on the data with complex structures. Recently, adversarial autoencoder (AE) (AAE) shows effectiveness on tackling such data by combining AE and adversarial training, but it cannot effectively extract classification information from the unlabeled data. In this brief, we propose dual AAE (Dual-AAE) which simultaneously maximizes the likelihood function and mutual information between observed examples and a subset of latent variables. By performing variational inference on the objective function of Dual-AAE, we derive a new reconstruction loss which can be optimized by training a pair of AEs. Moreover, to avoid mode collapse, we introduce the clustering regularization term for the category variable. Experiments on four benchmarks show that Dual-AAE achieves superior performance over state-of-the-art clustering methods. In addition, by adding a reject option, the clustering accuracy of Dual-AAE can reach that of supervised CNN algorithms. Dual-AAE can also be used for disentangling style and content of images without using supervised information.",
        "Cancer is a genetic disease comprising multiple subtypes that have distinct molecular characteristics and clinical features. Cancer subtyping helps in improving personalized treatment and making decision, as different cancer subtypes respond differently to the treatment. The increasing availability of cancer related genomic data provides the opportunity to identify molecular subtypes. Several unsupervised machine learning techniques have been applied on molecular data of the tumor samples to identify cancer subtypes that are genetically and clinically distinct. However, most clustering methods often fail to efficiently cluster patients due to the challenges imposed by high-throughput genomic data and its non-linearity. In this paper, we propose a pathway-based deep clustering method (PACL) for molecular subtyping of cancer, which incorporates gene expression and biological pathway database to group patients into cancer subtypes. The main contribution of our model is to discover high-level representations of biological data by learning complex hierarchical and nonlinear effects of pathways. We compared the performance of our model with a number of benchmark clustering methods that recently have been proposed in cancer subtypes. We assessed the hypothesis that clusters (subtypes) may be associated to different survivals by logrank tests. PACL showed the lowest p-value of the logrank test against the benchmark methods. It demonstrates the patient groups clustered by PACL may correspond to subtypes which are significantly associated with distinct survival distributions. Moreover, PACL provides a solution to comprehensively identify subtypes and interpret the model in the biological pathway level. The open-source software of PACL in PyTorch is publicly available at https://github.com/tmallava/PACL.",
        "We present a scalable weakly supervised clustering approach to learn facial action units (AUs) from large, freely available web images. Unlike most existing methods (e.g., CNNs) that rely on fully annotated data, our method exploits web images with inaccurate annotations. Specifically, we derive a weakly-supervised spectral algorithm that learns an embedding space to couple image appearance and semantics. The algorithm has efficient gradient update, and scales up to large quantities of images with a stochastic extension. With the learned embedding space, we adopt rank-order clustering to identify groups of visually and semantically similar images, and re-annotate these groups for training AU classifiers. Evaluation on the 1 millon EmotioNet dataset demonstrates the effectiveness of our approach: (1) our learned annotations reach on average 91.3% agreement with human annotations on 7 common AUs, (2) classifiers trained with re-annotated images perform comparably to, sometimes even better than, its supervised CNN-based counterpart, and (3) our method offers intuitive outlier/noise pruning instead of forcing one annotation to every image. Code is available.",
        "Single-cell RNAsequencing (scRNA-seq) technologies have enabled the large-scale whole-transcriptome profiling of each individual single cell in a cell population. A core analysis of the scRNA-seq transcriptome profiles is to cluster the single cells to reveal cell subtypes and infer cell lineages based on the relations among the cells. This article reviews the machine learning and statistical methods for clustering scRNA-seq transcriptomes developed in the past few years. The review focuses on how conventional clustering techniques such as hierarchical clustering, graph-based clustering, mixture models, $k$-means, ensemble learning, neural networks and density-based clustering are modified or customized to tackle the unique challenges in scRNA-seq data analysis, such as the dropout of low-expression genes, low and uneven read coverage of transcripts, highly variable total mRNAs from single cells and ambiguous cell markers in the presence of technical biases and irrelevant confounding biological variations. We review how cell-specific normalization, the imputation of dropouts and dimension reduction methods can be applied with new statistical or optimization strategies to improve the clustering of single cells. We will also introduce those more advanced approaches to cluster scRNA-seq transcriptomes in time series data and multiple cell populations and to detect rare cell types. Several software packages developed to support the cluster analysis of scRNA-seq data are also reviewed and experimentally compared to evaluate their performance and efficiency. Finally, we conclude with useful observations and possible future directions in scRNA-seq data analytics. AVAILABILITY: All the source code and data are available at https://github.com/kuanglab/single-cell-review.",
        "BACKGROUND: Gait symptoms and balance impairment are characteristic indicators for the progression in Parkinson's disease (PD). Current gait assessments mostly focus on straight strides with assumed constant velocity, while acceleration/deceleration and turning strides are often ignored. This is either due to the set up of typical clinical assessments or technical limitations in capture volume. Wearable inertial measurement units are a promising and unobtrusive technology to overcome these limitations. Other gait phases such as initiation, termination, transitioning (between straight walking and turning) and turning might be relevant as well for the evaluation of gait and balance impairments in PD. METHOD: In a cohort of 119 PD patients, we applied unsupervised algorithms to find different gait clusters which potentially include the clinically relevant information from distinct gait phases in the standardized 4x10 m gait test. To clinically validate our approach, we determined the discriminative power in each gait cluster to classify between impaired and unimpaired PD patients and compared it to baseline (analyzing all straight strides). RESULTS: As a main result, analyzing only one of the gait clusters constant, non-constant or turning led in each case to a better classification performance in comparison to the baseline (increase of area under the curve (AUC) up to 19% relative to baseline). Furthermore, gait parameters (for turning, constant and non-constant gait) that best predict motor impairment in PD were identified. CONCLUSIONS: We conclude that a more detailed analysis in terms of different gait clusters of standardized gait tests such as the 4x10 m walk may give more insights about the clinically relevant motor impairment in PD patients.",
        "Three-dimensional genome structure plays a pivotal role in gene regulation and cellular function. Single-cell analysis of genome architecture has been achieved using imaging and chromatin conformation capture methods such as Hi-C. To study variation in chromosome structure between different cell types, computational approaches are needed that can utilize sparse and heterogeneous single-cell Hi-C data. However, few methods exist that are able to accurately and efficiently cluster such data into constituent cell types. Here, we describe scHiCluster, a single-cell clustering algorithm for Hi-C contact matrices that is based on imputations using linear convolution and random walk. Using both simulated and real single-cell Hi-C data as benchmarks, scHiCluster significantly improves clustering accuracy when applied to low coverage datasets compared with existing methods. After imputation by scHiCluster, topologically associating domain (TAD)-like structures (TLSs) can be identified within single cells, and their consensus boundaries were enriched at the TAD boundaries observed in bulk cell Hi-C samples. In summary, scHiCluster facilitates visualization and comparison of single-cell 3D genomes.",
        "MOTIVATION: Identification of the genomic alterations driving tumorigenesis is one of the main goals in oncogenomics research. Given the evolutionary principles of cancer development, computational methods that detect signals of positive selection in the pattern of tumor mutations have been effectively applied in the search for cancer genes. One of these signals is the abnormal clustering of mutations, which has been shown to be complementary to other signals in the detection of driver genes. RESULTS: We have developed OncodriveCLUSTL, a new sequence-based clustering algorithm to detect significant clustering signals across genomic regions. OncodriveCLUSTL is based on a local background model derived from the simulation of mutations accounting for the composition of tri- or penta-nucleotide context substitutions observed in the cohort under study. Our method can identify known clusters and bona-fide cancer drivers across cohorts of tumor whole-exomes, outperforming the existing OncodriveCLUST algorithm and complementing other methods based on different signals of positive selection. Our results indicate that OncodriveCLUSTL can be applied to the analysis of non-coding genomic elements and non-human mutations data. AVAILABILITY AND IMPLEMENTATION: OncodriveCLUSTL is available as an installable Python 3.5 package. The source code and running examples are freely available at https://bitbucket.org/bbglab/oncodriveclustl under GNU Affero General Public License. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "Multiview subspace clustering has received significant attention as the availability of diverse of multidomain and multiview real-world data has rapidly increased in the recent years. Boosting the performance of multiview clustering algorithms is challenged by two major factors. First, since original features from multiview data are highly redundant, reconstruction based on these attributes inevitably results in inferior performance. Second, since each view of such multiview data may contain unique knowledge as against the others, it remains a challenge to exploit complimentary information across multiple views while simultaneously investigating the uniqueness of each view. In this paper, we present a novel dual shared-specific multiview subspace clustering (DSS-MSC) approach that simultaneously learns the correlations between shared information across multiple views and also utilizes view-specific information to depict specific property for each independent view. Further, we formulate a dual learning framework to capture shared-specific information into the dimensional reduction and self-representation processes, which strengthens the ability of our approach to exploit shared information while preserving view-specific property effectively. The experimental results on several benchmark datasets have demonstrated the effectiveness of the proposed approach against other state-of-the-art techniques.",
        "Recent advances in sequencing, mass spectrometry and cytometry technologies have enabled researchers to collect large-scale omics data from the same set of biological samples. The joint analysis of multiple omics offers the opportunity to uncover coordinated cellular processes acting across different omic layers. In this work, we present a thorough comparison of a selection of recent integrative clustering approaches, including Bayesian (BCC and MDI) and matrix factorization approaches (iCluster, moCluster, JIVE and iNMF). Based on simulations, the methods were evaluated on their sensitivity and their ability to recover both the correct number of clusters and the simulated clustering at the common and data-specific levels. Standard non-integrative approaches were also included to quantify the added value of integrative methods. For most matrix factorization methods and one Bayesian approach (BCC), the shared and specific structures were successfully recovered with high and moderate accuracy, respectively. An opposite behavior was observed on non-integrative approaches, i.e. high performances on specific structures only. Finally, we applied the methods on the Cancer Genome Atlas breast cancer data set to check whether results based on experimental data were consistent with those obtained in the simulations.",
        "BACKGROUND: Missing values frequently arise in modern biomedical studies due to various reasons, including missing tests or complex profiling technologies for different omics measurements. Missing values can complicate the application of clustering algorithms, whose goals are to group points based on some similarity criterion. A common practice for dealing with missing values in the context of clustering is to first impute the missing values, and then apply the clustering algorithm on the completed data. RESULTS: We consider missing values in the context of optimal clustering, which finds an optimal clustering operator with reference to an underlying random labeled point process (RLPP). We show how the missing-value problem fits neatly into the overall framework of optimal clustering by incorporating the missing value mechanism into the random labeled point process and then marginalizing out the missing-value process. In particular, we demonstrate the proposed framework for the Gaussian model with arbitrary covariance structures. Comprehensive experimental studies on both synthetic and real-world RNA-seq data show the superior performance of the proposed optimal clustering with missing values when compared to various clustering approaches. CONCLUSION: Optimal clustering with missing values obviates the need for imputation-based pre-processing of the data, while at the same time possessing smaller clustering errors.",
        "In this paper, I propose a new unsupervised change detection method for optical satellite imagery. The proposed technique consists of three phases. In the first stage, difference images are calculated using four different functions. Two of the functions were first used in this study. In the second stage, using Reconstruction Independent Component Analysis, this four-difference matrix is projected to one feature. In the last stage, clustering is performed. Kmeans tuned by Artificial Bee Colony (ABC-Kmeans) clustering technique has been developed and proposed by following a different strategy in the clustering phase. The effectiveness of the proposed approach was examined using two different datasets, Sardinia and Mexico. Quantitative evaluation was performed in two stages. In the first stage, proposed method was compared with different unsupervised change detection algorithms using False Alarm, Missed Alarm, Total Error, and Total Error Rate metrics which are calculated using ground truth image in dataset. In the second experimental study, the proposed approach is compared in detail with PCA-Kmeans approach, which is quite often preferred for similar studies, using the Mean Squared Error, Peak Signal to Noise Ratio, Structural Similarity Index, and Universal Image Quality Index metrics. According to quantitative and qualitative analysis, proposed approach can produce quite successful results using optical remote sensing data.",
        "Problems involving image segmentation, atomic cluster identification, segmentation of microstructure constituents in images and austenite reconstruction have seen various approaches attempt to solve them with mixed results. No single computational technique has been able to effectively tackle these problems due to the vast differences between them. We propose the application of graph cutting as a versatile technique that can provide solutions to numerous materials data analysis problems. This can be attributed to its configuration flexibility coupled with the ability to handle noisy experimental data. Implementation of a Bayesian statistical approach allows for the prior information, based on experimental results and already ingrained within nodes, to drive the expected solutions. This way, nodes within the graph can be grouped together with similar, neighboring nodes that are then assigned to a specific system with respect to calculated likelihoods. Associating probabilities with potential solutions and states of the system allows for quantitative, stochastic analysis. The promising, robust results for each problem indicate the potential usefulness of the technique so long as a network of nodes can be effectively established within the model system.",
        "STUDY DESIGN: Retrospective review of prospectively-collected, multicenter adult spinal deformity (ASD) databases. OBJECTIVE: To apply artificial intelligence (AI)-based hierarchical clustering as a step toward a classification scheme that optimizes overall quality, value, and safety for ASD surgery. SUMMARY OF BACKGROUND DATA: Prior ASD classifications have focused on radiographic parameters associated with patient reported outcomes. Recent work suggests there are many other impactful preoperative data points. However, the ability to segregate patient patterns manually based on hundreds of data points is beyond practical application for surgeons. Unsupervised machine-based clustering of patient types alongside surgical options may simplify analysis of ASD patient types, procedures, and outcomes. METHODS: Two prospective cohorts were queried for surgical ASD patients with baseline, 1-year, and 2-year SRS-22/Oswestry Disability Index/SF-36v2 data. Two dendrograms were fitted, one with surgical features and one with patient characteristics. Both were built with Ward distances and optimized with the gap method. For each possible n patient cluster by m surgery, normalized 2-year improvement and major complication rates were computed. RESULTS: Five hundred-seventy patients were included. Three optimal patient types were identified: young with coronal plane deformity (YC, n = 195), older with prior spine surgeries (ORev, n = 157), and older without prior spine surgeries (OPrim, n = 218). Osteotomy type, instrumentation and interbody fusion were combined to define four surgical clusters. The intersection of patient-based and surgery-based clusters yielded 12 subgroups, with major complication rates ranging from 0% to 51.8% and 2-year normalized improvement ranging from -0.1% for SF36v2 MCS in cluster [1,3] to 100.2% for SRS self-image score in cluster [2,1]. CONCLUSION: Unsupervised hierarchical clustering can identify data patterns that may augment preoperative decision-making through construction of a 2-year risk-benefit grid. In addition to creating a novel AI-based ASD classification, pattern identification may facilitate treatment optimization by educating surgeons on which treatment patterns yield optimal improvement with lowest risk. LEVEL OF EVIDENCE: 4.",
        "Breast Cancer is one of the most common causes of cancer death in women, representing a very complex disease with varied molecular alterations. To assist breast cancer prognosis, the classification of patients into biological groups is of great significance for treatment strategies. Recent studies have used an ensemble of multiple clustering algorithms to elucidate the most characteristic biological groups of breast cancer. However, the combination of various clustering methods resulted in a number of patients remaining unclustered. Therefore, a framework still needs to be developed which can assign as many unclustered (i.e. biologically diverse) patients to one of the identified groups in order to improve classification. Therefore, in this paper we develop a novel classification framework which introduces a new ensemble classification stage after the ensemble clustering stage to target the unclustered patients. Thus, a step-by-step pipeline is introduced which couples ensemble clustering with ensemble classification for the identification of core groups, data distribution in them and improvement in final classification results by targeting the unclustered data. The proposed pipeline is employed on a novel real world breast cancer dataset and subsequently its robustness and stability are examined by testing it on standard datasets. The results show that by using the presented framework, an improved classification is obtained. Finally, the results have been verified using statistical tests, visualisation techniques, cluster quality assessment and interpretation from clinical experts.",
        "Clustering is a prevalent analytical means to analyze single cell RNA sequencing (scRNA-seq) data but the rapidly expanding data volume can make this process computationally challenging. New methods for both accurate and efficient clustering are of pressing need. Here we proposed Spearman subsampling-clustering-classification (SSCC), a new clustering framework based on random projection and feature construction, for large-scale scRNA-seq data. SSCC greatly improves clustering accuracy, robustness, and computational efficacy for various state-of-the-art algorithms benchmarked on multiple real datasets. On a dataset with 68,578 human blood cells, SSCC achieved 20% improvement for clustering accuracy and 50-fold acceleration, but only consumed 66% memory usage, compared to the widelyused software package SC3. Compared to k-means, the accuracy improvement of SSCC can reach 3-fold. An R implementation of SSCC is available at https://github.com/Japrin/sscClust.",
        "BACKGROUND AND OBJECTIVES: P300 is an Event Related Potential control signal widely used in Brain Computer Interfaces. Using the oddball paradigm, a P300 speller allows a human to spell letters through P300 events produced by his/her brain. One of the most common issues in the detection of this event is that its structure may differ between different subjects and over time for a specific subject. The main purpose of this work is to deal with this inherent variability and identify the main structure of P300 using algorithmic clustering based on string compression. METHODS: In this work, we make use of the Normalized Compression Distance (NCD) to extract the main structure of the signal regardless of its inherent variability. In order to apply compression distances, we carry out a novel signal-to-ASCII process that transforms and merges different events into suitable objects to be used by a compression algorithm. Once the ASCII objects are created, we use NCD-driven clustering as a tool to analyze if our object creation method suitably represents the information contained in the signals and to explore if compression distances are a valid tool for identifying P300 structure. With the purpose of increasing the level of generalization of our study, we apply two different clustering methods: a hierarchical clustering algorithm based on the minimum quartet tree method and a multidimensional projection method. RESULTS: Our experimental results show good clustering performance over different experiments, showing the structure extraction capabilities of our procedure. Two datasets with recordings in different scenarios were used to analyze the problem and validate our results, respectively. It has to be pointed out that when the clustering performance over individual electrodes is analyzed, higher P300 activity is found in similar regions to other articles using the same datasets. This suggests that our approach might be used as an electrode-selection criteria. CONCLUSIONS: The proposed NCD-driven clustering methodology can be used to discover the structural characteristics of EEG and thereby, it is suitable as a complementary methodology for the P300 analysis.",
        "Ionic liquids (ILs) are ionic compounds with low melting points that can be designed to be used in an extensive set of commercial and industrial applications. However, the design of ILs is limited by the quantity and quality of the available data in the literature; therefore, the estimation of physicochemical properties of ILs by computational methods is a promising way of solving this problem, since it provides approximations of the real values, resulting in savings in both time and money. We studied two data sets of 281 and 134 liquids based on the molecule imidazole that were analyzed with QSPR techniques. This paper presents a software architecture that uses clustering techniques to improve the robustness of estimation models of the melting point of ILs. These results indicate an error of 6.25% in the previously unmodeled data set and an error of 4.43% in the second data set. We have an improvement with the second data set of 1.81% over the last results previously found.",
        "The reuse of business processes (BPs) requires similarities between them to be suitably identified. Various approaches have been introduced to address this problem, but many of them feature a high computational cost and a low level of automation. This paper presents a clustering algorithm that groups business processes retrieved from a multimodal search system (based on textual and structural information). The algorithm is based on Incremental Covering Arrays (ICAs) with different alphabets to determine the possible number of groups to be created for each row of the ICA. The proposed algorithm also incorporates Balanced Bayesian Information Criterion to determine the optimal number of groups and the best solution for each query. Experimental evaluation shows that the use of ICAs with strength four (4) and different alphabets reduces the number of solutions needed to be evaluated and optimizes the number of clusters. The proposed algorithm outperforms other algorithms in various measures (precision, recall, and F-measure) by between 12% and 88%. Friedman and Wilcoxon non-parametric tests gave a 90-95% significance level to the obtained results. Better options of repository search for BPs help companies to reuse them. By thus reusing BPs, managers and analysts can more easily get to know the evolution and trajectory of the company processes, a situation that could be expected to lead to improved managerial and commercial decision making.",
        "Balancing convergence and diversity has become a key point especially in many-objective optimization where the large numbers of objectives pose many challenges to the evolutionary algorithms. In this paper, an opposition-based evolutionary algorithm with the adaptive clustering mechanism is proposed for solving the complex optimization problem. In particular, opposition-based learning is integrated in the proposed algorithm to initialize the solution, and the nondominated sorting scheme with a new adaptive clustering mechanism is adopted in the environmental selection phase to ensure both convergence and diversity. The proposed method is compared with other nine evolutionary algorithms on a number of test problems with up to fifteen objectives, which verify the best performance of the proposed algorithm. Also, the algorithm is applied to a variety of multiobjective engineering optimization problems. The experimental results have shown the competitiveness and effectiveness of our proposed algorithm in solving challenging real-world problems.",
        "Clustering is one of the most universal approaches for understanding complex data. A pivotal aspect of clustering analysis is quantitatively comparing clusterings; clustering comparison is the basis for many tasks such as clustering evaluation, consensus clustering, and tracking the temporal evolution of clusters. In particular, the extrinsic evaluation of clustering methods requires comparing the uncovered clusterings to planted clusterings or known metadata. Yet, as we demonstrate, existing clustering comparison measures have critical biases which undermine their usefulness, and no measure accommodates both overlapping and hierarchical clusterings. Here we unify the comparison of disjoint, overlapping, and hierarchically structured clusterings by proposing a new element-centric framework: elements are compared based on the relationships induced by the cluster structure, as opposed to the traditional cluster-centric philosophy. We demonstrate that, in contrast to standard clustering similarity measures, our framework does not suffer from critical biases and naturally provides unique insights into how the clusterings differ. We illustrate the strengths of our framework by revealing new insights into the organization of clusters in two applications: the improved classification of schizophrenia based on the overlapping and hierarchical community structure of fMRI brain networks, and the disentanglement of various social homophily factors in Facebook social networks. The universality of clustering suggests far-reaching impact of our framework throughout all areas of science.",
        "In this work, an improved algorithm was developed for two-dimensional (2D) peak detection in complex two-dimensional liquid chromatography (LCxLC) data sets. In the first step, conventional one-dimensional peak detection was performed. In the second step, retention time, bidirectional overlap and unimodality criteria were applied to decide which of the individual peaks should be merged. To improve the peak detection with LCxLC analysis using shifting second dimension ((2)D) gradients, the variable thresholds, which permitted different thresholds for candidate peaks at different first dimension ((1)D) retention times, were employed for examination of the (2)D retention time differences. Furthermore, the bidirectional overlap criterion performed at specified height was recommended to improve detection for tailing peaks. The developed algorithm was further tested on data sets from different LCxLC analyses of a complex peptide mixture, and then quantitatively evaluated by comparison between the results by the algorithm and mass analysis. Evidently improved performance with an accuracy rate over 60% was obtained by the algorithm, even for peak detection with LCxLC analysis under relatively low (1)D sampling frequency or shifting (2)D gradients. This would help to improve LCxLC quantitative analysis and performance assessment.",
        "BACKGROUND: Single cell RNA sequencing (scRNA-seq) is applied to assay the individual transcriptomes of large numbers of cells. The gene expression at single-cell level provides an opportunity for better understanding of cell function and new discoveries in biomedical areas. To ensure that the single-cell based gene expression data are interpreted appropriately, it is crucial to develop new computational methods. RESULTS: In this article, we try to re-construct a neural network based on Gene Ontology (GO) for dimension reduction of scRNA-seq data. By integrating GO with both unsupervised and supervised models, two novel methods are proposed, named GOAE (Gene Ontology AutoEncoder) and GONN (Gene Ontology Neural Network) respectively. CONCLUSIONS: The evaluation results show that the proposed models outperform some state-of-the-art dimensionality reduction approaches. Furthermore, incorporating with GO, we provide an opportunity to interpret the underlying biological mechanism behind the neural network-based model.",
        "Healthcare question answering (HQA) system plays a vital role in encouraging patients to inquire for professional consultation. However, there are some challenging factors in learning and representing the question corpus of HQA datasets, such as high dimensionality, sparseness, noise, nonprofessional expression, etc. To address these issues, we propose an inception convolutional autoencoder model for Chinese healthcare question clustering (ICAHC). First, we select a set of kernels with different sizes using convolutional autoencoder networks to explore both the diversity and quality in the clustering ensemble. Thus, these kernels encourage to capture diverse representations. Second, we design four ensemble operators to merge representations based on whether they are independent, and input them into the encoder using different skip connections. Third, it maps features from the encoder into a lower-dimensional space, followed by clustering. We conduct comparative experiments against other clustering algorithms on a Chinese healthcare dataset. Experimental results show the effectiveness of ICAHC in discovering better clustering solutions. The results can be used in the prediction of patients' conditions and the development of an automatic HQA system.",
        "Considerable efforts have been made over the last decades to improve the robustness of clustering algorithms against noise features and outliers, known to be important sources of error in clustering. Outliers dominate the sum-of-the-squares calculations and generate cluster overlap, thus leading to unreliable clustering results. They can be particularly detrimental in computational biology, e.g., when determining the number of clusters in gene expression data related to cancer or when inferring phylogenetic trees and networks. While the issue of feature weighting has been studied in detail, no clustering methods using object weighting have been proposed yet. Here we describe a new general data partitioning method that includes an object-weighting step to assign higher weights to outliers and objects that cause cluster overlap. Different object weighting schemes, based on the Silhouette cluster validity index, the median and two intercluster distances, are defined. We compare our novel technique to a number of popular and efficient clustering algorithms, such as K-means, X-means, DAPC and Prediction Strength. In the presence of outliers and cluster overlap, our method largely outperforms X-means, DAPC and Prediction Strength as well as the K-means algorithm based on feature weighting.",
        "As a specific type of structural variation, inversions are enjoying particular traction as a result of their established role in evolution. Using third-generation sequencing technology to predict inversions is growing in interest, but many such methods focus on improving sensitivity, giving rise to either too many false positives or very long running times. In this paper, we propose a new framework for inversion detection based on a combination of two novel theoretical models: rectangle clustering and representative rectangle prediction. This combination can automatically filter out false positive inversion predictions while retaining correct ones, leading to a method that has both high sensitivity and high positive prediction values (PPV). Further, this new framework can run very fast on available data. Our software can be freely obtained at https://github.com/UTbioinf/RigInv.",
        "Liquid chromatography mass spectrometry is a popular technique for high throughput analysis of biological samples. Identification and quantification of molecular species via mass spectrometry output requires postexperimental computational analysis of the raw instrument output. While tandem mass spectrometry remains a primary method for identification and quantification, species-resolved precursor data provides a rich source of unexploited information. Several algorithms have been proposed to resolve raw precursor signals into species-resolved isotopic envelopes. Many methods are particularly dependent on user parameters, and because they lack a means to optimize parameters, tend to perform poorly. To this end we present XNet, a parameter-less Bayesian machine learning approach to isotopic envelope extraction through the clustering of extracted ion chromatograms. We evaluate the performance of XNet and other prevalent methods on a quantitative ground truth data set. XNet is publicly available with an Apache license.",
        "Clustering is a powerful machine learning tool for detecting structures in datasets. In the medical field, clustering has been proven to be a powerful tool for discovering patterns and structure in labeled and unlabeled datasets. Unlike supervised methods, clustering is an unsupervised method that works on datasets in which there is no outcome (target) variable nor is anything known about the relationship between the observations, that is, unlabeled data. In this paper, we focus on studying and reviewing clustering methods that have been applied to datasets of neurological diseases, especially Alzheimer's disease (AD). The aim is to provide insights into which clustering technique is more suitable for partitioning patients of AD based on their similarity. This is important as clustering algorithms can find patterns across patients that are difficult for medical practitioners to find. We further discuss the implications of the use of clustering algorithms in the treatment of AD. We found that clustering analysis can point to several features that underlie the conversion from early-stage AD to advanced AD. Furthermore, future work can apply semi-clustering algorithms on AD datasets, which will enhance clusters by including additional information.",
        "Water resource engineers extensively use regional flood frequency analysis to compute the discharge at ungauged sites with limited flow records in the river basins. Frequency and magnitude are the two important factors required to be analyzed for an effective assessment of flood disaster risk management. Globally, many linear clustering techniques are employed to categorize the watershed which are ineffective when dealing with noise and outliers. The present study overcomes this by proposing a relatively new nonlinear clustering algorithm based on hierarchical estimation of densities (NLCAHD) for the Cauvery basin, where the Homogeneity test (H) is enforced to identify the group of stations with same populations. Discordancy measure is carried out for screening the data in order to eliminate the conflicting sites from the group. The whole basin is classified into six homogeneous clusters, while the goodness of fit measure tests the data to distinguish the preferred distribution for the purpose of calculating the growth curves. A comparative study is made with the other linear algorithms such as K-means and C-means, which reveals the better performance of the proposed nonlinear model for identifying the homogeneous regions, in arriving at precise estimates of flood quantiles for various return periods up to 160 years.",
        "A wireless sensor network (WSN) is an essential component of the Internet of Things (IoTs) for information exchange and communication between ubiquitous smart objects. Clustering techniques are widely applied to improve network performance during the routing phase for WSN. However, existing clustering methods still have some drawbacks such as uneven distribution of cluster heads (CH) and unbalanced energy consumption. Recently, much attention has been paid to intelligent clustering methods based on machine learning to solve the above issues. In this paper, an affinity propagation-based self-adaptive (APSA) clustering method is presented. The advantage of K-medoids, which is a traditional machine learning algorithm, is combined with the affinity propagation (AP) method to achieve more reasonable clustering performance. AP is firstly utilized to determine the number of CHs and to search for the optimal initial cluster centers for K-medoids. Then the modified K-medoids is utilized to form the topology of the network by iteration. The presented method effectively avoids the weakness of the traditional K-medoids in aspects of the homogeneous clustering and convergence rate. Simulation results show that the proposed algorithm outperforms some latest work such as the unequal cluster-based routing scheme for multi-level heterogeneous WSN (UCR-H), the low-energy adaptive clustering hierarchy using affinity propagation (LEACH-AP) algorithm, and the energy degree distance unequal clustering (EDDUCA) algorithm.",
        "Tumor growth in the cervix is a complex process. Understanding this phenomena is quite relevant in order to establish proper diagnosis and therapy strategies and a possible startpoint is to evaluate its complexity through the scaling analysis, which define the tumor growth geometry. In this work, tumor interface from primary tumors of squamous cells and adenocarcinomas for cervical cancer were extracted. Fractal dimension and local roughness exponent (Barabasi and Stanley (1996)), alphaloc, were calculated to characterize the in vivo 3-D tumor growth. Image acquisition was carried out according to the standard protocol used for cervical cancer radiotherapy, i.e., axial, magnetic resonance T1 - weighted contrast enhanced images comprising the cervix volume for image registration. Image processing was carried out by a classification scheme based on quantum clustering algorithm (Mussa et al. (2015)) combined with the application of the K-means procedure upon contrasted images (Demirkaya et al. (2008)). The results show significant variations of the parameters depending on the tumor stage and its histological origin.",
        "Clustering white matter (WM) tracts from diffusion tensor imaging (DTI) is primarily important for quantitative analysis on pediatric brain development. A recently developed algorithm, density peaks (DP) clustering, demonstrates great robustness to the complex structural variations of WM tracts without any prior templates. Nevertheless, the calculation of densities, the core step of DP, is time consuming especially when the number of WM fibers is huge. In this paper, we propose a fast algorithm that accelerates the density computation about 50 times over the original one. We convert the global calculation for the density as well as critical parameter in the process into local computations, and develop a binary tree structure to orderly store the neighbors for these local computations. Hence, the density computation turns out to be a direct access of the structure, rendering significantly computational saving. Performing experiments on synthetic point data and the JHU-DTI data set and comparing results of our fast DP algorithm and existing clustering methods, we can validate the efficiency and effectiveness of our fast DP algorithm. Finally, we demonstrate the application of the proposed algorithm on the analysis of pediatric WM tract development.",
        "Though a lot of valuable algorithms of link prediction have been created, it is still difficult to improve the accuracy of link prediction for some networks. Such difficulties may be due to the intrinsic topological features of these networks. To reveal the correlation between the network topology and the link predictability, we generate a group of artificial networks by keeping some structural features of an initial seed network. Based on these artificial networks and some real networks, we find that five topological measures including clustering coefficient, structural consistency, random walk entropy, network diameter, and average path length significantly show their impact on the link predictability. Then, we define a topological score that combines these important topological features. Specifically, it is an integration of structural consistency with degree-related clustering coefficient defined in this work. This topological score exhibits high correlation with the link predictability. Finally, we propose an algorithm for link prediction based on this topological score. Our experiment on eight real networks verifies good performance of this algorithm in link prediction, which supports the reasonability of the new topological score. This work could be insightful for the study of the link predictability.",
        "Class imbalance problem has been extensively studied in the recent years, but imbalanced data clustering in unsupervised environment, that is, the number of samples among clusters is imbalanced, has yet to be well studied. This paper, therefore, studies the imbalanced data clustering problem within the framework of k -means-type competitive learning. We introduce a new method called self-adaptive multiprototype-based competitive learning (SMCL) for imbalanced clusters. It uses multiple subclusters to represent each cluster with an automatic adjustment of the number of subclusters. Then, the subclusters are merged into the final clusters based on a novel separation measure. We also propose a new internal clustering validation measure to determine the number of final clusters during the merging process for imbalanced clusters. The advantages of SMCL are threefold: 1) it inherits the advantages of competitive learning and meanwhile is applicable to the imbalanced data clustering; 2) the self-adaptive multiprototype mechanism uses a proper number of subclusters to represent each cluster with any arbitrary shape; and 3) it automatically determines the number of clusters for imbalanced clusters. SMCL is compared with the existing counterparts for imbalanced clustering on the synthetic and real datasets. The experimental results show the efficacy of SMCL for imbalanced clusters.",
        "The performance of data clustering algorithms is mainly dependent on their ability to balance between the exploration and exploitation of the search process. Although some data clustering algorithms have achieved reasonable quality solutions for some datasets, their performance across real-life datasets could be improved. This paper proposes an adaptive memetic differential evolution optimisation algorithm (AMADE) for addressing data clustering problems. The memetic algorithm (MA) employs an adaptive differential evolution (DE) mutation strategy, which can offer superior mutation performance across many combinatorial and continuous problem domains. By hybridising an adaptive DE mutation operator with the MA, we propose that it can lead to faster convergence and better balance the exploration and exploitation of the search. We would also expect that the performance of AMADE to be better than MA and DE if executed separately. Our experimental results, based on several real-life benchmark datasets, shows that AMADE outperformed other compared clustering algorithms when compared using statistical analysis. We conclude that the hybridisation of MA and the adaptive DE is a suitable approach for addressing data clustering problems and can improve the balance between global exploration and local exploitation of the optimisation algorithm.",
        "Organ segmentation is an important step in Ultrasound fetal images for early prediction of congenital abnormalities and to estimate delivery date. In many applications of 2D medical imaging, they face problems with speckle noise and object contours. Frequent scanning of fetal leads to clinical disturbances to the fetal growth and the quantitative interpretation of Ultrasonic images also a difficult task compared to other image modalities. In the present work a three-stage hybrid algorithm has been developed to segment the US fetal kidney images for the detection of shape and contour. At the first stage the hybrid Mean Median (Hybrid MM) filter is applied to reduce the speckle noise. Then a kernel based Fuzzy C - means clustering is used to detect the shape and contour. Finally, the texture features are obtained from the segmented images. Based on the obtained texture features, the abnormalities are detected. The Gaussian Radial basis function provides an accuracy of 80% at the second and third trimesters with weighted constant ranging from 4 to 8, compared to other global kernel functions. Similarly the proposed method has an accuracy of 86% with compared to other FCM techniques.",
        "BACKGROUND: Humans supply a variety of nutrients to their body in dietary life, which are directly related to health. Chronic diseases are long accumulated in the body on account of heredity or living habits, and draw attention as a main issue in the era of disease-controlled longevity. Therefore, it is essential to make health care continuously through the improvement in dietary habits. OBJECTIVE: By recommending alternative food products whose diet and nutrition structure is similar to that of the food products positively influencing users' health conditions, it is possible to satisfy user's health and preference. METHOD: We used the hybrid clustering based food recommendation method that uses chronic disease based clustering, diet and nutrition ontology, diet and nutrition knowledge base. Active users are classified into the chronic disease based cluster that has the nearest euclidean distance. According to the classified clusters, food products are recommended to users, and similar food products are also recommended with the use of food clustering and knowledge base. Food products are clustered with the uses of k-means algorithm and food and nutrient data system. Based on the created food clusters and food preference data, diet and nutrition knowledge base is generated. It is composed of food cluster filter, food similarity filter, universal preference filter, and user feedback filter. The universal preference filter represents the similarity weight between diet and nutrition, and user preference. The user feedback filter has the similarity weight between active user preference and diet and nutrition. They continue to be updated through associated feedback. RESULT: The proposed health decision-making method takes into account each user's health condition so that the method has more precision than an existing recommendation method. In addition, the proposed method brings about better evaluation results than a general user-by-user health context information based recommendation method. CONCLUSION: By recommending the food products related to users' chronic diseases through the proposed hybrid clustering, it is possible to help out their healthcare. In addition, by letting users receive satisfying feedback flexibly, it is possible to improve their dietary habits.",
        "PURPOSE: Conventional classification of patients with lower urinary tract symptoms into diagnostic categories based on a predefined symptom complex or predominant symptom appears inadequate. This is due to the frequent presentation of patients with multiple urinary symptoms which could not be perfectly categorized into traditional diagnostic groups. We used a novel clustering method to identify subtypes of male patients with lower urinary tract symptoms based on detailed multisymptom information. MATERIALS AND METHODS: We analyzed baseline data on 503 care seeking men in the LURN (Symptoms of Lower Urinary Tract Dysfunction Research Network) Observational Cohort Study. Symptoms and symptom severity were assessed using the LUTS (Lower Urinary Tract Symptoms) Tool and the AUA SI (American Urological Association Symptom Index), which include a total of 52 questions. We used a resampling based consensus clustering algorithm to identify patient subtypes with distinct symptom signatures. RESULTS: Four distinct symptom clusters were identified. The 166 patients in cluster M1 had predominant symptoms of frequency, nocturia, hesitancy, straining, weak stream, intermittency and incomplete bladder emptying suggestive of bladder outlet obstruction. The 93 patients in cluster M2 mainly endorsed post-micturition symptoms (eg post-void dribbling and post-void leakage) with some weak stream. The 114 patients in cluster M3 reported mostly urinary frequency without incontinence. The 130 patients in cluster M4 reported severe frequency, urgency and urgency incontinence. Most other urinary symptoms statistically differed between cluster pairs. Patient reported outcomes of bowel symptoms, mental health, sleep dysfunction, erectile function and urological pain significantly differed across the clusters. CONCLUSIONS: We identified 4 data derived clusters among men seeking care for lower urinary tract symptoms. The clusters differed from traditional diagnostic categories. Further subtype refinement will be done to incorporate clinical data and nonurinary patient reported outcomes.",
        "Clustering algorithms are critical data mining techniques used to analyze a wide range of data. This study compares the utility of ant colony optimization (ACO), genetic algorithm (GA), and K-means methods to cluster climatic variables affecting the yield of rainfed wheat in northeast Iran from 1984 to 2010 (27 years). These variables included sunshine hours, wind speed, relative humidity, precipitation, maximum temperature, minimum temperature, and the number of wet days. Seven climatic factors with higher correlations with detrended rainfed wheat yield were selected based on Pearson correlation coefficient significance (P value < 0.1). Three variables (i.e., sunshine hours, wind, and average relative humidity) were excluded for clustering. In the next step based on Pearson correlation (P value < 0.05) between the yield, and the seven climate attributes, fitness function, and silhouette index, only four attributes with higher correlation in its cluster were selected for reclustering. Four climate attributes had an extensive association with yield, so we used four-dimensional clustering to describe the common characteristics of low-, medium-, and high-yielding years, and this is the significance of this research that we have done four-dimensional clustering. The silhouette index showed that the best number of clusters for each station was equal to three clusters. At the last step, reclustering was done through the best-selected method. The results yielded that GA was the best method.",
        "More and more affordable high-throughput techniques for measuring molecular features of biomedical samples have led to a huge increase in availability and size of different types of multi-omic datasets, containing, for example, genetic or histone modification data. Due to the multi-view characteristic of the data, established approaches for exploratory analysis are not directly applicable. Here we present web-rMKL, a web server that provides an integrative dimensionality reduction with subsequent clustering of samples based on data from multiple inputs. The underlying machine learning method rMKL-LPP performed best for clinical enrichment in a recent benchmark of state-of-the-art multi-view clustering algorithms. The method was introduced for a multi-omic cancer subtype discovery setting, however, it is not limited to this application scenario as exemplified by a presented use case for stem cell differentiation. web-rMKL offers an intuitive interface for uploading data and setting the parameters. rMKL-LPP runs on the back end and the user may receive notifications once the results are available. We also introduce a preprocessing tool for generating kernel matrices from tables containing numerical feature values. This program can be used to generate admissible input if no precomputed kernel matrices are available. The web server is freely available at web-rMKL.org.",
        "PURPOSE: Time to event is an important aspect of clinical decision making. This is particularly true when diseases have highly heterogeneous presentations and prognoses, as in chronic lymphocytic lymphoma (CLL). Although machine learning methods can readily learn complex nonlinear relationships, many methods are criticized as inadequate because of limited interpretability. We propose using unsupervised clustering of the continuous output of machine learning models to provide discrete risk stratification for predicting time to first treatment in a cohort of patients with CLL. PATIENTS AND METHODS: A total of 737 treatment-naive patients with CLL diagnosed at Mayo Clinic were included in this study. We compared predictive abilities for two survival models (Cox proportional hazards and random survival forest) and four classification methods (logistic regression, support vector machines, random forest, and gradient boosting machine). Probability of treatment was then stratified. RESULTS: Machine learning methods did not yield significantly more accurate predictions of time to first treatment. However, automated risk stratification provided by clustering was able to better differentiate patients who were at risk for treatment within 1 year than models developed using standard survival analysis techniques. CONCLUSION: Clustering the posterior probabilities of machine learning models provides a way to better interpret machine learning models.",
        "WiFi fingerprint positioning has been widely used in the indoor positioning field. The weighed K-nearest neighbor (WKNN) algorithm is one of the most widely used deterministic algorithms. The traditional WKNN algorithm uses Euclidean distance or Manhattan distance between the received signal strengths (RSS) as the distance measure to judge the physical distance between points. However, the relationship between the RSS and the physical distance is nonlinear, using the traditional Euclidean distance or Manhattan distance to measure the physical distance will lead to errors in positioning. In addition, the traditional RSS-based clustering algorithm only takes the signal distance between the RSS as the clustering criterion without considering the position distribution of reference points (RPs). Therefore, to improve the positioning accuracy, we propose an improved WiFi positioning method based on fingerprint clustering and signal weighted Euclidean distance (SWED). The proposed algorithm is tested by experiments conducted in two experimental fields. The results indicate that compared with the traditional methods, the proposed position label-assisted (PL-assisted) clustering result can reflect the position distribution of RPs and the proposed SWED-based WKNN (SWED-WKNN) algorithm can significantly improve the positioning accuracy.",
        "Algorithms for community detection are usually stochastic, leading to different partitions for different choices of random seeds. Consensus clustering has proven to be an effective technique to derive more stable and accurate partitions than the ones obtained by the direct application of the algorithm. However, the procedure requires the calculation of the consensus matrix, which can be quite dense if (some of) the clusters of the input partitions are large. Consequently, the complexity can get dangerously close to quadratic, which makes the technique inapplicable on large graphs. Here, we present a fast variant of consensus clustering, which calculates the consensus matrix only on the links of the original graph and on a comparable number of additional node pairs, suitably chosen. This brings the complexity down to linear, while the performance remains comparable as the full technique. Therefore, our fast consensus clustering procedure can be applied on networks with millions of nodes and links.",
        "In this paper, we present a new multi-objective optimization approach for segmentation of Magnetic Resonance Imaging (MRI) of the human brain. The proposed algorithm not only takes advantages but also solves major drawbacks of two well-known complementary techniques, called fuzzy entropy clustering method and region-based active contour method, using multi-objective particle swarm optimization (MOPSO) approach. In order to obtain accurate segmentation results, firstly, two fitness functions with independent characteristics, compactness and separation, are derived from kernelized fuzzy entropy clustering with local spatial information and bias correction (KFECSB) and a novel adaptive energy weight combined with global and local fitting energy active contour (AWGLAC) model. Then, they are simultaneously optimized to finally produce a set of non-dominated solutions, from which L2-metric method is used to select the best trade-off solution. Our algorithm is both verified and compared with other state-of-the-art methods using simulated MR images and real MR images from the McConnell Brain Imaging Center (BrainWeb) and the Internet Brain Segmentation Repository (IBSR), respectively. The experimental results demonstrate that the proposed technique achieves superior segmentation performance in terms of accuracy and robustness.",
        "Integrative clustering is a clustering approach for multiple datasets, which provide different views of a common group of subjects. It enables analyzing multi-omics data jointly to, for example, identify the subtypes of diseases, cells, and so on, capturing the complex underlying biological processes more precisely. On the other hand, there has been a great deal of interest in incorporating the prior structural knowledge on the features into statistical analyses over the past decade. The knowledge on the gene regulatory network (pathways) can potentially be incorporated into many genomic studies. In this paper, we propose a novel integrative clustering method which can incorporate the prior graph knowledge. We first develop a generalized Bayesian factor analysis (GBFA) framework, a sparse Bayesian factor analysis which can take into account the graph information. Our GBFA framework employs the spike and slab lasso (SSL) prior to impose sparsity on the factor loadings and the Markov random field (MRF) prior to encourage smoothing over the adjacent factor loadings, which establishes a unified shrinkage adaptive to the loading size and the graph structure. Then, we use the framework to extend iCluster+, a factor analysis based integrative clustering approach. A novel variational EM algorithm is proposed to efficiently estimate the MAP estimator for the factor loadings. Extensive simulation studies and the application to the NCI60 cell line dataset demonstrate that the propose method is superior and delivers more biologically meaningful outcomes.",
        "High-dimensional flow and mass cytometry allow cell types and states to be characterized in great detail by measuring expression levels of more than 40 targeted protein markers per cell at the single-cell level. However, data analysis can be difficult, due to the large size and dimensionality of datasets as well as limitations of existing computational methods. Here, we present diffcyt, a new computational framework for differential discovery analyses in high-dimensional cytometry data, based on a combination of high-resolution clustering and empirical Bayes moderated tests adapted from transcriptomics. Our approach provides improved statistical performance, including for rare cell populations, along with flexible experimental designs and fast runtimes in an open-source framework.",
        "Various subspace clustering methods have been successively developed to process multi-view datasets. Most of the existing methods try to obtain a consensus structure coefficient matrix based on view-specific subspace recoveries. However, since view-specific structures contain individualized components that are intrinsically different from the consensus structure, directly adopting view-specific subspace structures might not be a reasonable choice. With this concern in mind, our goal in this paper is to seek novel strategies to extract valuable components from view-specific structures that are consistent with the consensus subspace structure. To this end, we propose a novel multi-view subspace clustering method named Split Multiplicative Multi-view Subspace Clustering (SM2SC) with the joint strength of a multiplicative decomposition scheme and a variable splitting scheme. Specifically, the multiplicative decomposition scheme effectively guarantees the structural consistency of the extracted components. Then the variable splitting scheme takes a step further via extracting the structural consistent components from view-specific structures. Furthermore, an alternating optimization algorithm is proposed to optimize the resulting optimization problem, which is non-convex and constrained. We prove that this algorithm could converge to a critical point. Finally, we provide empirical studies on real-world datasets that speak to the practical efficacy of our proposed method. The source code is released on GitHub.",
        "Inference of tumor and edema areas from brain magnetic resonance imaging (MRI) data remains challenging owing to the complex structure of brain tumors, blurred boundaries, and external factors such as noise. To alleviate noise sensitivity and improve the stability of segmentation, an effective hybrid clustering algorithm combined with morphological operations is proposed for segmenting brain tumors in this paper. The main contributions of the paper are as follows: firstly, adaptive Wiener filtering is utilized for denoising, and morphological operations are used for removing nonbrain tissue, effectively reducing the method's sensitivity to noise. Secondly, K-means++ clustering is combined with the Gaussian kernel-based fuzzy C-means algorithm to segment images. This clustering not only improves the algorithm's stability, but also reduces the sensitivity of clustering parameters. Finally, the extracted tumor images are postprocessed using morphological operations and median filtering to obtain accurate representations of brain tumors. In addition, the proposed algorithm was compared with other current segmentation algorithms. The results show that the proposed algorithm performs better in terms of accuracy, sensitivity, specificity, and recall.",
        "\"Vehicular Ad-hoc Networks\" (VANETs): As an active research area in the field of wireless sensor networks, they ensure road safety by exchanging alert messages about unexpected events in a decentralized manner. One of the significant challenges in the design of an efficient dissemination protocol for VANETs is the broadcast storm problem, owing to the large number of rebroadcasts. A generic solution to prevent the broadcast storm problem is to cluster the vehicles based on topology, density, distance, speed, or location in such a manner that only a fewer number of vehicles will rebroadcast the alert message to the next group. However, the selection of cluster heads and gateways of the clusters are the key factors that need to be optimized in order to limit the number of rebroadcasts. Hence, to address the aforementioned issues, this paper presents a novel distributed algorithm CDS_SC: Connected Dominating Set and Set Cover for cluster formation that employs a dominating set to choose cluster heads and set covering to select cluster gateways. The CDS_SC is unique among state-of-the-art algorithms, as it relies on local neighborhood information and constructs clusters incrementally. Hence, the proposed method can be implemented in a distributed manner as an event-triggered protocol. Also, the stability of cluster formation is increased along with a reduction in rebroadcasting by allowing a cluster head to be passive when all its cluster members can receive the message from the gateway vehicles. The simulation was carried out in dense, average, and sparse traffic scenarios by varying the number of vehicles injected per second per lane. Besides, the speed of each individual vehicle in each scenario was varied to test the degree of cohesion between vehicles with different speeds. The simulation results confirmed that the proposed algorithm achieved 99% to 100% reachability of alert messages with only 6% to 10% of rebroadcasting vehicles in average and dense traffic scenarios.",
        "The detection of the lumen and media-adventitia (MA) borders in intravascular ultrasound (IVUS) images is crucial for quantifying plaque burdens. The challenge of the segmentation work mainly roots in various artifacts in the image. Most of the published methods involve the establishment of complex models but do not behave well on images with artifacts. In this study, aiming at automatically delineating borders in IVUS frames acquired by 20MHz ultrasound probes, we present a fuzzy clustering-initialized hierarchical level set evolution (FC-HLSE) method. A cluster selection strategy based on the spatial fuzzy c-means (FCM) is proposed to generate the initial value and regularization term of the level set evolution (LSE). The contour convergence splits into two LSE steps between which an ingenious contour extraction (consisting of the morphological processing, the seek and linear interpolation, the gradient-based and circular fitting-based refinement) is carried out. We evaluate the proposed methodology on the publicly available 435 images by comparing auto-segmented results with the ground truth. The performance of the method is quantified using the Jaccard measure (JM), the Hausdorff distance (HD), the percentage of area difference (PAD), the linear regression and Bland-Altman analysis. Results reveal that our method can handle images with or without artifacts. The algorithm is able to extract the lumen/MA border with the JM of 0.90/0.89, the HD of 0.31/0.40mm, the PAD of 0.07/0.08 in average, which is better in some cases compared with several state-of-the-art methods.",
        "BACKGROUND: Single-cell RNA-sequencing (scRNA-seq) technologies have advanced rapidly in recent years and enabled the quantitative characterization at a microscopic resolution. With the exponential growth of the number of cells profiled in individual scRNA-seq experiments, the demand for identifying putative cell types from the data has become a great challenge that appeals for novel computational methods. Although a variety of algorithms have recently been proposed for single-cell clustering, such limitations as low accuracy, inferior robustness, and inadequate stability greatly impede the scope of applications of these methods. RESULTS: We propose a novel model-based algorithm, named VPAC, for accurate clustering of single-cell transcriptomic data through variational projection, which assumes that single-cell samples follow a Gaussian mixture distribution in a latent space. Through comprehensive validation experiments, we demonstrate that VPAC can not only be applied to datasets of discrete counts and normalized continuous data, but also scale up well to various data dimensionality, different dataset size and different data sparsity. We further illustrate the ability of VPAC to detect genes with strong unique signatures of a specific cell type, which may shed light on the studies in system biology. We have released a user-friendly python package of VPAC in Github ( https://github.com/ShengquanChen/VPAC ). Users can directly import our VPAC class and conduct clustering without tedious installation of dependency packages. CONCLUSIONS: VPAC enables highly accurate clustering of single-cell transcriptomic data via a statistical model. We expect to see wide applications of our method to not only transcriptome studies for fully understanding the cell identity and functionality, but also the clustering of more general data.",
        "BACKGROUND: Gene Co-expression Network Analysis (GCNA) helps identify gene modules with potential biological functions and has become a popular method in bioinformatics and biomedical research. However, most current GCNA algorithms use correlation to build gene co-expression networks and identify modules with highly correlated genes. There is a need to look beyond correlation and identify gene modules using other similarity measures for finding novel biologically meaningful modules. RESULTS: We propose a new generalized gene co-expression analysis algorithm via subspace clustering that can identify biologically meaningful gene co-expression modules with genes that are not all highly correlated. We use low-rank representation to construct gene co-expression networks and local maximal quasi-clique merger to identify gene co-expression modules. We applied our method on three large microarray datasets and a single-cell RNA sequencing dataset. We demonstrate that our method can identify gene modules with different biological functions than current GCNA methods and find gene modules with prognostic values. CONCLUSIONS: The presented method takes advantage of subspace clustering to generate gene co-expression networks rather than using correlation as the similarity measure between genes. Our generalized GCNA method can provide new insights from gene expression datasets and serve as a complement to current GCNA algorithms.",
        "BACKGROUND: Gene-set analysis (GSA) has been commonly used to identify significantly altered pathways or functions from omics data. However, GSA often yields a long list of gene-sets, necessitating efficient post-processing for improved interpretation. Existing methods cluster the gene-sets based on the extent of their overlap to summarize the GSA results without considering interactions between gene-sets. RESULTS: Here, we presented a novel network-weighted gene-set clustering that incorporates both the gene-set overlap and protein-protein interaction (PPI) networks. Three examples were demonstrated for microarray gene expression, GWAS summary, and RNA-sequencing data to which different GSA methods were applied. These examples as well as a global analysis show that the proposed method increases PPI densities and functional relevance of the resulting clusters. Additionally, distinct properties of gene-set distance measures were compared. The methods are implemented as an R/Shiny package GScluster that provides gene-set clustering and diverse functions for visualization of gene-sets and PPI networks. CONCLUSIONS: Network-weighted gene-set clustering provides functionally more relevant gene-set clusters and related network analysis.",
        "Cloud computing technology is widely used at present. However, cloud computing servers are far from terminal users, which may lead to high service request delays and low user satisfaction. As a new computing architecture, fog computing is an extension of cloud computing that can effectively solve the aforementioned problems. Resource scheduling is one of the key technologies in fog computing. We propose a resource scheduling method for fog computing in this paper. First, we standardize and normalize the resource attributes. Second, we combine the methods of fuzzy clustering with particle swarm optimization to divide the resources, and the scale of the resource search is reduced. Finally, we propose a new resource scheduling algorithm based on optimized fuzzy clustering. The experimental results show that our method can improve user satisfaction and the efficiency of resource scheduling.",
        "OBJECTIVE: Real-time closed-loop neural feedback control requires the analysis of action potential traces within several milliseconds after they have been recorded from the brain. The current generation of spike clustering algorithms were mostly designed for off-line use and also require a significant amount of computational resources. A new spike clustering algorithm, termed 'enhanced growing neural gas (EGNG)', was therefore developed that is computationally lightweight and memory conserving. The EGNG algorithm can adapt to changes of the electrophysiological recording environment and can classify both pre-recorded and streaming action potentials. APPROACH: The algorithm only uses a small number of EGNG nodes and edges to learn the neural spike distributions which eliminates the need of retaining the neural data in the system memory to conserve computational resources. Most of the computations revolve around calculating Euclidian distances, which is computationally inexpensive and can be implemented in parallel using digital circuit technology. MAIN RESULTS: EGNG was evaluated off-line using both synthetic and pre-recorded neural spikes. Streaming synthetic neural spikes were also used to evaluate the ability of EGNG to classify action potentials in real-time. The algorithm was also implemented in hardware with a Field Programming Gate Array (FPGA) chip, and the worst-case clustering latency was 3.10 micros, allowing a minimum of 322 580 neural spikes to be clustered per second. SIGNIFICANCE: The EGNG algorithm provides a viable solution to classification of neural spikes in real-time and can be implemented with limited computational resources as a front-end spike clustering unit for future tethered-free and miniaturized closed-loop neural feedback systems.",
        "Dissimilarity measures play a crucial role in clustering and, are directly related to the performance of clustering algorithms. However, effectively measuring the dissimilarity is not easy, especially for categorical data. The main difficulty of the dissimilarity measurement for categorical data is that its representation lacks a clear space structure. Therefore, the space structure-based representation has been proposed to provide the categorical data with a clear linear representation space. This representation improves the clustering performance obviously but only applies to small data sets because its dimensionality increases rapidly with the size of the data set. In this paper, we investigate the possibility of reducing the dimensionality of the space structure-based representation while maintaining the same representation ability. A lightweight representation scheme is proposed by taking a set of representative objects as the reference system (called the reference set) to position other objects in the Euclidean space. Moreover, a preclustering-based strategy is designed to select an appropriate reference set quickly. Finally, the representation scheme together with the k -means algorithm provides an efficient method to cluster the categorical data. The theoretical and the experimental analysis shows that the proposed method outperforms state-of-the-art methods in terms of both accuracy and efficiency.",
        "Image cosegmentation aims at extracting the common objects from multiple images simultaneously. Existing methods mainly solve cosegmentation via the pre-defined graph, which lacks flexibility and robustness to handle various visual patterns. Besides, similar backgrounds also confuse the identifying of the common foreground. To address these issues, we propose a novel Multi-view Saliency-Guided Clustering algorithm (MvSGC) for the image cosegmentation task. In our model, the unsupervised saliency prior is used as partition-level side information to guide the foreground clustering process. To achieve robustness to noises and missing observations, similarities on instance-level and partition-level are both considered. Specifically, a unified clustering model with cosine similarity is proposed to capture the intrinsic structure of data and keep partition result consistent with the side information. Moreover, we leverage multi-view weight learning to integrate multiple feature representations to further improve the robustness of our approach. A K-means-like optimization algorithm is developed to proceed the constrained clustering in a highly efficient way with theoretical support. Experimental results on three benchmark datasets (i.e., the iCoseg, MSRC and Internet image dataset) and one RGB-D image dataset demonstrate the superiority of applying our clustering method for image cosegmentation.",
        "The application of data mining has been increasing day to day whereas the data base is also enhancing simultaneously. Hence retrieving required content from a huge data base is a critical task. This paper focus on biomedical engineering field, it concentrates on initial stage of database such as data preprocessing and cleansing to deal with noise and missing data in large biomedical data sets. The database of biomedical is huge and enhancing nature retrieving of specific content will be a critical task. Suggesting prescription with respect to identified disease based on profile analysis of specific patient is not available in current system. This paper proposes a recommendation system of prescription based on disease identification is done by combining user and professional suggestion with profile based analysis. Hence this focuses on profile based suggestions and report will be generated. The retrieving of specific suggestion from a huge database is done by hybrid feature selection algorithm. This approach focuses on enabling recommendation based on user profile and implementing Hybrid feature selection algorithm to retrieve specific content from a huge database. Hence it attains better retrieval of required content from a huge database compared to other existing approaches and suggests better recommendation with respect to user profile.",
        "In this contribution, we review recent developments and applications of a dynamic clustering algorithm SWINGER tailored for the multiscale molecular simulations of biomolecular systems. The algorithm on-the-fly redistributes solvent molecules among supramolecular clusters. In particular, we focus on its applications in combination with the adaptive resolution scheme, which concurrently couples atomistic and coarse-grained molecular representations. We showcase the versatility of our multiscale approach on a few applications to biomolecular systems coupling atomistic and supramolecular water models such as the well-established MARTINI and dissipative particle dynamics models and provide an outlook for future work.",
        "The grouping of clusters is an important task to perform for the initial stage of clinical implication and diagnosis of a disease. The researchers performed evaluation work on instance distributions and cluster groups for epidemic classification, based on manual data extracted from various repositories, in order to evaluate Euclidean points. This study was carried out on Weka (3.9.2) using 281 real-life health records of diabetes mellitus patients including males and females of ages>20 and <87, who were simultaneously suffering from other chronic disease symptoms, in Nigeria from 2017 to 2018. Updated plugins of K-mean and self-organizing map(SOM) machine learning algorithms were used to cluster the data class of mellitus type for initial clinical implications. The results of the K-mean assessment were built in 0.21 seconds with nine iterations for \"type\" and eight for \"class\" attributes. Out of 281 instances, 87 (30.97%) were classified as negative and 194 (69.03%) as positive in the testing on the Euclidean space plot. By assessment for Euclidean points, SOM discovered the search space in a more effective way, but K-mean positioning potencies are impulsive in convergence. This study is important for epidemiological disease diagnosis in countries with a high epidemic risk and low socioeconomic status.",
        "Spectral clustering plays a significant role in applications that rely on multi-view data due to its well-defined mathematical framework and excellent performance on arbitrarily-shaped clusters. Unfortunately, directly optimizing the spectral clustering inevitably results in an NP-hard problem due to the discrete constraints on the clustering labels. Hence, conventional approaches intuitively include a relax-and-discretize strategy to approximate the original solution. However, there are no principles in this strategy that prevent the possibility of information loss between each stage of the process. This uncertainty is aggravated when a procedure of heterogeneous features fusion has to be included in multi-view spectral clustering. In this paper, we avoid an NP-hard optimization problem and develop a general framework for multi-view discrete graph clustering by directly learning a consensus partition across multiple views, instead of using the relax-and-discretize strategy. An effective re-weighting optimization algorithm is exploited to solve the proposed challenging problem. Further, we provide a theoretical analysis of the model's convergence properties and computational complexity for the proposed algorithm. Extensive experiments on several benchmark datasets verify the effectiveness and superiority of the proposed algorithm on clustering and image segmentation tasks.",
        "Finding the informative subspaces of high-dimensional datasets is at the core of numerous applications in computer vision, where spectral-based subspace clustering is arguably the most widely studied method due to its strong empirical performance. Such algorithms first compute an affinity matrix to construct a self-representation for each sample using other samples as a dictionary. Sparsity and connectivity of the self-representation play important roles in effective subspace clustering. However, simultaneous optimization of both factors is difficult due to their conflicting nature, and most existing methods are designed to address only one factor. In this paper, we propose a post-processing technique to optimize both sparsity and connectivity by finding good neighbors. Good neighbors induce key connections among samples within a subspace and not only have large affinity coefficients but are also strongly connected to each other. We reassign the coefficients of the good neighbors and eliminate other entries to generate a new coefficient matrix. We show that the few good neighbors can effectively recover the subspace, and the proposed post-processing step of finding good neighbors is complementary to most existing subspace clustering algorithms. Experiments on five benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods with negligible additional computation cost.",
        "OBJECTIVE: Research in animal models has shown that many EEG sleep features reflect local conditions, which is a consequence of relative inactivity of neuronal clusters. In humans, the authors previously reported that focal sleep patterns appear on the cortex during the wake state and suggested that this underlies the condition described as drowsiness. The focal changes at individual electrodes appeared as a combination of increased instantaneous amplitude in the delta band and decreased instantaneous frequency in the theta-alpha band during non-REM sleep, with the opposite occurring during the wake state, permitting their categorization as \"active\" and \"inactive.\" A limitation of the previous work was the use of a binary k-means clustering algorithm, which created the possibility that the findings were biased toward a predominantly inactive state while the study subject was still awake. The present study tested the hypothesis that analyzing the same data by using a continuous rather than binary classifier would overcome this limitation. METHODS: An analysis was performed on records from six patients with refractory epilepsy who were undergoing video-electrocorticographic monitoring with implanted subdural grid electrodes. A fuzzy c-means clustering algorithm was utilized after feature extraction from the recordings to create state classifications for each moment in each recording. A subsequent analysis was performed to determine the relative contributions of instantaneous amplitude versus instantaneous frequency to the classification. RESULTS: Localized state changes consistent with the hypothesis were observed. The contributions from instantaneous frequency and amplitude appeared roughly equal. CONCLUSIONS: This study reveals evidence of local sleep during the wake state in humans.",
        "The human precuneus is involved in many high-level cognitive functions, which strongly suggests the existence of biologically meaningful subdivisions. However, the functional parcellation of the precuneus needs much to be investigated. In this study, we developed an eigen clustering (EIC) approach for the parcellation using precuneus-cortical functional connectivity from fMRI data of the Human Connectome Project. The EIC approach is robust to noise and can automatically determine the cluster number. It is consistently demonstrated that the human precuneus can be subdivided into six symmetrical and connected parcels. The anterior and posterior precuneus participate in sensorimotor and visual functions, respectively. The central precuneus with four subregions indicates a media role in the interaction of the default mode, dorsal attention, and frontoparietal control networks. The EIC-based functional parcellation is free of the spatial distance constraint and is more functionally coherent than parcellation using typical clustering algorithms. The precuneus subregions had high accordance with cortical morphology and revealed good functional segregation and integration characteristics in functional task-evoked activations. This study may shed new light on the human precuneus function at a delicate level and offer an alternative scheme for human brain parcellation.",
        "Algorithms for quantifying the differences between two lattices are used for Bravais lattice determination, database lookup for unit cells to select candidates for molecular replacement, and recently for clustering to group together images from serial crystallography. It is particularly desirable for the differences between lattices to be computed as a perturbation-stable metric, i.e. as distances that satisfy the triangle inequality, so that standard tree-based nearest-neighbor algorithms can be used, and for which small changes in the lattices involved produce small changes in the distances computed. A perturbation-stable metric space related to the reduction algorithm of Selling and to the Bravais lattice determination methods of Delone is described. Two ways of representing the space, as six-dimensional real vectors or equivalently as three-dimensional complex vectors, are presented and applications of these metrics are discussed. (Note: in his later publications, Boris Delaunay used the Russian version of his surname, Delone.).",
        "BACKGROUND: Medical data stream clustering has become an integral part of medical decision systems since it extracts highly-sensitive information from a tremendous flow of medical data. However, clustering and maintaining of medical data streams is still a challenging task. That is because the evolving of medical data streams imposes various challenges for clustering such as the ability to discover the arbitrary shape of a cluster, the ability to group data streams without a predefined number of clusters, and the ability to maintain the data clusters dynamically. OBJECTIVE: To support the online medical decisions, there is a need to address the clustering challenges. Therefore, in this paper, we propose an effective density-based clustering and dynamic maintenance framework for grouping the patients with similar symptoms into meaningful clusters and monitoring the patients' status frequently. METHODS: For clustering, we generate a set of initial medical data clusters based on the combination of Piece-wise Aggregate Approximation and the density-based spatial clustering of applications with noise called (PAA+DBSCAN) algorithm. For maintenance, when new medical data streams arrive, we maintain the initially generated medical data clusters dynamically. Since the incremental cluster maintenance is time-consuming, we further propose an Advanced Cluster Maintenance (ACM) approach to improve the performance of the dynamic cluster maintenance. RESULTS: The experimental results on real-world medical datasets demonstrate the effectiveness and efficiency of our proposed approaches. The PAA+DBSCAN algorithm is more efficient and effective than the exact DBSCAN algorithm. Moreover, the ACM approach requires less running time in comparison with the Baseline Cluster Maintenance (BCM) approach using different tuning parameter values in all datasets. That is because the BCM approach tracks all the data points in the cluster. CONCLUSION: The proposed framework is capable of clustering and maintaining the medical data streams effectively by means of grouping the patients who share similar symptoms and tracking the patients status that naturally tends to be changing over time.",
        "Popular clustering algorithms based on usual distance functions (e.g., the Euclidean distance) often suffer in high dimension, low sample size (HDLSS) situations, where concentration of pairwise distances and violation of neighborhood structure have adverse effects on their performance. In this article, we use a new data-driven dissimilarity measure, called MADD, which takes care of these problems. MADD uses the distance concentration phenomenon to its advantage, and as a result, clustering algorithms based on MADD usually perform well for high dimensional data. We establish it using theoretical as well as numerical studies. We also address the problem of estimating the number of clusters. This is a challenging problem in cluster analysis, and several algorithms are available for it. We show that many of these existing algorithms have superior performance in high dimensions when they are constructed using MADD. We also construct a new estimator based on a penalized version of the Dunn index and prove its consistency in the HDLSS asymptotic regime. Several simulated and real data sets are analyzed to demonstrate the usefulness of MADD for cluster analysis of high dimensional data.",
        "The belief rule-based classification system (BRBCS) is a promising technique for addressing different types of uncertainty in complex classification problems, by introducing the belief function theory into the classical fuzzy rule-based classification system. However, in the BRBCS, high numbers of instances and features generally induce a belief rule base (BRB) with large size, which degrades the interpretability of the classification model for big data sets. In this paper, a BRB learning method based on the evidential C-means clustering (ECM) algorithm is proposed to efficiently design a compact belief rule-based classification system (CBRBCS). First, a supervised version of the ECM algorithm is designed by means of weighted product-space clustering to partition the training set with the goals of obtaining both good inter-cluster separability and inner-cluster pureness. Then, a systematic method is developed to construct belief rules based on the obtained credal partitions. Finally, an evidential partition entropy-based optimization procedure is designed to get a compact BRB with a better trade-off between accuracy and interpretability. The key benefit of the proposed CBRBCS is that it can provide a more interpretable classification model on the premise of comparative accuracy. Experiments based on synthetic and real data sets have been conducted to evaluate the classification accuracy and interpretability of the proposal.",
        "Multimodal optimization problems (MMOPs) are common in real-world applications and involve identifying multiple optimal solutions for decision makers to choose from. The core requirement for dealing with such problems is to balance the ability of exploration in the global space and exploitation in the multiple optimal areas. In this paper, based on the differential evolution (DE), we propose a novel algorithm focusing on the formulation, balance, and keypoint of species for MMOPs, called FBK-DE. First, nearest-better clustering (NBC) is used to divide the population into multiple species with minimum size limitations. Second, to avoid placing too many individuals into one species, a species balance strategy is proposed to adjust the size of each species. Third, two keypoint-based mutation operators named DE/keypoint/1 and DE/keypoint/2 are proposed to evolve each species together with traditional mutation operators. The experimental results of FBK-DE on 20 benchmark functions are compared with 15 state-of-the-art multimodal optimization algorithms. The comparisons show that the proposed FBK-DE performs competitively with these algorithms.",
        "PURPOSE: This study investigated the benefits of implementing a cluster analysis technique to extract relevant information from a computed tomography (CT) dose registry archive. METHODS: A CT patient dose database consisting of about 12,000 examinations and 29,000 single scans collected from three CT systems was interrogated. The database was divided into six subsets according to the equipment and the reference phantoms in the definition of the dose indicators. Hierarchical (single, average, and complete linkage, Ward) and not hierarchical (K-means) clustering methods were implemented using R software. The suitable number of clusters for each CT system was determined by analysing the dendrogram, the within clusters sum of squares, and the cluster content. Summary statistics were produced for each cluster, and the outliers of the dose indicator distribution were investigated. RESULTS: Ward clustering identified the most common combinations of scanning parameters for each group. The optimal number of clusters for each CT equipment system ranged from 5 to 15. The main diagnostic applications were then extracted from each cluster. Outlier analysis of the dose indicator distribution of each cluster revealed potential improper settings that resulted in increased patient dose. CONCLUSIONS: Clustering methods applied to CT patient dose archives provide a quick and effective overview of the main combinations of currently used exposure parameters and the consequences for dose indicator distributions, also when protocol labels and/or study descriptions are not homogeneous.",
        "Since wireless sensor networks (WSNs) are powered by energy-constrained batteries, many energy-efficient routing protocols have been proposed to extend the network lifetime. However, most of the protocols do not well balance the energy consumption of the WSNs. The hotspot problem caused by unbalanced energy consumption in the WSNs reduces the network lifetime. To solve the problem, this paper proposes a PSO (Particle Swarm Optimization)-based uneven dynamic clustering multi-hop routing protocol (PUDCRP). In the PUDCRP protocol, the distribution of the clusters will change dynamically when some nodes fail. The PSO algorithm is used to determine the area where the candidate CH (cluster head) nodes are located. The adaptive clustering method based on node distribution makes the cluster distribution more reasonable, which balances the energy consumption of the network more effectively. In order to improve the energy efficiency of multi-hop transmission between the BS (Base Station) and CH nodes, we also propose a connecting line aided route construction method to determine the most appropriate next hop. Compared with UCCGRA, multi-hop EEBCDA, EEMRP, CAMP, PSO-ECHS and PSO-SD, PUDCRP prolongs the network lifetime by between 7.36% and 74.21%. The protocol significantly balances the energy consumption of the network and has better scalability for various sizes of network.",
        "Fuzzy clustering methods identify naturally occurring clusters in a dataset, where the extent to which different clusters are overlapped can differ. Most methods have a parameter to fix the level of fuzziness. However, the appropriate level of fuzziness depends on the application at hand. This paper presents an entropy c -means (ECM), a method of fuzzy clustering that simultaneously optimizes two contradictory objective functions, resulting in the creation of fuzzy clusters with different levels of fuzziness. This allows ECM to identify clusters with different degrees of overlap. ECM optimizes the two objective functions using two multiobjective optimization methods, nondominated sorting genetic algorithm II (NSGA-II) and multiobjective evolutionary algorithm based on decomposition (MOEA/D). We also propose a method to select a suitable tradeoff clustering from the Pareto front. Experiments on challenging synthetic datasets as well as real-world datasets show that ECM leads to better cluster detection compared to the conventional fuzzy clustering methods as well as previously used multiobjective methods for fuzzy clustering.",
        "Multiview clustering (MVC), which aims to explore the underlying cluster structure shared by multiview data, has drawn more research efforts in recent years. To exploit the complementary information among multiple views, existing methods mainly learn a common latent subspace or develop a certain loss across different views, while ignoring the higher level information such as basic partitions (BPs) generated by the single-view clustering algorithm. In light of this, we propose a novel marginalized multiview ensemble clustering (M(2)VEC) method in this paper. Specifically, we solve MVC in an EC way, which generates BPs for each view individually and seeks for a consensus one. By this means, we naturally leverage the complementary information of multiview data upon the same partition space. In order to boost the robustness of our approach, the marginalized denoising process is adopted to mimic the data corruptions and noises, which provides robust partition-level representations for each view by training a single-layer autoencoder. A low-rank and sparse decomposition is seamlessly incorporated into the denoising process to explicitly capture the consistency information and meanwhile compensate the distinctness between heterogeneous features. Spectral consensus graph partitioning is also involved by our model to make M(2)VEC as a unified optimization framework. Moreover, a multilayer M(2)VEC is eventually delivered in a stacked fashion to encapsulate nonlinearity into partition-level representations for handling complex data. Experimental results on eight real-world data sets show the efficacy of our approach compared with several state-of-the-art multiview and EC methods. We also showcase our method performs well with partial multiview data.",
        "Aims: The aim of this study is to compare some machine learning methods with traditional statistical parametric analyses using logistic regression to investigate the relationship of risk factors for diabetes and cardiovascular (cardiometabolic risk) for U.S. adults using a cross-sectional data from participants in a wellness improvement program. Methods: Logistic regression was used to find the relationship between individual risk factors, predictor and cardiometabolic risk. Supervised machine learning methods were used to predict risk and produce a ranking of variables' importance. A clustering method was used to identify subpopulations of interest. Predictors were divided into those that are nonmodifiable and those that are modifiable. Results: The population comprised 217,254 adults of whom 8.1% had diabetes. Using logistic regression, six variables were identified to be negatively related and eleven were positively related to cardiometabolic risk. Three supervised machine learning classifiers (random forest, gradient boosting, and bagging) were applied with average AUC to be 0.806. Each classifier also produced a ranking of variables' importance. Four subgroups were identified with a k-medoid clustering algorithm, which were mainly distinguished by gender and diabetes status. Conclusions: The study illustrates that machine learning is an important addition to traditional logistic regression in terms of identifying important cardiometabolic risk factors and ranking their importance and the potential for interventions based on lifestyle and medications at an individual level.",
        "We introduce a tensor-based clustering method to extract sparse, low-dimensional structure from high-dimensional, multi-indexed datasets. This framework is designed to enable detection of clusters of data in the presence of structural requirements which we encode as algebraic constraints in a linear program. Our clustering method is general and can be tailored to a variety of applications in science and industry. We illustrate our method on a collection of experiments measuring the response of genetically diverse breast cancer cell lines to an array of ligands. Each experiment consists of a cell line-ligand combination, and contains time-course measurements of the early signalling kinases MAPK and AKT at two different ligand dose levels. By imposing appropriate structural constraints and respecting the multi-indexed structure of the data, the analysis of clusters can be optimized for biological interpretation and therapeutic understanding. We then perform a systematic, large-scale exploration of mechanistic models of MAPK-AKT crosstalk for each cluster. This analysis allows us to quantify the heterogeneity of breast cancer cell subtypes, and leads to hypotheses about the signalling mechanisms that mediate the response of the cell lines to ligands."
      ]
    }
  ]
}