{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hacksaremeta/IS-Sentence-Completion/blob/main/src/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# DataManager Class\n",
    "- Provides save and load functionality for datasets in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging\n",
    "from Bio import Entrez, Medline\n",
    "\n",
    "class DataManager():\n",
    "    \"\"\"Provides save and load functionality for datasets in json format\"\"\"\n",
    "    def __init__(self, email, root_dir):\n",
    "        self.email = email\n",
    "        self.root_dir = root_dir\n",
    "        self.log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def _exists_dataset(self, name):\n",
    "        \"\"\"Checks whether a dataset with the given name exists\"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            return False\n",
    "            \n",
    "        for file in os.listdir(self.root_dir):\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(self.root_dir, file), 'r') as f:\n",
    "                    content = json.load(f)\n",
    "                    if content[\"name\"] == name:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def _fetch_papers(self, query : str, limit : int) -> 'list[dict]':\n",
    "        \"\"\"Retrieves data from PubMed\"\"\"\n",
    "        Entrez.email = self.email\n",
    "        record = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=limit))\n",
    "        idlist = record[\"IdList\"]\n",
    "\n",
    "        self.log.info(\"\\nFound %d records for %s.\" % (len(idlist), query.strip()))\n",
    "\n",
    "        records = Medline.parse(Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode = \"text\"))\n",
    "        return list(records)\n",
    "\n",
    "    def _fetch_abstracts(self, query : str, limit : int) -> 'list[str]':\n",
    "        \"\"\"Retrieves abstracts from PubMed\"\"\"\n",
    "        papers = self._fetch_papers(query, limit)\n",
    "        list_of_abstracts = [p['AB'] for p in papers]\n",
    "\n",
    "        return list_of_abstracts\n",
    "        \n",
    "    def create_dataset(self, queries : 'list[str]', name : str, limit=50, overwrite=False) -> None:\n",
    "        \"\"\"\n",
    "        Wraps other methods in this class\n",
    "        Creates a dataset from multiple queries\n",
    "        Does nothing if the dataset is already present (param overwrite)\n",
    "        Limits every query to <limit> results\n",
    "        \"\"\"\n",
    "        exists_dataset = self._exists_dataset(name)\n",
    "        if not exists_dataset or (exists_dataset and overwrite):\n",
    "            self.log.info(\"Dataset does not exist, fetching from PubMed...\")\n",
    "\n",
    "            res = dict()\n",
    "            res[\"name\"] = name\n",
    "            res[\"data\"] = list()\n",
    "            \n",
    "            for q in queries:\n",
    "                q_data = dict()\n",
    "                q_data[\"query\"] = q\n",
    "                q_data[\"abstracts\"] = self._fetch_abstracts(q, limit)\n",
    "                res[\"data\"].append(q_data)\n",
    "            \n",
    "            self._save_dataset(res, name)\n",
    "        else:\n",
    "            self.log.info(\"Dataset already exists, skipping fetch\")\n",
    "\n",
    "    def _save_dataset(self, dataset: dict, name : str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a file <name>.json in the dataset directory\n",
    "        For JSON file structure see below\n",
    "        Param dataset has a structure analogous to the JSON file\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root_dir):\n",
    "            os.makedirs(self.root_dir)\n",
    "\n",
    "        with open(os.path.join(self.root_dir, name + \".json\"), 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "    def load_full_dataset(self, name : str) -> 'list[str]':\n",
    "        \"\"\"\n",
    "        Finds the file that matches given <name> in JSON information,\n",
    "        parses it, loading all abstracts into a list (one string for each abstract)\n",
    "        and returns it (Error if dataset doesn't exist)\n",
    "        \"\"\"\n",
    "        # TODO: <use Python json to load dataset from file>\n",
    "        pass\n",
    "\n",
    "    def load_query_from_dataset(self, name : str, query : str) -> 'list[str]':\n",
    "        \"\"\"Like load_full_dataset but only loads abstracts for a single query\"\"\"\n",
    "        # TODO: <use Python json to load single query from dataset>\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Set log level\n",
    "    logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(name)s: %(message)s')\n",
    "\n",
    "    # Create DataManager in '../res/datasets' folder\n",
    "    data_folder = os.path.join(\"..\", \"res\", \"datasets\")\n",
    "    dman = DataManager(\"hexameter.trash@gmail.com\", data_folder)\n",
    "\n",
    "    dataset_name = \"RNA Dataset\"\n",
    "    queries = [\"RNA\", \"mRNA\", \"tRNA\"]\n",
    "\n",
    "    # Gather maximum of 100 abstracts for each query\n",
    "    # I would suggest around 5 - 20 abstracts in total for the small data sets\n",
    "    # and maybe 500 - 5000 for the final ones but we'll have to test\n",
    "    # since that depends on how long it takes to train the network\n",
    "    # This only queries PubMed if data if the data is not already present\n",
    "    dman.create_dataset(queries, dataset_name, 5)\n",
    "\n",
    "    # Load the dataset\n",
    "    abstracts = dman.load_full_dataset(dataset_name)\n",
    "    abstracts_mrna = dman.load_query_from_dataset(dataset_name, queries[1])\n",
    "\n",
    "    # Do stuff with abstracts\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
